{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ MEMORY OPTIMIZATION HELPERS LOADED! Ready for NASDAQ 100+ scale.\n",
            "Key features: Lazy parameter generation, vectorized calcs, streaming CSV, chunked data processing.\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# CRITICAL MEMORY OPTIMIZATION FOR NASDAQ 100+ SCALE COMPUTATION\n",
        "# ================================================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from operator import mul\n",
        "from typing import Iterator, Dict, List, Any, Optional\n",
        "import gc  # Garbage collection for memory management\n",
        "\n",
        "# --- Core Memory Optimization Helpers ---\n",
        "\n",
        "def iter_param_combinations(indicator_name: str, INDICATOR_PARAMS: Dict) -> Iterator[Dict[str, Any]]:\n",
        "    \"\"\"Yield parameter combinations lazily to avoid materializing large lists.\"\"\"\n",
        "    if indicator_name not in INDICATOR_PARAMS:\n",
        "        return\n",
        "    param_names = list(INDICATOR_PARAMS[indicator_name].keys())\n",
        "    param_values = list(INDICATOR_PARAMS[indicator_name].values())\n",
        "    from itertools import product\n",
        "    for combo in product(*param_values):\n",
        "        yield {param_names[i]: combo[i] for i in range(len(param_names))}\n",
        "\n",
        "\n",
        "def count_param_combinations(indicator_name: str, INDICATOR_PARAMS: Dict) -> int:\n",
        "    \"\"\"Count combinations without generating them.\"\"\"\n",
        "    if indicator_name not in INDICATOR_PARAMS:\n",
        "        return 0\n",
        "    lengths = [len(v) for v in INDICATOR_PARAMS[indicator_name].values()]\n",
        "    return reduce(mul, lengths, 1)\n",
        "\n",
        "\n",
        "def _ensure_df(results_like):\n",
        "    \"\"\"Accept list[dict] or DataFrame and return a DataFrame view.\"\"\"\n",
        "    if isinstance(results_like, list):\n",
        "        return pd.DataFrame(results_like)\n",
        "    return results_like\n",
        "\n",
        "\n",
        "# --- Optimized Calculation Functions ---\n",
        "\n",
        "def calculate_signals_vectorized(ticker_data: pd.DataFrame, signals: pd.Series) -> Dict[str, Any]:\n",
        "    \"\"\"Vectorized calculation of strategy metrics without creating temporary DataFrames.\n",
        "    Returns metrics as dict to avoid DataFrame overhead.\n",
        "    \"\"\"\n",
        "    daily_return = ticker_data['Close'].pct_change()\n",
        "    position = signals.shift(1).fillna(0).astype('float32')\n",
        "    strategy_return = (position * daily_return).astype('float32')\n",
        "\n",
        "    valid_returns = strategy_return.dropna()\n",
        "    if valid_returns.empty:\n",
        "        return {\n",
        "            'total_return': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan,\n",
        "            'win_rate': np.nan, 'total_signals': 0, 'buy_signals': 0, 'sell_signals': 0\n",
        "        }\n",
        "\n",
        "    total_return = float((1.0 + valid_returns).prod() - 1.0)\n",
        "\n",
        "    if valid_returns.std() > 1e-8:\n",
        "        sharpe = float((valid_returns.mean() / valid_returns.std()) * np.sqrt(252))\n",
        "    else:\n",
        "        sharpe = 0.0 # Or np.nan, depending on how you want to handle zero std\n",
        "\n",
        "    # Max drawdown calculation (vectorized)\n",
        "    equity = (1.0 + valid_returns).cumprod()\n",
        "    running_max = equity.cummax()\n",
        "    drawdown = (equity - running_max) / running_max\n",
        "    max_drawdown = float(drawdown.min())\n",
        "\n",
        "    win_rate = float((valid_returns > 0).mean())\n",
        "\n",
        "    return {\n",
        "        'total_return': total_return,\n",
        "        'sharpe': sharpe,\n",
        "        'max_drawdown': max_drawdown,\n",
        "        'win_rate': win_rate,\n",
        "        'total_signals': int(signals.notna().sum()),\n",
        "        'buy_signals': int((signals == 1).sum()),\n",
        "        'sell_signals': int((signals == -1).sum())\n",
        "    }\n",
        "\n",
        "\n",
        "def stream_results_to_csv(results: List[Dict], filename: str, mode: str = 'a') -> None:\n",
        "    \"\"\"Stream results to CSV without loading all data into memory.\"\"\"\n",
        "    if not results:\n",
        "        return\n",
        "\n",
        "    df_chunk = pd.DataFrame(results)\n",
        "    file_exists = os.path.exists(filename)\n",
        "\n",
        "    df_chunk.to_csv(\n",
        "        filename,\n",
        "        mode=mode,\n",
        "        header=not file_exists,\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    del df_chunk # Clear memory\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def load_data_and_apply_indicators_chunked(\n",
        "    raw_data: pd.DataFrame,\n",
        "    tickers: List[str],\n",
        "    apply_indicators_single_ticker_func: callable,\n",
        "    chunk_size: int = 10\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Memory-efficient alternative to raw_data.groupby('Ticker').apply(apply_indicators_single_ticker_func).\n",
        "    Processes tickers in chunks to control memory usage and avoids large intermediate DataFrames.\n",
        "    `apply_indicators_single_ticker_func` should accept a single ticker's DataFrame and return a processed DataFrame.\n",
        "    \"\"\"\n",
        "    processed_chunks = []\n",
        "\n",
        "    for i in range(0, len(tickers), chunk_size):\n",
        "        current_chunk_tickers = tickers[i:i + chunk_size]\n",
        "        print(f\"Processing data for tickers {i+1}-{i+len(current_chunk_tickers)} of {len(tickers)}...\")\n",
        "\n",
        "        # Filter raw_data for the current chunk of tickers\n",
        "        chunk_data_raw = raw_data[raw_data['Ticker'].isin(current_chunk_tickers)].copy()\n",
        "\n",
        "        chunk_results_list = []\n",
        "        for ticker in current_chunk_tickers:\n",
        "            ticker_df = chunk_data_raw[chunk_data_raw['Ticker'] == ticker].copy()\n",
        "            if not ticker_df.empty:\n",
        "                try:\n",
        "                    processed_ticker_df = apply_indicators_single_ticker_func(ticker_df)\n",
        "                    chunk_results_list.append(processed_ticker_df)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error applying indicators to {ticker}: {e}\")\n",
        "\n",
        "        if chunk_results_list:\n",
        "            # Concatenate results for the current chunk\n",
        "            combined_chunk_df = pd.concat(chunk_results_list, ignore_index=False) # Keep original index if meaningful\n",
        "            processed_chunks.append(combined_chunk_df)\n",
        "\n",
        "        del chunk_data_raw, chunk_results_list # Explicitly free memory\n",
        "        gc.collect()\n",
        "\n",
        "    if processed_chunks:\n",
        "        return pd.concat(processed_chunks, ignore_index=False) # Final concat of all chunks\n",
        "    return pd.DataFrame() # Return empty if no data processed\n",
        "\n",
        "\n",
        "print(\"ðŸš€ MEMORY OPTIMIZATION HELPERS LOADED! Ready for NASDAQ 100+ scale.\")\n",
        "print(\"Key features: Lazy parameter generation, vectorized calcs, streaming CSV, chunked data processing.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Core Indicator Params & Signal Logic Loaded (NO ADX, RSI, ATR, Entropy)!\n",
            "Available indicators: ['KAMA', 'Supertrend', 'MFI']\n",
            "Example KAMA combinations: 150\n",
            "Example Supertrend combinations: 18\n",
            "Example MFI combinations: 8\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define parameter ranges for each indicator (NO ADX, NO RSI, NO ATR, NO Entropy)\n",
        "INDICATOR_PARAMS = {\n",
        "    'KAMA': {\n",
        "        'er_period': list(range(5, 8)),      # Efficiency ratio periods\n",
        "        'fast_period': list(range(7, 12)),    # Fast EMA periods\n",
        "        'slow_period': list(range(15, 25)),     # Slow EMA periods\n",
        "    },\n",
        "    'Supertrend': {\n",
        "        'period': list(range(7, 16)),  # ATR period\n",
        "        'multiplier': [1.0, 2.0,]  # ATR multiplier\n",
        "    },\n",
        "    'MFI': {\n",
        "        'period': list(range(7, 15)),  # MFI period\n",
        "        'overbought': [80],  # Sell when MFI > overbought\n",
        "        'oversold': [20]  # Buy when MFI < oversold\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define buy signal logic for each indicator\n",
        "def get_buy_signal(indicator_name: str, data: pd.Series, params: dict) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Generate buy signals for different indicators.\n",
        "    Returns: 1 (buy), -1 (sell), 0 (hold), NaN (no signal)\n",
        "    \"\"\"\n",
        "    if indicator_name == 'KAMA':\n",
        "        short_kama = params.get('short_kama', pd.Series(index=data.index))\n",
        "        long_kama = params.get('long_kama', pd.Series(index=data.index))\n",
        "        \n",
        "        crossover_up = (short_kama > long_kama) & (short_kama.shift(1) <= long_kama.shift(1))\n",
        "        crossover_down = (short_kama < long_kama) & (short_kama.shift(1) >= long_kama.shift(1))\n",
        "        \n",
        "        signals = pd.Series(0, index=data.index)\n",
        "        signals[crossover_up] = 1\n",
        "        signals[crossover_down] = -1\n",
        "        \n",
        "        return signals\n",
        "\n",
        "    elif indicator_name == 'Supertrend':\n",
        "        supertrend = data\n",
        "        close = params.get('close', pd.Series(index=data.index))\n",
        "        \n",
        "        return pd.Series(np.where(close > supertrend, 1, np.where(close < supertrend, -1, 0)), index=data.index)\n",
        "\n",
        "    elif indicator_name == 'MFI':\n",
        "        overbought = params.get('overbought', 80)\n",
        "        oversold = params.get('oversold', 20)\n",
        "        return pd.Series(np.where(data < oversold, 1, np.where(data > overbought, -1, 0)), index=data.index)\n",
        "\n",
        "    return pd.Series([0] * len(data), index=data.index) # Default: no signal\n",
        "\n",
        "print(\"âœ… Core Indicator Params & Signal Logic Loaded (NO ADX, RSI, ATR, Entropy)!\")\n",
        "print(f\"Available indicators: {list(INDICATOR_PARAMS.keys())}\")\n",
        "print(f\"Example KAMA combinations: {count_param_combinations('KAMA', INDICATOR_PARAMS)}\")\n",
        "print(f\"Example Supertrend combinations: {count_param_combinations('Supertrend', INDICATOR_PARAMS)}\")\n",
        "print(f\"Example MFI combinations: {count_param_combinations('MFI', INDICATOR_PARAMS)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# OPTIMIZED GRID SEARCH FUNCTION\n",
        "# ================================================================================\n",
        "\n",
        "def run_comprehensive_grid_search_optimized(\n",
        "    data: pd.DataFrame,\n",
        "    indicator_params: Dict,\n",
        "    indicator_class,\n",
        "    max_combinations_per_indicator: int = 0,\n",
        "    stream_results_dir: Optional[str] = None,\n",
        "    summary_interval: int = 100 # Print summary every N combinations\n",
        ") -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
        "    \"\"\"\n",
        "    Memory-optimized comprehensive grid search.\n",
        "    - Iterates parameter combinations lazily.\n",
        "    - Uses vectorized calculations for signals and metrics.\n",
        "    - Stores results as list[dict] per (ticker, indicator) to save memory.\n",
        "    - Optionally streams results to CSV files incrementally.\n",
        "    \"\"\"\n",
        "    all_results: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}\n",
        "    tickers = pd.Index(data['Ticker']).unique().tolist()\n",
        "\n",
        "    print(\"\\nðŸš€ Starting Optimized Grid Search...\")\n",
        "    print(f\"Processing {len(tickers)} tickers with {len(indicator_params)} indicators.\")\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"\\nðŸ“Š Processing {ticker}...\")\n",
        "        ticker_data = data.loc[data['Ticker'] == ticker].copy()\n",
        "        ticker_data.sort_index(inplace=True) # Ensure data is sorted by date\n",
        "        \n",
        "        # Initialize Indicator class for the current ticker's data\n",
        "        indicator_obj = indicator_class(ticker_data)\n",
        "        all_results[ticker] = {}\n",
        "\n",
        "        for indicator_name, _ in indicator_params.items():\n",
        "            total_combos = count_param_combinations(indicator_name, indicator_params)\n",
        "            if total_combos == 0:\n",
        "                print(f\"    No combinations for {indicator_name}. Skipping.\")\n",
        "                continue\n",
        "            \n",
        "            current_limit = max_combinations_per_indicator if max_combinations_per_indicator > 0 else total_combos\n",
        "            if current_limit < total_combos:\n",
        "                print(f\"    ðŸ“‰ Limiting {indicator_name} to {current_limit} random combinations out of {total_combos}.\")\n",
        "                # For true random sampling, we would need to materialize a subset.\n",
        "                # For lazy iteration, we can just take the first N, or implement a reservoir sampling if needed.\n",
        "                # For now, we take the first N as iterated.\n",
        "            else:\n",
        "                print(f\"    ðŸ” Evaluating {total_combos} {indicator_name} combinations.\")\n",
        "\n",
        "            indicator_results: List[Dict[str, Any]] = []\n",
        "            processed_count = 0\n",
        "            batch_to_stream: List[Dict[str, Any]] = []\n",
        "\n",
        "            for params in iter_param_combinations(indicator_name, indicator_params):\n",
        "                processed_count += 1\n",
        "                if max_combinations_per_indicator > 0 and processed_count > max_combinations_per_indicator:\n",
        "                    break # Stop if limit reached\n",
        "                \n",
        "                if processed_count % summary_interval == 0 or processed_count == current_limit:\n",
        "                    print(f\"      -> Processed {processed_count}/{current_limit} combinations for {indicator_name}...\")\n",
        "\n",
        "                try:\n",
        "                    # Calculate indicator values\n",
        "                    # (Ensure these indicator_obj methods exist and return pd.Series)\n",
        "                    indicator_values = pd.Series([]) # Default empty Series\n",
        "                    if indicator_name == 'KAMA':\n",
        "                        indicator_values = indicator_obj.kama(\n",
        "                            er_period=params['er_period'],\n",
        "                            fast_period=params['fast_period'],\n",
        "                            slow_period=params['slow_period']\n",
        "                        )\n",
        "                    elif indicator_name == 'Supertrend':\n",
        "                        indicator_values = indicator_obj.supertrend(period=params['period'], multiplier=params['multiplier'])\n",
        "                    elif indicator_name == 'MFI':\n",
        "                        indicator_values = indicator_obj.mfi(params['period'])\n",
        "                    else:\n",
        "                        print(f\"        Unknown indicator {indicator_name}. Skipping combination.\")\n",
        "                        continue\n",
        "\n",
        "                    # Generate signals using the global get_buy_signal function\n",
        "                    # Pass ticker_data['Close'] if needed for KAMA/Supertrend signal logic\n",
        "                    signal_params = params.copy()\n",
        "                    if indicator_name == 'KAMA' or indicator_name == 'Supertrend':\n",
        "                        signal_params['close'] = ticker_data['Close']\n",
        "                    signals = get_buy_signal(indicator_name, indicator_values, signal_params)\n",
        "\n",
        "                    # Calculate metrics using the vectorized function\n",
        "                    metrics = calculate_signals_vectorized(ticker_data, signals)\n",
        "\n",
        "                    # Store results for this combination\n",
        "                    result_entry = {\n",
        "                        'Ticker': ticker,\n",
        "                        'Indicator': indicator_name,\n",
        "                        'Parameters': params,\n",
        "                        **metrics # Unpack all calculated metrics\n",
        "                    }\n",
        "                    indicator_results.append(result_entry)\n",
        "                    batch_to_stream.append(result_entry)\n",
        "\n",
        "                    # Stream results if batch size reached or last combination\n",
        "                    if stream_results_dir and (len(batch_to_stream) >= 500 or processed_count == current_limit):\n",
        "                        stream_filename = os.path.join(stream_results_dir, f\"{ticker}_{indicator_name}_results.csv\")\n",
        "                        stream_results_to_csv(batch_to_stream, stream_filename)\n",
        "                        batch_to_stream.clear() # Clear batch after streaming\n",
        "                        gc.collect()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    âŒ Error with {indicator_name} params {params}: {e}\")\n",
        "                    # Optionally log error to file\n",
        "                    continue\n",
        "            \n",
        "            # Stream any remaining results in batch_to_stream\n",
        "            if stream_results_dir and batch_to_stream:\n",
        "                stream_filename = os.path.join(stream_results_dir, f\"{ticker}_{indicator_name}_results.csv\")\n",
        "                stream_results_to_csv(batch_to_stream, stream_filename)\n",
        "                batch_to_stream.clear()\n",
        "                gc.collect()\n",
        "\n",
        "            # Store final list of results for this (ticker, indicator)\n",
        "            all_results[ticker][indicator_name] = indicator_results\n",
        "\n",
        "            # Show best result for this indicator (from in-memory list)\n",
        "            if indicator_results:\n",
        "                try:\n",
        "                    # Use _ensure_df to safely convert for analysis if needed (though it's a list here)\n",
        "                    results_df_for_display = pd.DataFrame(indicator_results)\n",
        "                    valid_sharpes = results_df_for_display['sharpe'].dropna()\n",
        "                    if not valid_sharpes.empty:\n",
        "                        best_idx = valid_sharpes.idxmax()\n",
        "                        best_result = results_df_for_display.loc[best_idx]\n",
        "                        print(f\"    âœ… Best {indicator_name} for {ticker}: Sharpe={best_result['sharpe']:.3f}, Return={best_result['total_return']:.3f}\")\n",
        "                    else:\n",
        "                        print(f\"    âš ï¸ No valid Sharpe Ratios for {indicator_name} for {ticker}.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    âš ï¸ Error showing best result for {indicator_name} for {ticker}: {e}\")\n",
        "\n",
        "        # Explicitly clear ticker_data to free memory after all indicators processed for ticker\n",
        "        del ticker_data, indicator_obj\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"\\nâœ… Optimized Grid Search Complete!\")\n",
        "    return all_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Optimized Analysis & Plotting Functions Loaded!\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# OPTIMIZED RESULTS ANALYSIS AND PLOTTING FUNCTIONS\n",
        "# ================================================================================\n",
        "\n",
        "def analyze_grid_search_results_optimized(all_results: Dict[str, Dict[str, List[Dict[str, Any]]]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Updated to accept list- or DataFrame-backed results from the optimized grid search.\n",
        "    \"\"\"\n",
        "    analysis = {\n",
        "        'best_per_ticker': {},\n",
        "        'best_indicators_overall': {},\n",
        "        'summary_stats': {},\n",
        "        'top_overall_sharpe': {},\n",
        "        'top_overall_return': {}\n",
        "    }\n",
        "\n",
        "    print(\"\\nðŸ” Analyzing Grid Search Results...\")\n",
        "\n",
        "    # Best indicator per ticker\n",
        "    for ticker, ticker_results in all_results.items():\n",
        "        best_sharpe = -np.inf\n",
        "        best_return = -np.inf\n",
        "        best_indicator = None\n",
        "        best_params = None\n",
        "        best_result_row = None\n",
        "\n",
        "        for indicator_name, results_list in ticker_results.items():\n",
        "            df = _ensure_df(results_list)\n",
        "            if df.empty or 'sharpe' not in df.columns or 'total_return' not in df.columns:\n",
        "                continue\n",
        "            \n",
        "            # Ensure numeric columns are float and handle NaNs\n",
        "            df['sharpe'] = pd.to_numeric(df['sharpe'], errors='coerce')\n",
        "            df['total_return'] = pd.to_numeric(df['total_return'], errors='coerce')\n",
        "\n",
        "            # Find best sharpe within this indicator for this ticker\n",
        "            max_sharpe_idx = df['sharpe'].idxmax()\n",
        "            if pd.notna(max_sharpe_idx):\n",
        "                current_result_row = df.loc[max_sharpe_idx]\n",
        "                current_sharpe = current_result_row['sharpe']\n",
        "                current_return = current_result_row['total_return']\n",
        "\n",
        "                if current_sharpe > best_sharpe:\n",
        "                    best_sharpe = current_sharpe\n",
        "                    best_return = current_return\n",
        "                    best_indicator = indicator_name\n",
        "                    best_params = current_result_row['Parameters']\n",
        "                    best_result_row = current_result_row.to_dict()\n",
        "\n",
        "        if best_indicator:\n",
        "            analysis['best_per_ticker'][ticker] = {\n",
        "                'indicator': best_indicator,\n",
        "                'sharpe': best_sharpe,\n",
        "                'total_return': best_return,\n",
        "                'params': best_params,\n",
        "                'full_result': best_result_row\n",
        "            }\n",
        "            print(f\"  {ticker}: Best is {best_indicator} (Sharpe: {best_sharpe:.3f}, Return: {best_return:.3f})\")\n",
        "        else:\n",
        "            analysis['best_per_ticker'][ticker] = {'indicator': None, 'sharpe': np.nan, 'total_return': np.nan, 'params': None, 'full_result': None}\n",
        "            print(f\"  {ticker}: No valid best indicator found.\")\n",
        "\n",
        "    # Best performing indicators overall (average Sharpe)\n",
        "    indicator_performance: Dict[str, List[float]] = {}\n",
        "    all_sharpes_overall: List[float] = []\n",
        "    all_returns_overall: List[float] = []\n",
        "\n",
        "    for _ticker, ticker_results in all_results.items():\n",
        "        for indicator_name, results_list in ticker_results.items():\n",
        "            df = _ensure_df(results_list)\n",
        "            if df.empty or 'sharpe' not in df.columns or 'total_return' not in df.columns:\n",
        "                continue\n",
        "            \n",
        "            # Ensure numeric columns and filter NaNs\n",
        "            sharpes = pd.to_numeric(df['sharpe'], errors='coerce').dropna().tolist()\n",
        "            returns = pd.to_numeric(df['total_return'], errors='coerce').dropna().tolist()\n",
        "            \n",
        "            if sharpes: # Only add if there are valid sharpe values\n",
        "                indicator_performance.setdefault(indicator_name, []).extend(sharpes)\n",
        "                all_sharpes_overall.extend(sharpes)\n",
        "                all_returns_overall.extend(returns) # Also collect returns for overall stats\n",
        "\n",
        "    best_indicators_overall = {\n",
        "        k: (np.nanmean(v) if len(v) > 0 else np.nan) for k, v in indicator_performance.items()\n",
        "    }\n",
        "    analysis['best_indicators_overall'] = dict(sorted(best_indicators_overall.items(), key=lambda x: (-np.nan_to_num(x[1]), x[0])))\n",
        "\n",
        "    # Top overall Sharpe and Return (across all tickers, all indicators)\n",
        "    if all_sharpes_overall and all_returns_overall:\n",
        "        temp_df = pd.DataFrame({'sharpe': all_sharpes_overall, 'total_return': all_returns_overall})\n",
        "        if not temp_df.empty:\n",
        "            top_sharpe_idx = temp_df['sharpe'].idxmax()\n",
        "            top_return_idx = temp_df['total_return'].idxmax()\n",
        "            analysis['top_overall_sharpe'] = temp_df.loc[top_sharpe_idx].to_dict()\n",
        "            analysis['top_overall_return'] = temp_df.loc[top_return_idx].to_dict()\n",
        "\n",
        "    # Summary statistics\n",
        "    analysis['summary_stats'] = {\n",
        "        'num_tickers_processed': len(all_results),\n",
        "        'num_indicators_evaluated': len(indicator_performance),\n",
        "        'avg_sharpe_across_all_combos': float(np.nanmean(all_sharpes_overall)) if len(all_sharpes_overall) else np.nan,\n",
        "        'avg_return_across_all_combos': float(np.nanmean(all_returns_overall)) if len(all_returns_overall) else np.nan,\n",
        "    }\n",
        "    print(\"âœ… Analysis Complete.\")\n",
        "    return analysis\n",
        "\n",
        "\n",
        "def create_results_summary_table_optimized(\n",
        "    all_results: Dict[str, Dict[str, List[Dict[str, Any]]]],\n",
        "    output_file: Optional[str] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a flat summary table from optimized grid search results, accepting list-backed results.\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ“ Creating Summary Table...\")\n",
        "    summary_rows: List[Dict[str, Any]] = []\n",
        "    for ticker, ticker_results in all_results.items():\n",
        "        for indicator_name, results_list in ticker_results.items():\n",
        "            df = _ensure_df(results_list)\n",
        "            if df.empty:\n",
        "                continue\n",
        "            for _, row in df.iterrows():\n",
        "                summary_rows.append({\n",
        "                    'Ticker': ticker,\n",
        "                    'Indicator': indicator_name,\n",
        "                    'Parameters': row.get('Parameters'),\n",
        "                    'Sharpe_Ratio': row.get('sharpe'),\n",
        "                    'Total_Return': row.get('total_return'),\n",
        "                    'Max_Drawdown': row.get('max_drawdown'),\n",
        "                    'Win_Rate': row.get('win_rate'),\n",
        "                    'Total_Signals': row.get('total_signals'),\n",
        "                    'Buy_Signals': row.get('buy_signals'),\n",
        "                    'Sell_Signals': row.get('sell_signals'),\n",
        "                })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    if output_file:\n",
        "        os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
        "        summary_df.to_csv(output_file, index=False)\n",
        "        print(f\"ðŸ“Š Summary results saved to {output_file}\")\n",
        "    print(\"âœ… Summary Table Created.\")\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "def find_similar_parameter_combinations_optimized(\n",
        "    results_like,\n",
        "    target_params: Dict[str, Any],\n",
        "    max_distance: float = 5.0\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Find similar parameter combinations, accepting list or DataFrame for results.\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ”Ž Finding similar parameters to {target_params} within distance {max_distance}...\")\n",
        "    df = _ensure_df(results_like)\n",
        "    if df.empty or 'Parameters' not in df.columns:\n",
        "        print(\"No valid results or 'Parameters' column found for similarity search.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    similar_results: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Need to redefine calculate_param_distance or move it to a common helper if it's not globally available\n",
        "    # Assuming calculate_param_distance is available globally or can be defined here if needed.\n",
        "    # For now, let's assume it's globally available or define it locally if not. It was removed in previous steps.\n",
        "    # Re-adding a basic version here for self-containment if not present.\n",
        "    def _calculate_param_distance(p1: dict, p2: dict) -> float:\n",
        "        if not p1 or not p2:\n",
        "            return float('inf')\n",
        "        common_keys = set(p1.keys()) & set(p2.keys())\n",
        "        if not common_keys:\n",
        "            return float('inf')\n",
        "        distance = 0.0\n",
        "        for key in common_keys:\n",
        "            try:\n",
        "                # Only compare numeric values\n",
        "                val1 = float(p1[key])\n",
        "                val2 = float(p2[key])\n",
        "                distance += (val1 - val2) ** 2\n",
        "            except (ValueError, TypeError):\n",
        "                # Non-numeric or missing keys are not included in distance calculation\n",
        "                continue\n",
        "        return distance ** 0.5\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            distance = _calculate_param_distance(row['Parameters'], target_params)\n",
        "            if distance <= max_distance:\n",
        "                row_copy = row.to_dict()\n",
        "                row_copy['Parameter_Distance'] = distance\n",
        "                similar_results.append(row_copy)\n",
        "        except Exception as e:\n",
        "            print(f\"    Error calculating distance for params {row.get('Parameters')}: {e}\")\n",
        "            continue\n",
        "\n",
        "    results_df = pd.DataFrame(similar_results).sort_values('Parameter_Distance')\n",
        "    print(f\"âœ… Found {len(results_df)} similar combinations.\")\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def plot_indicator_performance_comparison_optimized(\n",
        "    all_results: Dict[str, Dict[str, List[Dict[str, Any]]]],\n",
        "    metric: str = 'sharpe' # Changed to 'sharpe' to match optimized metrics\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create comparison plots of indicator performance, accepting list-backed results.\n",
        "    Only builds a small DataFrame for plotting.\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ“ˆ Preparing plot data for metric: {metric}...\")\n",
        "    plot_data: List[Dict[str, Any]] = []\n",
        "    for ticker, ticker_results in all_results.items():\n",
        "        for indicator_name, results_list in ticker_results.items():\n",
        "            df = _ensure_df(results_list)\n",
        "            if df.empty or metric not in df.columns:\n",
        "                continue\n",
        "            \n",
        "            # Convert metric column to numeric and drop NaNs\n",
        "            valid_metric_values = pd.to_numeric(df[metric], errors='coerce').dropna()\n",
        "            for val in valid_metric_values.tolist():\n",
        "                plot_data.append({\n",
        "                    'Ticker': ticker,\n",
        "                    'Indicator': indicator_name,\n",
        "                    metric: val,\n",
        "                })\n",
        "\n",
        "    if not plot_data:\n",
        "        print(\"No data to plot for the specified metric.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame if no data\n",
        "\n",
        "    plot_df = pd.DataFrame(plot_data)\n",
        "    print(\"âœ… Plot data prepared.\")\n",
        "    return plot_df\n",
        "\n",
        "print(\"âœ… Optimized Analysis & Plotting Functions Loaded!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Chunked Data Processing Function Loaded!\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# MEMORY-EFFICIENT DATA LOADING AND INDICATOR APPLICATION\n",
        "# ================================================================================\n",
        "\n",
        "def load_data_and_apply_indicators_chunked(\n",
        "    raw_data: pd.DataFrame,\n",
        "    tickers: List[str],\n",
        "    apply_indicators_single_ticker_func: callable,\n",
        "    chunk_size: int = 10\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Memory-efficient alternative to raw_data.groupby('Ticker').apply(apply_indicators_single_ticker_func).\n",
        "    Processes tickers in chunks to control memory usage and avoids large intermediate DataFrames.\n",
        "    `apply_indicators_single_ticker_func` should accept a single ticker's DataFrame and return a processed DataFrame.\n",
        "    \"\"\"\n",
        "    processed_chunks = []\n",
        "\n",
        "    print(f\"\\nðŸ“¦ Starting chunked data processing for {len(tickers)} tickers...\")\n",
        "\n",
        "    for i in range(0, len(tickers), chunk_size):\n",
        "        current_chunk_tickers = tickers[i:i + chunk_size]\n",
        "        print(f\"  Processing tickers {i+1}-{i+len(current_chunk_tickers)} of {len(tickers)} in chunk.\")\n",
        "\n",
        "        # Filter raw_data for the current chunk of tickers\n",
        "        chunk_data_raw = raw_data[raw_data['Ticker'].isin(current_chunk_tickers)].copy()\n",
        "\n",
        "        chunk_results_list = []\n",
        "        for ticker in current_chunk_tickers:\n",
        "            ticker_df = chunk_data_raw[chunk_data_raw['Ticker'] == ticker].copy()\n",
        "            if not ticker_df.empty:\n",
        "                try:\n",
        "                    processed_ticker_df = apply_indicators_single_ticker_func(ticker_df)\n",
        "                    chunk_results_list.append(processed_ticker_df)\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error applying indicators to {ticker}: {e}\")\n",
        "\n",
        "        if chunk_results_list:\n",
        "            # Concatenate results for the current chunk\n",
        "            # Using ignore_index=False to preserve original dates/indices if desired\n",
        "            combined_chunk_df = pd.concat(chunk_results_list, ignore_index=False)\n",
        "            processed_chunks.append(combined_chunk_df)\n",
        "\n",
        "        del chunk_data_raw, chunk_results_list # Explicitly free memory\n",
        "        gc.collect()\n",
        "\n",
        "    if processed_chunks:\n",
        "        print(\"\\nâœ… All chunks processed. Final concatenation...\")\n",
        "        return pd.concat(processed_chunks, ignore_index=False) # Final concat of all chunks\n",
        "    print(\"\\nâš ï¸ No data processed. Returning empty DataFrame.\")\n",
        "    return pd.DataFrame() # Return empty if no data processed\n",
        "\n",
        "print(\"âœ… Chunked Data Processing Function Loaded!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lightweight helpers and redefinitions to reduce memory without changing outputs\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from operator import mul\n",
        "\n",
        "\n",
        "def iter_param_combinations(indicator_name: str):\n",
        "    \"\"\"Yield parameter combinations lazily to avoid materializing large lists.\"\"\"\n",
        "    if indicator_name not in INDICATOR_PARAMS:\n",
        "        return\n",
        "    param_names = list(INDICATOR_PARAMS[indicator_name].keys())\n",
        "    param_values = list(INDICATOR_PARAMS[indicator_name].values())\n",
        "    from itertools import product\n",
        "    for combo in product(*param_values):\n",
        "        yield {param_names[i]: combo[i] for i in range(len(param_names))}\n",
        "\n",
        "\n",
        "def count_param_combinations(indicator_name: str) -> int:\n",
        "    \"\"\"Count combinations without generating them.\"\"\"\n",
        "    if indicator_name not in INDICATOR_PARAMS:\n",
        "        return 0\n",
        "    lengths = [len(v) for v in INDICATOR_PARAMS[indicator_name].values()]\n",
        "    return reduce(mul, lengths, 1)\n",
        "\n",
        "\n",
        "def _ensure_df(results_like):\n",
        "    \"\"\"Accept list[dict] or DataFrame and return a DataFrame view.\"\"\"\n",
        "    if isinstance(results_like, list):\n",
        "        return pd.DataFrame(results_like)\n",
        "    return results_like\n",
        "\n",
        "\n",
        "def run_comprehensive_grid_search(data: pd.DataFrame, indicator_params: dict,\n",
        "                                  indicator_class,\n",
        "                                  max_combinations_per_indicator: int = 0,\n",
        "                                  stream_results_dir: str | None = None) -> dict:\n",
        "    \"\"\"\n",
        "    Memory-light reimplementation:\n",
        "    - Iterates parameter combinations lazily\n",
        "    - Stores results as list[dict] per (ticker, indicator)\n",
        "    - Avoids building temporary DataFrames inside the inner loop\n",
        "    - Optional streaming append to CSV per (ticker, indicator)\n",
        "    \"\"\"\n",
        "    all_results: dict[str, dict[str, list[dict]]] = {}\n",
        "    tickers = pd.Index(data['Ticker']).unique().tolist()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"\\nðŸ“Š Processing {ticker}...\")\n",
        "        ticker_data = data.loc[data['Ticker'] == ticker].copy()\n",
        "        ticker_data.sort_index(inplace=True)\n",
        "        indicator_obj = indicator_class(ticker_data)\n",
        "        all_results[ticker] = {}\n",
        "\n",
        "        for indicator_name, _ in indicator_params.items():\n",
        "            total_combos = count_param_combinations(indicator_name)\n",
        "            if total_combos == 0:\n",
        "                continue\n",
        "            limit = max_combinations_per_indicator or 0\n",
        "            if limit and limit > 0 and limit < total_combos:\n",
        "                print(f\"    ðŸ“‰ Limiting to first {limit} of {total_combos} {indicator_name} combinations\")\n",
        "            else:\n",
        "                print(f\"    ðŸ” Evaluating {total_combos} {indicator_name} combinations\")\n",
        "\n",
        "            indicator_results: list[dict] = []\n",
        "            processed = 0\n",
        "\n",
        "            for params in iter_param_combinations(indicator_name):\n",
        "                processed += 1\n",
        "                if limit and processed > limit:\n",
        "                    break\n",
        "                try:\n",
        "                    # Calculate indicator values based on parameters\n",
        "                    if indicator_name == 'RSI':\n",
        "                        indicator_values = indicator_obj.rsi(params['period'])\n",
        "                    elif indicator_name == 'ADX':\n",
        "                        indicator_values = indicator_obj.adx(params['period'])\n",
        "                    elif indicator_name == 'KAMA':\n",
        "                        indicator_values = indicator_obj.kama(\n",
        "                            er_period=params['er_period'],\n",
        "                            fast_period=params['fast_period'],\n",
        "                            slow_period=params['slow_period']\n",
        "                        )\n",
        "                    elif indicator_name == 'ATR':\n",
        "                        indicator_values = indicator_obj.atr(params['period'])\n",
        "                    elif indicator_name == 'MFI':\n",
        "                        indicator_values = indicator_obj.mfi(params['period'])\n",
        "                    elif indicator_name == 'Supertrend':\n",
        "                        indicator_values = indicator_obj.supertrend(period=params['period'], multiplier=params['multiplier'])\n",
        "                    elif indicator_name == 'Entropy':\n",
        "                        indicator_values = indicator_obj.entropy(params['period'])\n",
        "                    else:\n",
        "                        # If unknown indicator, skip\n",
        "                        continue\n",
        "\n",
        "                    # Generate signals without building a temporary DataFrame\n",
        "                    if indicator_name == 'KAMA':\n",
        "                        _params = dict(params)\n",
        "                        _params['close'] = ticker_data['Close']\n",
        "                        signals = get_buy_signal(indicator_name, indicator_values, _params)\n",
        "                    else:\n",
        "                        signals = get_buy_signal(indicator_name, indicator_values, params)\n",
        "\n",
        "                    # Strategy returns using Series-only math\n",
        "                    daily_return = ticker_data['Close'].pct_change()\n",
        "                    position = signals.shift(1).fillna(0).astype('float32')\n",
        "                    strategy_return = (position * daily_return).astype('float32')\n",
        "\n",
        "                    valid_returns = strategy_return.dropna()\n",
        "                    if valid_returns.size > 0:\n",
        "                        total_return = float((1.0 + valid_returns).prod() - 1.0)\n",
        "                        std = float(valid_returns.std())\n",
        "                        sharpe = float((valid_returns.mean() / std) * np.sqrt(252)) if std > 0 else 0.0\n",
        "                        equity = (1.0 + valid_returns).cumprod()\n",
        "                        max_drawdown = calculate_max_drawdown(equity)\n",
        "                        win_rate = float((valid_returns > 0).mean())\n",
        "                    else:\n",
        "                        total_return = np.nan\n",
        "                        sharpe = np.nan\n",
        "                        max_drawdown = np.nan\n",
        "                        win_rate = np.nan\n",
        "\n",
        "                    res = {\n",
        "                        'Indicator': indicator_name,\n",
        "                        'Parameters': params,\n",
        "                        'Total_Return': total_return,\n",
        "                        'Sharpe_Ratio': sharpe,\n",
        "                        'Max_Drawdown': max_drawdown,\n",
        "                        'Win_Rate': win_rate,\n",
        "                        'Total_Signals': int(signals.notna().sum()),\n",
        "                        'Buy_Signals': int((signals == 1).sum()),\n",
        "                        'Sell_Signals': int((signals == -1).sum()),\n",
        "                    }\n",
        "\n",
        "                    indicator_results.append(res)\n",
        "\n",
        "                    # Optional streaming append\n",
        "                    if stream_results_dir is not None:\n",
        "                        os.makedirs(stream_results_dir, exist_ok=True)\n",
        "                        ticker_filename = os.path.join(stream_results_dir, f\"{ticker}_{indicator_name}.csv\")\n",
        "                        pd.DataFrame([res]).to_csv(\n",
        "                            ticker_filename,\n",
        "                            mode='a',\n",
        "                            header=not os.path.exists(ticker_filename),\n",
        "                            index=False,\n",
        "                        )\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    âŒ Error with {indicator_name} params {params}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Store results as list (not DataFrame) to save memory\n",
        "            all_results[ticker][indicator_name] = indicator_results\n",
        "\n",
        "            # Show best result for this indicator\n",
        "            if indicator_results:\n",
        "                try:\n",
        "                    valid = [r for r in indicator_results if pd.notna(r.get('Sharpe_Ratio'))]\n",
        "                    if valid:\n",
        "                        best = max(valid, key=lambda x: x['Sharpe_Ratio'])\n",
        "                        print(f\"    âœ… Best {indicator_name}: Sharpe={best['Sharpe_Ratio']:.3f}, Return={best['Total_Return']:.3f}\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "def analyze_grid_search_results(all_results: dict) -> dict:\n",
        "    \"\"\"Updated to accept list- or DataFrame-backed results.\"\"\"\n",
        "    analysis = {\n",
        "        'best_per_ticker': {},\n",
        "        'best_indicators': {},\n",
        "        'parameter_distances': {},\n",
        "        'summary_stats': {}\n",
        "    }\n",
        "\n",
        "    # Best indicator per ticker\n",
        "    for ticker, ticker_results in all_results.items():\n",
        "        best_sharpe = -999.0\n",
        "        best_indicator = None\n",
        "        best_params = None\n",
        "\n",
        "        for indicator_name, results_like in ticker_results.items():\n",
        "            df = _ensure_df(results_like)\n",
        "            if df is None or df.empty or 'Sharpe_Ratio' not in df.columns:\n",
        "                continue\n",
        "            idx = df['Sharpe_Ratio'].astype(float).idxmax()\n",
        "            if pd.notna(idx):\n",
        "                current_sharpe = float(df.loc[idx, 'Sharpe_Ratio'])\n",
        "                if current_sharpe > best_sharpe:\n",
        "                    best_sharpe = current_sharpe\n",
        "                    best_indicator = indicator_name\n",
        "                    best_params = df.loc[idx, 'Parameters']\n",
        "\n",
        "        analysis['best_per_ticker'][ticker] = {\n",
        "            'indicator': best_indicator, 'sharpe': best_sharpe, 'params': best_params\n",
        "        }\n",
        "\n",
        "    # Best performing indicators overall\n",
        "    indicator_perf: dict[str, list[float]] = {}\n",
        "    for _ticker, ticker_results in all_results.items():\n",
        "        for indicator_name, results_like in ticker_results.items():\n",
        "            df = _ensure_df(results_like)\n",
        "            if df is None or df.empty or 'Sharpe_Ratio' not in df.columns:\n",
        "                continue\n",
        "            indicator_perf.setdefault(indicator_name, []).extend(\n",
        "                df['Sharpe_Ratio'].astype(float).dropna().tolist()\n",
        "            )\n",
        "\n",
        "    best_indicators = {\n",
        "        k: (np.nanmean(v) if len(v) else np.nan) for k, v in indicator_perf.items()\n",
        "    }\n",
        "\n",
        "    analysis['best_indicators'] = dict(sorted(best_indicators.items(), key=lambda x: (-np.nan_to_num(x[1]), x[0])))\n",
        "\n",
        "    # Summary statistics\n",
        "    all_sharpes = [s for v in indicator_perf.values() for s in v]\n",
        "    analysis['summary_stats'] = {\n",
        "        'num_tickers': len(all_results),\n",
        "        'num_indicators': len(indicator_perf),\n",
        "        'avg_sharpe_overall': float(np.nanmean(all_sharpes)) if len(all_sharpes) else np.nan,\n",
        "    }\n",
        "\n",
        "    return analysis\n",
        "\n",
        "\n",
        "def create_results_summary_table(all_results: dict, output_file: str | None = None) -> pd.DataFrame:\n",
        "    \"\"\"Create a flat summary accepting list- or DataFrame-backed results.\"\"\"\n",
        "    summary_rows: list[dict] = []\n",
        "    for ticker, ticker_results in all_results.items():\n",
        "        for indicator_name, results_like in ticker_results.items():\n",
        "            df = _ensure_df(results_like)\n",
        "            if df is None or df.empty:\n",
        "                continue\n",
        "            for _, row in df.iterrows():\n",
        "                summary_rows.append({\n",
        "                    'Ticker': ticker,\n",
        "                    'Indicator': indicator_name,\n",
        "                    'Parameters': row.get('Parameters'),\n",
        "                    'Sharpe_Ratio': row.get('Sharpe_Ratio'),\n",
        "                    'Total_Return': row.get('Total_Return'),\n",
        "                    'Max_Drawdown': row.get('Max_Drawdown'),\n",
        "                    'Win_Rate': row.get('Win_Rate'),\n",
        "                    'Total_Signals': row.get('Total_Signals'),\n",
        "                })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    if output_file:\n",
        "        summary_df.to_csv(output_file, index=False)\n",
        "        print(f\"ðŸ“Š Results saved to {output_file}\")\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "def find_similar_parameter_combinations(results_like, target_params: dict, max_distance: float = 5.0) -> pd.DataFrame:\n",
        "    \"\"\"Accept list or DataFrame for results_like; compute Euclidean distance over shared params.\"\"\"\n",
        "    df = _ensure_df(results_like)\n",
        "    if df is None or df.empty or 'Parameters' not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    similar_results = []\n",
        "    for _, row in df.iterrows():\n",
        "        try:\n",
        "            distance = calculate_param_distance(row['Parameters'], target_params)\n",
        "            if distance <= max_distance:\n",
        "                row_copy = row.to_dict()\n",
        "                row_copy['Parameter_Distance'] = distance\n",
        "                similar_results.append(row_copy)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(similar_results).sort_values('Parameter_Distance')\n",
        "\n",
        "\n",
        "def plot_indicator_performance_comparison(all_results: dict, metric: str = 'Sharpe_Ratio'):\n",
        "    \"\"\"Plot comparison accepting list-backed results; only builds a small DataFrame for plotting.\"\"\"\n",
        "    plot_data: list[dict] = []\n",
        "    for ticker, ticker_results in all_results.items():\n",
        "        for indicator_name, results_like in ticker_results.items():\n",
        "            df = _ensure_df(results_like)\n",
        "            if df is None or df.empty or metric not in df.columns:\n",
        "                continue\n",
        "            valid_results = df[[metric]].dropna()\n",
        "            for val in valid_results[metric].tolist():\n",
        "                plot_data.append({\n",
        "                    'Ticker': ticker,\n",
        "                    'Indicator': indicator_name,\n",
        "                    metric: val,\n",
        "                })\n",
        "\n",
        "    if not plot_data:\n",
        "        print(\"No data to plot.\")\n",
        "        return\n",
        "\n",
        "    plot_df = pd.DataFrame(plot_data)\n",
        "    # The actual plotting code (e.g., seaborn boxplot) can remain unchanged downstream\n",
        "    return plot_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fetching Full Portfolio Data ---\n",
            "Fetching QQQ (full history)...\n",
            "Successfully fetched 1922 records for QQQ\n",
            "\n",
            "âœ… All available data fetched and combined successfully!\n",
            "Combined data shape: (1922, 6)\n",
            "Tickers in combined data: ['QQQ']\n",
            "Data covers: 2018-01-02 to 2025-08-25\n",
            "\n",
            "--- Plotting Individual Ticker Performance ---\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAEWCAYAAAApeJ66AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAU7JJREFUeJzt3Qd8U9UXB/BD9560hdLB3qNQ9t4IiCi4UJZ/cAAyBQVFBERx4wIcIDhAEARU9pINsmfZs6UtLaOL7jb/z7lpxktSOrNeft/PJyR5eUnebUrfyb3nnltBoVAoCAAAAMAE7EzxJgAAAAAMgQcAAACYDAIPAAAAMBkEHgAAAGAyCDwAAADAZBB4AAAAgMkg8AAAAACTQeABAAAAJoPAAwAAAEwGgQcAAACYDAIPACgX586do8GDB1OVKlXI2dmZgoODxf2oqKhy2b+0zwEAy1IBa7UAQFmtWbOGBg0aRH5+fjRixAiqVq0a3bhxgxYvXkz379+nlStXUv/+/Uu9f2mfAwCWB4EHAJTJ1atXqXHjxhQWFkZ79uyhgIAA9WN3796lDh06UExMDJ0+fVoECyXdvzTvAQCWC0MtAFAmn376KaWnp9MPP/wgCQhYxYoV6fvvv6e0tDSxX2n2L+1zAMAyoccDAMqE8y2cnJzo+vXrhe7DvRC5ubkUHR1d4v1L8x4AYLnQ4wEApZacnEyxsbHUpEmTR+7HwyQ8FFLS/VNTU0v1HACwXAg8AKDUVCd5T0/PR+6nepyDiJLsz69f0vdA4AFg2RzMfQAAYL2Ke7LnxytUqEDe3t4l2p/zNzIyMkr8HACwXMjxAIAyUdXUuHbt2iPzL/Ly8ujWrVsl3r807wEAlgtDLQBQJv369RNJn/v27TP4+N69e0W9jWeeeaZU+5f2OQBgmdDjAQBlcuXKFZH4WbVqVVFjw9/fX/0YF/ZS1dg4efKk6JUo6f6leQ8AsFwIPACgzP78809RVZTzK3Srij548IBWrFhBTzzxRKn3L+1zAMDyIPAAgHJx9uxZmjt3Lu3cuZMSEhIoPz+fXFxc6NixY1S/fv0y71/a5wCAZUHgAQBG8csvv9Dw4cPFIm58u7z3L+1zAMC8MJ0WAIxi6NChFBcXR1OnTqWQkBD68MMPy3X/0j4HAMwLPR4AAABgMphOCwAAACaDwAMAAABsJ/C4ffu2SAzjefmurq7UqFEjOnr0qPpxHgmaMWMGVa5cWTzevXt3unz5suQ1eB7/iy++SF5eXuTj4yOm2vES2QAAAGBZzBp48Nz7du3akaOjI23atImioqLo888/J19fX/U+n3zyCX399df03Xff0X///Ufu7u7Uq1cvyszMVO/DQce5c+do27ZttH79elFg6JVXXjFTqwAAAMAik0s5E33//v2i3LEhfGjBwcH0xhtv0OTJk9WrWwYFBdHSpUvp+eefp/Pnz4v5+0eOHKHmzZuLfTZv3kx9+vQRlQz5+UXhWgC87DYveMWLTAEAANgShUIhFlrkc6adnZH7JBRmVK9ePcWECRMUTz/9tCIgIEARERGh+OGHH9SPX716lYMixYkTJyTP69ixo2LcuHHi9uLFixU+Pj6Sx3NychT29vaKNWvWGHzfzMxMRXJysvoSFRUl3gcXXHDBBRdcbPkSHR2tMDaz1vHglSYXLlxIkyZNorffflv0WowbN46cnJxo2LBhFB8fL/bjHg5tfF/1GF8HBgZKHndwcCA/Pz/1Prq48uGsWbP0th8/fpw8PDzK3C7uQUlJSRE5J0aPHE1Iju2SY5vk2i45tkmu7ZJjm+TertjYWOratavo+Tc2B3M3lodHVEV/mjZtKkoicz4HBx7GMm3aNBHsqPAvUmhoqFj7gX+hyqNdiYmJFBAQILtfTrm1S45tkmu75NgmubZLjm2Se7sqFKQZmCLdwKyBB89U0V1foV69emIxKFapUiVxfefOHbGvCt+PiIhQ78NrNmjLzc0VM11Uz9fl7OwsLrr4F6m8fpn4wyvP17MUcmyXHNsk13bJsU1ybZcc2yT3dpmKWX9yPKPl4sWLkm2XLl2i8PBwcZt7IDh42LFjh6R3gme3tGnTRtzn66SkJLFIlAovIMURXKtWrUzWFgAAALDwHo+JEydS27ZtxVDLs88+S4cPH6YffvhBXFQR2IQJE2jOnDlUq1YtEYi8++67Iuv2ySefVPeQPPbYY/Tyyy+LIZqcnBx6/fXXxYyX4sxoAQAAABsJPFq0aEFr164VORezZ88WgcWXX34p6nKovPnmm/Tw4UNRl4N7Ntq3by+my/JS2CrLli0TwUa3bt1E99fAgQNF7Y/yxD0o2dnZxd6XAyCuNSKn7rjStovrtNjb2xv12AAAwDpgkbiC4Rtvb29RI8RQcikHHNevXxcn3uLgHynvyydnOdUFKUu7uKIsD5tZ2s+D28M5QjwzSm5BotzaJcc2ybVdcmyT3Nt19epVql27dqHnQdn0eFjLyZaX3eZv7DzzpTi/bPwcTnDlab2WdqIti9K0i5+Tnp6uTgDWThIGAICSyc9X9hXY2VnvuQWBRxH4RMsnTs4XcXNzK9ZzEHhI8Ro7TPVNAcMuAAAll5qZQ32+3kv+7s60+rU25GBvnb0u1nnUJpSXlyeuuagZlJ4qaOMcEQAAKLmT0UkUfT9DXP9xNIasFQKPYpJTz4U54OcHAFA2/127r769+ZzhytzWAIEHAACAFfj23yvq23suJYrhb2uEwAMAAMAKzdt2iawRAg+Zi46Opv/9738iOZbzVLgq7Pjx4+nevXuS/c6dOyeKuPEaBFxOnqdVzZgxQyTW6jpw4AD16dOHfH19RT2VRo0a0RdffKHOhwEAgPJz4+5Dqjp1g972r3deoezc4pV5sCQIPGSMV//lRfguX75Mv//+O125ckVUd+US9FxqntezYYcOHRLl5bleyYYNG0TZ+g8++ICWLl1KPXr0kBROW7duHXXu3JlCQkLo33//pQsXLohAhqvLcrVYa+36AwCwVE8t2G9w+7cvNCUnB+s7jWM6rYyNGTNG9HJs3bpVPaU1LCxMrAJco0YNeuedd2jBggU0YsQIUXp+zZo16jol3DPCvR6877x58+itt94SFWRHjRpFTzzxhLqsPRs5ciQFBQWJ7X/88Qc999xzZmszAICc/HshgR6kG54NGOChv9ipNbC+UMnMREGs7FyzXErSm8C9GVu2bKHRo0ergw4VriDKZelXrlxJJ0+epKioKJo0aZJecbQmTZpQ9+7dRW8J4wCGh2jeeOMNvffr16+fCFRU+wIAQNmLhb209Eihj1f2lv5ttxbo8SihjJw8qj9ji1neO2p2L3JzKt5HxsMrHKhwT4YhvP3BgwdiWEV1v7D99u3bJ24XtW/dunXV+wAAQNkkpmUZ3L51YkdKTM2iMP/iFbW0NAg8ZK64vSSP2k+3eFpJ9gUAgJL751QsfbTpgt72QE9nqh3kKS7WCoFHCbk62oueB3OUTOf3Lq6aNWuK9z5//jw99dRTeo/zdp7BUqtWLfV9zucwtB8PoTDtfdu1a2dw34iIiBK1CQAApOKTM2ns7yfU9zvUqkh7L98Vt59tHkrWDjkeJcQncx7uMMelJEGMv7+/mJHCyaMZGRmSx+Lj42nZsmU0fPhwEWzwEAknkOquvnvq1Cnavn272I/16tWL/Pz8xNRZXX///bcY3lHtCwAApXM5IVVyf3LPOurbkVV9ydoh8JCxb7/9lrKyskTAsGfPHlHTY/PmzSIgUdXp4GBm0aJFIsF04MCBdPjwYbp16xatWrVKJIzyc1999VXxeu7u7jR//nz666+/6JVXXqHTp0/TjRs3aPHixSLgePnll0V9DwAAKL0hiw+rb7es6kdNQn3on9fb02fPNKEudQLJ2iHwkDEeGjly5AhVr15dFAfjKbK9e/cWQcf+/fvJw8ND7MfDJlzLg1eN5cd5P96/f//+9M8//0hWk+XgZOfOnSI46dChA1WrVk1Mp506dapkii0AAJTdbyNbietGId70dGQIyQECD5mrWrWqKATGwys8lMK9HDwtlnsrtHH10dWrV4vpslyBlHsw/vzzT7p+/brea3LAwT0nycnJYhinZ8+e4j0SExNN2DIAAPlJy8pV3/ZwdrDKAmFFkV+L4JFmzZpFX3/9tejh0M3pUOF6Hjx8wkXD9u7d+8jX45LpPPQydOhQMZwDAACll5CSqb69eUIHkiPMarFBL730UpH7cPDBpdCLg4MPHmoBAICyuZOirN1RvaI7hfhaZ52OoqDHAwAAoIzWnbhN09acpozssi2WeSUxTVxba3Gw4kDgAQAAUIiHWbk0YcUJ2nouvtB9Np+NpwkrT9Lvh6Np2/k7ZXq/lIwcdaEwucJQCwAAQCEW77tO607Gisu1D3sb3OfoDeVK3yw2SVo3qbhu3ntInT7dpb5f3OUxrBF6PIoJy72XTWGJrAAAluxygnLoQ9eha/fo7bVnxNBKUkEvBcvKKd3fut5fSRP5XZ2KX6na2sg3pConjo6OosgWTxXlEuPFqR5qrJLp5laadvFzsrOzxc+PE1axlgsAWJPrdzWBx/rTcdSykjIgeP6HQ+opr6uPxaj3ycrNK9UqtOk6uSF+bvL9W4nAowhcPCskJIRiYmJElc7inmz5Gz6faOUWeJS2XW5ubhQWFiaeCwBgDfLyFXT2dor6/rgVJ+nnF+pRoFbx0B/2XJM8Jyu35D0e3/57RW9bvcpeJFcIPIqBK3xyFdCcHE132qPwyZkLcfF6KXI60Za2XRy8ya33BwDkb+OZOL1tlxPTqeUjejWySxF4fLf7qt62+sEIPGwenzy1S4cXdYLmIRqubyG3wEOO7QIAMER7hViVOdtu0pqzmmRSXYmpyjocxXElIY2uJaZRRQ9nunU/XWwb3bkGtajqR37uGGoBAACwGZk5hfdqRMVJV481VIejKHfTsqj7F7sl297oUZvGdqtFcoevrQAAADruPcwu0f7LChZzu3H3IeXkFT3ccvTGA71tz7YIJVuAwAMAAEBHutZibcXRqpqfuM7NV1BqZtHPvZKg32sS5OVCtgCBBwAAABHFJ2fSwav3xAw+VY9HFR/XIp/3+TNNyMHezuAU3MKk6gQ2qh4TW4AcDwAAsHkcbDzz/QGKvp9Br3WqQUFezupprbeLqEY6MDJEcn/N8dsUGa7sASlMmk6vSGVv2+jtMHuPx8yZM8UUS+1L3bp11Y9nZmbSmDFjxPRNntI6cOBAunNHWgf/1q1b1LdvX1EnIjAwkKZMmSKKXAEAABQXF/DioIP9fOAG7byQIG7XreRJXz0fQT3rB+k959OnG9O+t7robW8c4l3k+6Xp9HhU9i66Z0UuzD7U0qBBA4qLi1Nf9u3bp35s4sSJ9M8//9CqVato9+7dFBsbSwMGDFA/npeXJ4IOrox54MAB+vnnn2np0qU0Y8YMM7UGAACskXbhr4ycPNp7+a64XbuSJ/WPqEI/DG1O/RpXljyna91AydL1bgVlzhftvV7k+6Vp9XiE+rnKukS6xQ21cGGpSpUq6W1PTk6mxYsX0/Lly6lr165i25IlS6hevXp06NAhat26NW3dupWioqJo+/btFBQURBEREfT+++/TW2+9JXpTUJ4bAACKo7BS57WDPNS3v3yuCeVkZ9HmC8o6Hk4O0u/uqrLnha3vYijH48OnGtGAZlXIlpi9x+Py5csUHBxM1atXpxdffFEMnbBjx46JSqHdu3dX78vDMFx2++DBg+I+Xzdq1EgEHSq9evWilJQUOnfunBlaAwAA5sjPSEjJLNNrZBayuFu1iu7q25wOMLylsteDi349agVZLrdenB6PKr6u5OJoO70dZu/xaNWqlRgaqVOnjhhmmTVrFnXo0IHOnj1L8fHxosfCx8dH8hwOMvgxxtfaQYfqcdVjhcnKyhIXFQ5UVJU5y2MVVX4N1bomciLHdsmxTXJtlxzbJNd2mbpNczacp5/236Cvn4+gx3WGQ4qDj/XkLf26GvNfaEqOdhXU7eDrcF9n2v1GR3JwsKMKxG00HGD8dugGDWkdXuh7pmUpl+Bwd7Iz+2ev+rxsIvDo3bu3+nbjxo1FIBIeHk5//PEHuboaL9Fm7ty5IsjRxSuockJreXyIPFTEH6ScSovLsV1ybJNc2yXHNsm1XaZuEwcd7IMN59SrxxbXw+w8Onk7jd74S3+htshAO0pIUCaZarfL21tBdnn8mLQWR/0gN4q6oyx9vuFkNPWqXvh5LCVDGXhkP0yhhITirQNmLKp22UyOhzbu3ahduzZduXKFevToIZJGk5KSJL0ePKtFlRPC14cPH5a8hmrWi6G8EZVp06bRpEmTJD0eoaGhYtl7Ly+vcvkQuUuOX08uf0jk2i45tkmu7ZJjm+TaLlO2Sfub+p3UHPGexV2QcuWRaJq29qzBxzrVrihmSpakXb+/6ktNZm8Xt9NzK4jn8/EZOp70gnyQsMqBFFiMWiHGxO1KSyteqXfZBR7c8KtXr9KQIUMoMjJSLEi2Y8cOMY2WXbx4UeSAtGnTRtzn6w8++EBEpKpfkG3btongoX79+oW+j7Ozs7jo4l+k8vpPwr9o5fl6lkKO7ZJjm+TaLjm2Sa7tMlWb9l5OlNw/GZNcZA0NlcKCDvZap5oGj/1R7fJ205xXqgV40GdbL9EfR6Pp79fbU7BWcJGVm0fZecqAycvNySI+d1OuHm7W1k6ePFlMk71x44aYDvvUU0+JFWAHDRpE3t7eNGLECNEz8e+//4pk05deekkEGzyjhfXs2VMEGByonDp1irZs2ULTp08XtT8MBRYAACAvZ25LhwhiHjy62NejSparKpWuGd2W2tTwL9XxONorT+BRscm0YNdVupuWTUsPKIeCDE2ldX9EgqpcmbXFMTExIsi4d++e6Lpq3769mCrLt9m8efNEJMg9HpwMyjNWFixYoH4+Bynr16+nUaNGiYDE3d2dhg0bRrNnzzZjqwAAwFSiC5aTV8kp6EkoDA99fL3jCl0qJPDgmhrNwnxLfTyq97+a+FC9zVlr2u3DrFyKnKMcjmH2dqbrabAUZg08VqxY8cjHXVxcaP78+eJSGE5G3bhxoxGODgAALF1CimaGIitqZdgDV+/RvO2XCn3cUWvNlfLi6aI51R67qT97xtaYf2AJAACghDJz8kTZ8QfpysXcHAp6DooKPG7c0/REsDpBnhTur6k+GlvEuixFea+ffn6hnVb+RF4R9T1sAQIPAACwKjxc0u+bfdT6wx0UFaeswxToqczru3VPM/SSm5dPh67dE0GK6nnv6CSUcuCydnQ79f3RnWuW6dh4UTldGQUzWAyt0WKLbC+rBQAArFpccqZeWfIAT2eKTc6kRfuu0+DW4VS1ojv1/XofXbyTSq2r+9GKV9rQUQPDHJHhvuTn7kTPNQ8VJdB1V5otKQ9n/dNqekHgo8rxeFTviC1AjwcAAFgVQ2uh1A/WrAi78WycuOaggx26dr/QYZhZTzQQ1x8/3Zjef7JhmY/NUODx455r6p6O5ILCYTUDPWhYm6pkixB4AACAVblcEFCoNKzipV4Zlvm4OolhFm1vrz1DdwrWc2kQ7EU3PuorLoFeLuV6bB5aiaQqufkK+nDjeXF7wxllUNSiqi/Z2eCMFobAAwAArMr1u9IE0U+fbkIujprTWW5+Pl1JlPaKLP/vFk1ceUrcTko3XolyQz0ebM+lRDHMcjpGWXfE1dF2Mx0QeAAAgFW5/1A5k0UlyMuFRrSvrr6/8UwcnbiVVOjzb5dx5sqjaNfs0Obv7kSnYjTH9GLrMLJVthtyAQCAVbpXEHi81qkGdaodIJJD2fMtQmnFkWiR06HK6zBEtb8pS497uTpSVo5y+CfY24VqBHiQrUKPBwAAWJUHBYFHh1oVJaXNDfU2tKyqv27Ln6PaGvX4RneuIa5faqdJHt17+S6tO3lb3NZet8UWoccDAACsssfD103ac8HTYXVV8XWlTk4BtPuScjG5znUCqFpFd6Me3+SedWhAsyqiV8O+QgUxxZf9dTJWXLs4ahJhbRECDwAAsBqL9l5T53hU9pbOSMnK1Z8u62RvJ9nP3QSLsvFslZqBnoWuxeKilQhri2y79QAAYDWyc/Np5ZFocdvT2YF8dXI1DCWUcu0O7f20p92agqHAw9nBtns8EHgAAIDF48JbXCZdVTzsgwGN9PZJz9YvR94k1EcMd6i4FzLd1aSBh6Ntn3ptu/UAAGAV3lx9Sl2J1NAwCxvfvbbeNg40OK+jqDobxqK9QJyKi43neCDwAAAAi5aUnk1bzt2RbHMxMFzxRJNgerZ5iF7CqXbSqTGn0hpiMMfDAYEHAACAxYqYvU1vW6CXcjVaXV3rBknuO+sEHuH+bmRKSC7VZ9utBwAAq8TVSg3RzZ/gKa3awx11Kilnm5gKkkv1IfAAAACz4sXbdl64QwqFwuDj7gUzUb4fEkld6wbS36+3K/S1XLXyJ5qH+4pVYCt6aHpHqpi4eJd2YquKi433eKCOBwAAmNyKw7do6pozNKhlKJ29nUJnbisXT6se4C5O1qtHtSVvV0cRjKjqczQJ8aFeDSo98nW1i4r5ezip8zrWj20vEksLK2luLIZWoHWx8eRSBB4AAGByHHSw3w8r63KoXEtUrjz726GbNKZLTbp0J00sK8/xgq+7Y5Gv6+um2cfBTtOz0LCKN5mDvYE4x8XGezxsu/UAAGCRuJcjL19Bvb7cI+43DfUpVm6Ej1aPR/SDdDI3e3v906wLejwAAACMTzVs8u+FBIO9AJkFq7ey9KxcOnbzgfr+O33rF+s9tGewnI5RDt+Yk6Ecj2wDpd1tCQIPAAAwiaE/HaZzsSnqtVa0+bg6UXxOpmQZ+VPRyhLolbxcKDLcl6yRgQ4PUi1yZ6sQeAAAgEl6O3hp+MLUCHSn+BRN4BGXnEH/nIoTtwe1DCvVe7as5kfmZqhyaWetSqq2CIEHAAAYXUJqlt62Po0q0c176aIXZGzXWjS7f0P6bMtF2nQ2XpJ06qOVMFocF95/jLafv0PddIqJmYODVnbp9L71qFu9IKpW0Z1sGQIPAAAwqvx8Bb38y1G97QtejKSUzBxKSMkS9TbYa51qiMBDW0lXlOXkzccbB5Ml0O7x8HRxsPmggyHwAAAAo3r6uwOFJnp6uTiKy6Mqi1rzLBBDlUttHabTAgCA0Vy6k0rHbymTRIvDUJBhuJ6pdXDUyi6Nik0x67FYCgQeAABgNJxroe387MeoY+0AGtOlRomGaqxV6+r+6ttpWXlmPRZLgaEWAAAwCg4YFu66Ktnm6mRPv/yvZYlep25l0y7sVp647LtKZi4CD4YeDwAAMIr0nDxKzcwt02ssH9mK6lbyIjkI9XUz9yFYBPR4AACAUXD10bJqW7MiWbvVr7Wh9afj6PWuNc19KBYBgQcAAJSrnDxlSfCH2XnqZe15mmzzqsUr6MX1PTaeiadJPWqTHHC7i9t2W2AxQy0fffSRWK54woQJ6m2ZmZk0ZswY8vf3Jw8PDxo4cCDduSNNVLp16xb17duX3NzcKDAwkKZMmUK5uWWPsgEAoOQOXr1HjWdto0933qKriWlim6uTA43tVova1NAkWj7KRwMb0+JhzcXqtCA/FhF4HDlyhL7//ntq3LixZPvEiRPpn3/+oVWrVtHu3bspNjaWBgwYoH48Ly9PBB3Z2dl04MAB+vnnn2np0qU0Y8YMM7QCAMC2bT0XT4N+PCQWgvvzdCK98utxsf1umn7V0kfhuh5c4RM1MOTJ7IFHWloavfjii/Tjjz+Sr69mEaDk5GRavHgxffHFF9S1a1eKjIykJUuWiADj0KFDYp+tW7dSVFQU/fbbbxQREUG9e/em999/n+bPny+CEQAAMJ1Xfj1m7kMAK2D2HA8eSuFei+7du9OcOXPU248dO0Y5OTliu0rdunUpLCyMDh48SK1btxbXjRo1oqAgTT3+Xr160ahRo+jcuXPUtGlTg++ZlZUlLiopKcqiLvn5+eJSVvwavCBSebyWJZFju+TYJrm2S45tklO7uPR5YT5/prHVt09On1Vh7bKJwGPFihV0/PhxMdSiKz4+npycnMjHx0eynYMMfky1j3bQoXpc9Vhh5s6dS7NmzdLbnpiYKPJKyuND5B4b/iDt7MzeqVRu5NguObZJru2SY5vk1K6lh5Uryeqa2CmE2lVxpISEBLJ2cvmsCmuX7AOP6OhoGj9+PG3bto1cXFxM+t7Tpk2jSZMmSXo8QkNDKSAggLy8vMrlQ+REWX49uf1yyq1dcmyTtbcrLSuXeGTf3Vn65ykxNZP4fycnkVtbm+T6WWmr4PhAffuJxpXp79PKQKRh1SDxmcmBXD4rQ+3itAfZBx48lMIRcLNmzSTJonv27KFvv/2WtmzZIvI0kpKSJL0ePKulUqVK4jZfHz58WPK6qlkvqn0McXZ2Fhdd/ItUXr9M/MtZnq9nKeTYLjm2yVrbxdMwH/tyL3GF7D1vdiEnBzt10iLnD1TydKKN4/3Iz8O0X1aMzRo/K13JBYXCxnerRbUC3dWBR/UAT6tulxw/q8LaZSpm+8l169aNzpw5QydPnlRfmjdvLhJNVbcdHR1px44d6udcvHhRTJ9t06aNuM/X/BraXXjcg8K9FvXr1zdLuwCg9OKTMyk2OZPiUzJp8b7rdOzmA3rwMJtGL1POjohPzabp686Z+zDBgPtpyoR+fw8nquqvWfo9xNfVjEcFlqhMPR7cI3H9+nWqUaMGOTiU7KU8PT2pYcOGkm3u7u6iZodq+4gRI8SQiJ+fnwgmxo4dK4INTixlPXv2FAHGkCFD6JNPPhF5HdOnTxcJq4Z6NADActf0eOXXo6IGhMrHmy+ob3MBqtyCYlQbzxaevwXmwTkPF+KVSfqVvV2pfrAXvdMjnJrVqCxZnRWAleo3Ij09XQQFXLSrQYMGoheCcWDAhcDKy7x58+jxxx8XhcM6duwohk/WrFmjftze3p7Wr18vrjkgGTx4MA0dOpRmz55dbscAAMZ38346bT+foK50qUt7u6+bZtEtMK8Np+Oo6tQNYnjsxr10crSvoC4S1q9BRWoapimRAFCmHg9Ozjx16hTt2rWLHnvsMfV2nvo6c+ZMmjp1amleVryeNk465ZocfClMeHg4bdy4sVTvBwCWIS45o9j7Bvug695SjFmuHAK7eCdVXIf5uZGHs4PsppuCBQQe69ato5UrV4ohD+2EFO79uHpVugQyAEBxcjuKKzsXJ7Wy4lLmDnYVKFwrF6OkLhUEG9LXfVjGIwNbUKrAg+tdGJoe9fDhQ5NmxgKAPMSVIPC4nJBGsUkZ6PkoJS5f3mveHsrNV9D7/RtQu5oVqXqAR4lfZ8n+60Y5PpC/UuV48IyTDRs2qO+rgo1FixapZ5wAABTXoWuapNK3HqtLLQ2s5Mld+CptP9ppsmOTmysJaSLoYO/+dY5eXPRfqV4n+r5yeOztPnVp47gOVD3Anf54FX//wUg9Hh9++KFYF4XXSeGVYL/66itxm9dR4cXcAACKK+ZBOu29fFfc3jS+A9Wr7EXd6gVSz3l7dPZU6M2kQA9rya04rJwMoN3blJevKPGCbPuuKD+zMD93MYtl5xudy/U4Qb5K1ePRvn17UWuDgw5eK4UXa+OhF147hRdzAwDQDhAW7b1G7/11ljIMzFq5dT9dXFev6C6CDlbDQNd/Wpb0uTl5pltbQk7+u35fb1tGjvJn+9O+62KWCl8iZm+lxFTDq8qejklS33Zzsjfi0YIclbqOB9fu4BVlAQAepeY7m8Q3asbJjP9rX03yeFySMr9DO2eDv33P7Fef7qfn0L7LiXT8VhJ1qh1Auy8lqve5GJ9KjUK8TdYOuUjJ0F/M7ecDN2hMl5o0b/sl9bak9Bya8ddZWjhY/8sk59io1AwseX4I2LZS9Xjw9FUuaa6Lt23atKk8jgsAZCA5PUcddLDZ66NE74f68YwcemPVKYMnsOHtqtGkHrVp0bAWogz3nCcb0MutK6sfX3lUOmQARcvNyzdYK+XTLRfFtZNOsS9V70h6di6didEsIvbjXk1iKZJ8wSSBB9fp4HVVDHWplraGBwDIz4oj+sHBnA3nxQmQPfalJo+jc50Ag6/h5+5EE3vUpio+rjSidbB6eyUvea3XYizHbz2g/Vfuilopry8/od5+aU5vci5YC4f9fvgW3XuoLHuucv9htpi+/P7689Tv23008mflSuJcyh7ApIHH5cuXDa6FUrduXbpy5UqpDwYA5GXuJk3Zc22jCtZeeZilXFiMta6urHhZlHFda4rr21rd/YVZeyJG0sNii6XoByw4IGauDF70H20+pyk3zwvw/fV6O/X9aWvOqG9r5+xeTkgVQQnj6rL7ChKBWd1KnsZvBMhOqQIPb29vunZN/z8zBx283goA2C4OJjafjZNUI20Q7EUrXlGuscS2Rd0RRcM8XZTlz0e2r0YujsVLUgz1U3btxzzIEDNi+GII98BOXHlK9LC8tES6irWtSC1YMbaw4l51KymTeXVd+7APebo4SKbNqvxxNFp9WztwATBqcmn//v1pwoQJtHbtWpFkqgo63njjDXriiSdK85IAYOV4kbA3/jhF52KVi4Vp+21EK53JsPzt+Y7oteC8gkk9axf7fUIKcgq4HkX7j/8Vt796PoL6R1SR7JeZo6lw+u/FRPHt366EU0atXWKa4cJsHOipDG0TTr8cvCl5nKcpc1GxU9FJtPpYjN66Oqxr3UBydsCMFjBRjwevBMs9Gzy0Uq1aNXGpV6+eWFn2s88+K81LAoCVW3U0xmDQwbkZvu5Oeou73bj7UL1suptT8b8Dhfi66VU7Hb/iJD3QyU9IzcoxOGXUllwz0Mux980uNP1xzVD5rCcaSB5XJfl6FhRs4wBR28WCVWi9CnpEAErKobRDLVwsbNu2bWKxOFdXV2rcuLFYQRYAbM+fx2Jo8T7DJbTt7DTfons1CKIt55QnskUF+3NQUhL+Hob3f5idK3kt3SGC9Ow8cteqfmoLrhcEd9pC/ZSBm4puETaeSaTcbvg1VT1JqmEygJIq9f9C/mXt2bOnuACA7Zq+7gz9dkg6e+WFVmG0/D/ltppaxcAWvhhJz3x/UDIrwtetZIEH54JwYqTuYnHa03bZTzpriRgqXmaLgYch3CvFw17d6wVR74aV1D0fqoqyhni52lYQB+Wn2L85X3/9Nb3yyitiqXq+/Sjjxo0rj2MDAAv337V7ekHH8pdbUdsaFalb3UBaeuAGfTigkfoxzrHQrgfBcgqm1pYEV8vUDTy0czrYhtNxkvvpOZpES1vw66GbtOKIMhF0et96osT5kNbhBvddNrKVyOXg4m6qHpDRnWvSkv031Ps0DfOhE7c0FUu90OMBxg485s2bRy+++KIIPPh2YfiXFoEHgG1YVtCroU1VX6NbvSBx0TWkTbhkWMbbteQnMBeR1CjN4ej15R668kFvcrC30+v90J26awveXXdWfbtFVT8a2aF6oftWrehOk3vVkWwL8HSmgc1C6M/jMdShVkX68KlG1OETZTIvw1ALGD3wuH79usHbAGCbuKfi71Ox6kTDlIKpm4FFFPZ6p089SeAxtXfdEr93YprhNUTupGaJYQPO99D118lYigzXX/VWjm7ekw6xcAJvaXz+bBP69OnG6tlAz0SG0KqCWS66ycIARpvVkpOTI6bQnj9/vqRPBQAZOa01ZPLLiFbUPNxXJI9qL19viPaU1hmP1y9VyW1DPRosvaBX41ed6aG2luPR6dNdetVfS0v789JOOMU6OVBaJc4OcnR0pMxMw3PDAcB2jF52TFy3rOZHEaE+tHpU22I/d/Gw5qIKJiehlkab6v508No9ve3RD9LFzBXt5NVXO1an7/dcI1cne1FUTHcWh9xxj1J5tfnSnTT1be5ZAjBZHY8xY8bQxx9/TLm5tjVmCgCaHoc7KcrhjsZVSv7Nl3M/5g5oVOxqpbpGd1EWLtT1v6VHqe1HO8WiZowDmyoFwwxcJKvatI2UJvNcDw6uVJ0U+97qQq91MvyzKo16lZUl0rlXy9YCOCg/pZoPdeTIEdqxYwdt3bqVGjVqpFcmfc2aNeV1fABggSe2Bu9tVt8f372WyY9BezhnUMsw9VoiKoeu3VcHRZV1vplvOhNHzzQPJbnKys0n1UiUVykSdx/ljZ51xPTn51rI9+cHFhp4+Pj40MCBA8v/aADA7I7dvC8qXnKF0IZVvPRmL6w/HSeZumqO2Q3aa4z0bVSZbt1/SPuv6A+98PBKNX/pF6PcQvJD5ILLnKu4lrJHqTAVPZzpzcdKngwMUOrAIz8/nz799FO6dOkSZWdnU9euXWnmzJmicikAWD9e2G3gwoOSbdfn9pF0q+/QKqHdo77+dFlT4IBCe8ZGJS/Df4P4xBvs41KsxFQ54CnDz/1wSH3f0b5Uo+kARlWi38oPPviA3n77bfLw8KAqVaqIQmKc7wEA8jD8pyN62y7Ep0run9T6Rv3NoKZkLtsmdqTlI1uJGhQDmkkXiFPhNWC4roet0F6/BsBSleh/5C+//EILFiygLVu20Lp16+iff/6hZcuWiZ4QALBsWbl51PfrvTRgwX6xJL0hF+9Igwz23e6r4poTNv84Ek037ilXJ906sWOpk0PLQ60gT2pbs6K4Xa+y4eXdXZ2Uf+LOzuqlzgvJLMNicdcS0+idtWdEz5AlSs6QFlUDsPrA49atW9SnTx/1/e7du4su2NhYZREhALBcX22/LFaPPX4rib7997Le44WdkLnw1tnbyVR/xhZ688/TYhvX7KhVsIqpJSgsl8HVURlscNDRPyJY3C7LrJbRy46Laq3aVUEtSYpW4NGqmm0USwOZBx48fZZLpuvW9eCiYgBguXhdkwW7lD0X7OgNTZ0LlX8vJIhrB7sK9Pfr7eiVjpoS21PXKAMOle71gyxqOqWzg12ha7qoqHpnvtx+mbp/sZtuFHMBNe2foWrYiWuQWOr6LKxBsBf9NrKVuQ8HoOzJpTyNbvjw4eTs7KzexsXEXnvtNcmUWkynBbCsFUonrzqlN6TCPRzaQyW8iBjjAlyNQ3woLTOXfthzTWw7eztF8vzSVBs1Ju3qmoUlocanaIaXriSk0bt/naVfRxT/5Hy3kDLtloKLpu0sCB5bVfNHYinII/AYNmyY3rbBgweX5/EAQDniMuFdPpOWz/Z0caDUzFy6dT+dagcpC0JpJ41yGXPWLNzX4GuO7VpTTGG1NDve6ER/n4ylr3ZcNhh4ZOqUTC9JrkdqZo4oTKbbA+JUSE+LsXFvjZuzPQV6uogvhK/9doy2nNPMNhrXraZZjgug3AOPJUuWlGR3ADCz5TqFtRgXgOLAo+e8PXR6Zk+xvDmfhFXDCK1r+Itr7g35elBTGvf7CfVz145uS03DDAck5lYjwIO61g2UBh5aPTqTetamHQU9AqwkQ0VrT9zW2xYVlyJKxZtaUno29fxyjwh8hrUJF8XCtIOOMD838nEr/dosABZZQAwArINuzYqZ/erTzH+i1PffXnNGJFtyhU/el7/BB3tr8riqV5QW36phQQmlhjTUKd+uPdzQIFj6mH0JAg97A0M5T87fTzc+6kumFn0/QwQd7GcDi+GtfLW1yY8JoCQwCAggQxxE/PRfnOQbPgvzd6MAT2dJFdJdFxPp651XxH1vV0dJTwAPy6g0DvEWvSOWjAOEt/sUr7JmSYZJ0rMsZ2XbB+nZj3y8srdl5d8AWFTgsXDhQmrcuDF5eXmJS5s2bWjTpk2SxFUuUObv7y+KlnGZ9jt3NF2Kqim+ffv2JTc3NwoMDKQpU6Zg8TqwebsvJdIPB2PpiM7sFe6Gn9mvQaHP0/1e7+ygGaqY/0IzsgZPRlQp1nTS+sHK2h88zLTuxO1Ck0e5d+GDjecNPmasKqhcgbQ0gcf0vvWMcjwAsgk8QkJC6KOPPqJjx47R0aNHRQn2/v3707lz58TjEydOFEXKVq1aRbt37xb1QgYMGKB+fl5engg6uHz7gQMH6Oeff6alS5fSjBkzzNgqAPPiZEPuyTCE11/hXo3CJKRKT77+Hk7k6ewgpqta2kyWwgR6udCp93oanLHi4+aoF2TN23aJJqw8SS/++J/B10tI1cyGeappFck0Yw5KOOeiPK0/HUsN3ttCY5YdF5+ltqM37tP4FSfFbV+ttqiM7KA5NgBLZdbAo1+/fqIgWa1atah27dqiJDv3bBw6dIiSk5Np8eLF9MUXX4iAJDIyUiS3coDBjzNeHTcqKop+++03ioiIoN69e9P7779P8+fPF8EIgC3ik+i6k4aL+nHC6KNmc3Bypm6OxL6pXenYuz0M5jlYKg6uDA2lfD84Un07vWCWy497r6mnGHN1V12ciKsy58mGNKaLZsbIyiO3KGL2Nvpp3/VyO3ZVMu+GM3FUbdpGOlAwzZk9/Z1mHR2e0XL83R7qQKh+IdVbASyNxeR4cO/FihUr6OHDh2LIhXtBuDAZV0dVqVu3LoWFhdHBg8r/fHzdqFEjCgrSLFTVq1cvSklJUfeaANiKE7ceUNWpG0SlUV1cEOzA1K7idmQh02TZghebGTyJay9Db81aVfenNx+rI27z0ApXZNUeLflulzIIUcnJyxc9D6xmoIeoceLl4kCqNBhVou7s9ZqE3bLSDZjeKKjBwvVYtAV5u5CfuxNN6F6LPh7YiH4Z0bLcjgHAmMz+1+TMmTMi0OB8Du7tWLt2LdWvX59OnjxJTk5O5OMjna7GQUZ8fLy4zdfaQYfqcdVjhcnKyhIXFQ5UGK85Ux7rzvBrcBep3NawkWO75NKmlUeiadpaw2W8N49vr67Xwe30dnWgb56PoLEFXfZsfLea1CzMl5zsK1jsz6K8PqvKXspZOzwcpTsktedyIo3tWkN9f+XhW3St4IQf6Omsfm8fV0d6kJ4jmf1T2uPSbRdPd9Ze7C2/4LEVOlOjZz9RX2x3cbCjZyJD1K9lCeTy/8rW2mUzgUedOnVEkMFDK6tXrxZFyjifw5jmzp1Ls2bN0tuemJgoAqDy+BC5PfxB2tlZTKdSmcmxXXJoU0JadqFBx6CmgeRTIYMSEqSLmt1PSlbfbh3uRYMa8VTTfEpIsMxS4OX5WdnnKhe5M6Saj4PkZ3Dp9j31bQdFrvqxNuFetPG85jFv5wql/tnptqtRJVdJ4OHvak+x8XfI3U4T6ByaEEmUm0YJCWlkieTw/8oW22UzgQf3atSsqRwz5TyOI0eO0FdffUXPPfecyNNISkqS9HrwrJZKlSqJ23x9+PBhyeupZr2o9jFk2rRpNGnSJEmPR2hoKAUEBIjZNeXxIfKURH49uf1yyq1dltammAfpouufZ58U10/HLkjur3i5FTWu4kV7zt2kTg3DyblgoTRtwff5X2VeQlouiRlhtvJZ1cjjHg/9RfKYo5OL5Gfh4aGs5spScyqoH2tZM0MSePh6upb6Z6jbLidn6VBZYnoePfNzlDoYeTIi2OI/L0v7f1Ve5NyutLQ02wk8dPEPgIdBOAjhBeh27NghptGyixcviumzPDTD+JoTUvmbhuo/4rZt20TwwMM1heG1ZrTXm1HhX6Ty+mXiX87yfD1LIcd2WUqbOLGx46fK3r4L7z9W7CXnt0ZJp5jXC/YmFyd7iqjiKYIOQ+3qWlczRHkvLdvsbTflZxXoVfjsnMzcfMlre2jVMTkRnaR+LFQnMKxAyuMqa7ty8vVnJCXqzDTinBtr+Lws5f9VeZNzu2wi8OCeB56JwgmjqamptHz5ctq1axdt2bKFvL29acSIEaJnws/PTwQTY8eOFcFG69bKynw9e/YUAcaQIUPok08+EXkd06dPF7U/DAUWAJaMAwAVriZanMDjamIa3binGTrgEt58YipqDJpnqFSr6C4SFtvVrEi2xNA0VBWe8cO9Tp4ujqK2x6YzhnPFWujUCNEt1FYY7qLPyMkjNyfDf3p/1apE+v2QSHr112N6+4T7S6vJAlgbswYe3FMxdOhQiouLE4EGFxPjoKNHjx7i8Xnz5omokns8uBeEZ6wsWLBA/Xx7e3tav349jRo1SgQkvEIu54jMnj3bjK0CKJ37DzWBR1ZBSWwVVeKX6lsJF67i4EGVcFjFx5X+eK2NuC6uFa+0pn9OxdIzzUPJljgYWLWVgxFOFuUgjhfVy8l7dKJdaSu4TltzhlYfi6F1Y9rplXdnh66JMTChsHor2r0wANbIrL/BXKfjUVxcXERNDr4UJjw8nDZu3GiEowMwLe1CVdq1NtKzc+nxb/aJb+F/vtaG1py4TdPXnaWnI0No+X/KwGNij9olCjpYkJcLCk4VUM1QOR+nnOFmyPKXpQXJ+OfPQYTKH0ej6dlCgri45AxysrejFUei1bVDvnq+qd5+aVmaBFIHuwr01fMR6oJhKnKZ2gy2S16DVABW7H9LjxoMPKJiU+ha4kM6FZ0khkZm/n1OVMxUBR0s3L/4yahA1CysZKvKvtqpOrWtIR2S+nhgY7F6r8qbq08bfO6xm/epzdydFDlnu3ob11p54cdDlJun6dnKz1dIejzqVfai/hFV1HVHVBoUlHoHsFYIPAAsQLJWTQjdwON2kmYqLC9db2cgCcxayplbCi6nrt1DxEXBHuWZSP2eDB7qeqJJsHrRPX936VL0HEjwENnAhZpqo9oOXL1Hp2I0s2a0p9B+NzhSFCtjozvXpItzHqNudQNpSq86yPEAq4fAA8ACJGfoBh6ab8KxSZoTEg8F6M6o4BOS9lL2UDQ+qe+e0ll9f2rvwhdXc3W0F1VLC7NkeAtx7WBfQazbwrOTeHis46f/0usF5c8Lk6a16q0qCOGhlMcaVtJbrG/x8BaScu0A1gqDhQAWICWz8B6PWK0ejwW7rqrLdbMPn2pEL7QKM81ByjDJ9NqHfcRqrzyN1pDnW4TSq500lUwN8S3o6biTkiXWbeE1U0Z1rkExDzLEpWEVLzp723DuSFrBOjBR8Q/p9RXKeizVA9CjAfKGwAPAApy5La0aqFrAbM3xGPr1kGaKJdOubBziiyGWsrCzq0D+Hs6UUfDz1vXRwMZFvoafm3SIJSouhe6laWpvFBZ0sNSCgHPXFc2Qyw2dNVkA5AaBB4AF4BkR2o7dfEAtqvrRpD+UC4QZwt3/7W2sBoexuDrp10yZ3b9BqZ+ru6Ab+2l4cxFQHrp2j347dEtdr4XFpWitHaW1Gi6AHCHHA8DM+Nv2iVvKb7wtqyoLUy09cINaz90h2W/ZSOl0zk+ebiy+sUP5O/VeTxrapmqpn7/9vH5BMSd7e3q8cTDNebIRDW6tHB6bs+E8ZeXkUUKaZqitdXVpcTIAuUHgAWBGJ6OTaMpqTa9G4xD9olIquuu3uDgUr6Q6lFxhxbsKw7NNtGnPRDK03H2AhyYZmGt73ElVFo9rU92fvnxOv74HgJwg8AAwE55u+eT8/ZK1OR41LZYLfhXVxQ+l172eZv2akirObBPtwEO7V+PgtfsUXxB4zH+xGVXCDCWQOeR4AJjJ/XRNiXT2+TNNKF87c1TLrsmdJSeuotYcgZKb3rceRd9Pp5c7GqeaK1cuVWkW7qu3yF+7mv7kp1MLBECO0OMBYAacVHj4uqZKJevXJNhgvQhOcqxa0V1vqmVp1wsBw/hnvGViR1EKvTQeNUzGtINKR3s7vcTgmPv6wzMAcoTAA8DEtpyLp4bvbaHRy45LtnOPRtMwXxrXrZZehUyVpcNb0qCWoWLdECSWWpblL7emN3rUVt9vEioty67bm7VwcDPJ/Zv3NasMA8gZAg8AE3p33VmDS51rm9SjtmQNEO1ExzB/N5o7oLHeuiFgflxx9MmmVdT3w3WSgRsES3tEeNE/bSPal34WDYA1QeABYCJcLEq3GBgb3raqWKJeG68B8r921UQ59J71peWzwXJxOfsnI4KJO6N4pkuHWsoAcd9bXSQ9VyqvFuSTjO1QhSZr9ZYAyBmSSwFM5FS0tDqpynv96lMFAwu/zehX3wRHBeXty+eb0ufPRohAY+lLLUU+T2HTc6f1qUeTe9ame3cTydkRs5TANqDHA8BEBi/+z+B2Q0EHWDdV7wZfF1UTxFBPCICcIfAAMCPPIpZjBwCQG/zVAzBRWXQHuwqUm6+gPVO6iCXQeTrtrCeKtx4IAIBcIPAAMEFS6YQVJ0XQEejpTKF+rmJ2CtftAACwNQg8AIzofFwKfbvzCu24oFw0rFpFd+R0AIBNQ+ABYCSbz8bRa79Ji4RV9HQ22/EAAFgCJJcCGMlXO67obQvwQOABALYNgQeAEYdZdIX7S6tZAgDYGgQeAIXIy1fQ/YfSFWSLK0ln5VnGSaWlXYAMAEAukOMBYIBCoaABCw/QmZgk2jyhI9UO8izR81cfi5Hc/25wM+pcJ5BcUJ0SAGwcejwADHj2+4N0KjqJ8hVEP+y5VqLn5ucraM6G85JtXeoi6AAAYAg8AHQkpmbRkRsP1PeDvIqfEHr81gOq/vZGybZXO1UnZwcEHQAADEMtADqe+/6g5H5Sek6xh2eGLT6svl/Rw4mOTu9R7scHAGDN0OMBoOPa3YeS+3dSMov1vGlrzlBqVq76/oBmSCQFANCFwAOAiO6lZYm8jtYf7lBv++r5CHEd8yCjyOffupdOK45Eq+/PebIhvd2nnpGOFgDAemGoBYCI+i84QLFJ0p6NOpWUM1lik4oOPKJ0ana80DKsnI8QAEAe0OMBNm/p4Ti9oIMF+7iK65TMXDp284HI4TDkSkIqvfbbMck2OzusxwIAYAgCD7Bptx9k0HcHYvW2D2hahTydNR2CAxceoC3n4vWfn5RBk/44Jdk2vS+GWAAALDLwmDt3LrVo0YI8PT0pMDCQnnzySbp48aJkn8zMTBozZgz5+/uTh4cHDRw4kO7cuSPZ59atW9S3b19yc3MTrzNlyhTKzdUk+QEUlpfR4dNd6vvrx7anLnUCqEGwF338dGO9VWQX7roquX8lIY3afbSTTsckq7d98FRDGtmhugmOHgDAOpk18Ni9e7cIKg4dOkTbtm2jnJwc6tmzJz18qJlVMHHiRPrnn39o1apVYv/Y2FgaMGCA+vG8vDwRdGRnZ9OBAwfo559/pqVLl9KMGTPM1CqwBlwK/fFv9qrve7s6UsMq3rTkpZa0YVwHcrTX/69xN01aBn32+ii9fbg6KQAAWGhy6ebNmyX3OWDgHotjx45Rx44dKTk5mRYvXkzLly+nrl27in2WLFlC9erVE8FK69ataevWrRQVFUXbt2+noKAgioiIoPfff5/eeustmjlzJjk5OZmpdWDJxv5+XORuqBya2sXgfn7uTur1WnTXX9lzKVFy38XRjqoU5IUAAIAVzGrhQIP5+fmJaw5AuBeke/fu6n3q1q1LYWFhdPDgQRF48HWjRo1E0KHSq1cvGjVqFJ07d46aNm2q9z5ZWVniopKSopyRkJ+fLy5lxa/BiYjl8VqWRC7t2ns5kfZfuSduT32sDvWo5kKO9hUMtis5I0eSbPqotp98t4fF/Gzk8lnJvU1ybZcc22QL7bK5wIMbPmHCBGrXrh01bNhQbIuPjxc9Fj4+PpJ9Ocjgx1T7aAcdqsdVjxWWWzJr1iy97YmJiSKnpDzawkEUf5B2dvLJ37XmdmXm5tOY1ZcoLiWL7qcrezqebhJAT9R2E21KSLAz2KYQbye6+UAZpN5Ly6SEhAT1YzX8XejqPeXvS51AN0q6f5cshTV/VrbUJrm2S45tsoV22VzgwbkeZ8+epX379hn9vaZNm0aTJk2S9HiEhoZSQEAAeXl5lcuHyImJ/Hpy++W0xnatOhZDb/15Rm/7lD6NyN/d8ZFterVzLXp77VlxmwOW2Cwnigj1EQvBFcQvwuRedcUwoaWw1s/K1tok13bJsU1yb1daWpptBR6vv/46rV+/nvbs2UMhIZoy05UqVRJJo0lJSZJeD57Vwo+p9jl8WLM+hupx1WOGODs7i4su/kUqr18m/uUsz9ezFNbUrozsPKo3Q5pHpNK7YSUK8lYOnTyqTS+0Cqcqvm407Cfl79jBa/epWbgfrT4WTXHJmeTqaE9rx7SlupXKHrDa8mdly22Sa7vk2Ca5t8tUzPqT4+4qDjrWrl1LO3fupGrVqkkej4yMJEdHR9qxQ1PGmqfb8vTZNm3aiPt8febMGUkXOM+Q4Z6L+vXrm7A1YEmycvWDjjpBnuTv7kSDWobRwsGRxX4tfo6uv08pa3881yLUIoMOAABL5WDu4RWesfLXX3+JWh6qnAxvb29ydXUV1yNGjBDDIpxwysHE2LFjRbDBiaWMp99ygDFkyBD65JNPxGtMnz5dvLahXg2wLjykwSf5jrUDxAyT4hqySNML1rN+kAg07EtZTdRX633z8pUJWPuuKPM56lVWllUHAAArCDwWLlworjt37izZzlNmhw8fLm7PmzdPdGlx4TCeicIzVhYsWKDe197eXgzT8CwWDkjc3d1p2LBhNHv2bBO3Bsq7x+KNP07R+tNx6m2HpnWjSt4ukv22R90RM0+61wsibzdHSkzNohYfbJfs88PQ5mU6Fh9XR/Xta4lpNP/fK+r7XPsDAACsJPAozvQdFxcXmj9/vrgUJjw8nDZu3FjORwfG6L3YeSGB7qZl0cDIEINFutiZmGTq961+kvG2qHga0qaquJ2SmUPN52yn7FzNtLZTM3rSEzrPi5rdq8zH7eZkr7697qS0vHqYn1uZXx8AwJZYRHIpyF9OXj7VemeT+v728wm0aJi0J4J7LjafjaN3150z+Brztl+mW/fT6VRMMrWo6isJOliT2Vsl959rHkpuTg5GTbry0FrPBQAAioa/mjaA8xJKm99QXu8/eZV0IbXt56Xr7bBvdlymRfuuq+/zMY/qVIMycvJo8b7rooLoj3uVjx++fv+R73l6Zk/yctEMkcghExwAQA7kNR8I9ExaeZJqvL2RPtp0QQxP/HMqljp9+i9VnbqBYpMyjP7+q4/FiPf/S2eIgtWfsVldhjwzJ49WHolWP1a9ojtdfP8xmtyrDvVrElzo6w9uHUb73tIvd26KoAMAAEoOgYeMnL2drO4J4HyK5PQcWnPitrj/3e6r1HjmVhr7+wm6eS9dbNthoNehvCzYdYWGLzks6el4sVUYXZ/bh5wdlL926dl51OWzXWIYpu67myk1S1OR68dhzcmhIAekeoB7oe8zvG1VCvF1o99GtFJv+3NUWyO1CgAAygpDLTLASbpHbjygZ78/KO43CfEWeRBFOXrzgTpZszxx0PPJ5ouSbV89H0H9I6qI25N71qEPNp4Xtx+k59DHmy6o9wv2dqE9b3ZRBx2P6r24+mEf9RBSu5r+9Pfr7ahqRXeT9XbsmiydjQUAAEVD4GGlgUZWbj452FUQJ+gn5++XBBq6QUclLxeKT9GsQTOua036eucVMfzRPyKYutQJLNdchWFLpJVkP3yqkTroYDUDPSSPq/I6fN0cad9bXcnOQD7Ke/3qi7yQbwY1o7TMXPJ0cZDkrfDxNw6RruljTH+OaiOCHAAAKBkEHlbmVHQS9Z+/X31/7ei2RfZu7H6zMx25/oCq+LoqT9gVKojAg/1v6VFaNLQ5NQn1ob9O3qYXWoWVeiYI52tsOhtPey9LF0vr00haur5tTX9qXd2PDl2TJohO7FHbYNDBXmpXTVxYSQqJlZdPnm5Mb64+rb7vZK+ZYgsAAMWHwMOMuF7FyegH9GKr8EJPuI8KOthTCw6ob2ufzC/OeYx45WbXghoU7WtVlDyvsreLWGuEjfzlKHWoVVEEDDvOJ9Dvr7QWU1UfZuVKqnYWhmebzPz7nLqMuMqx6d3Jwc5OFPbS5uxgTyteUZa85yRX7ZLmlorXdpEEHgV5KgAAUDIIPEwg+n66qLipXTDrTkqmukgWD5fw+iFFmfGXcpVUQxa+2Iza1qwoqn1yEMEn90dRBR0qql6Kg9fuietxv5+grVHxtHViR6oZaDgguJ2UQc9+d1Bc6xrfrRb5exRdsv7zZ5rQGwUJqME+rmSpdHuBtIuKAQBA8SHwMLJPt1yg+f9eFbdPvdeTvF0d6d11Z+nXQzfV+yzcddVg4MFJmu/9fU6yrypR89a9dPp82yX1tq71AkWwoVuUqzQOXL1Lm88p183hHpDCAo92H+2U3K8R4E5pWbkiYfSpppqcjkdpV1PTExPkJS2Hbkm080meiQyhEF/LDZIAACwZAg8jrzeiCjrY51sv0pDW4XqBBFfjXHU0mp5pHiqew0mfbar7U8yDDL19Wb/GwZSTn68OPH7+X8siezh0/TS8OU1ZdZruPVTW0dD2wo//qW8v3H2VRrSvJpllcjI6ibad16wGzMZ1q0WTetQWxcI4+OAAqzi4J2jN6LbkaGdn8cMXe9/sQg+zc7EaLQBAGSDwMBKuttl6hrSENy949stB/UCCTVl9mh5vHEzf77lKX26/TF3qBNCNgnobKg2reIlZHZwP4mxnTzc+6it6RYqTH6Kra90gOjq9O1Wb9ug1bpLSc0RuwxfPRYj7D7PyaMBC5bRdlfkvNKMe9YPUPQPFDTpUmoX5kjUIxbosAABlZtlfMa0UJ2Z2mX9Sfb9+ZS91Eqa2Kj6uNL1vPfX9b3ZeFkEH+/diIl2/+1Dc5lknnCy6fmwHqqYzhbM0QYf2FFTtwlujO9cwuJ+qCBn3xnRbqGmXSt/GlS2+twIAACwDzhZGMH/XVcnskQ+eaqi3ounbfeqKCptV/TWBxAKt52nrXj+oxEMpxcWzXTiweadPPZrSq06h+6Vm5tC2KOnwChvbtaZRjgsAAOQJQy1GsOlMnLqC6NrR7cSwi7ZpvetS70aVxe1Ld1If+VpznpQGLcbAgY0KVwDdf0U5s2X5yFb0wiJlvsd7f52ji1rHuvSlFtQ01Je8XPErBAAAxYezRjnjxc4ycpTLtS8aGimGQtx1lk7vUDtAfbtNDX+91+CeB17MbWSH6vR0ZAiZ0vdDmtO6E7fpsYaVqKKHMznaV6CcPIV6uIXtfKMjVQ+w3JobAABguTDUUs5cHO1p75udaf3LjSV1LPgkruKhFYhwbY9Lc3pT3UrKE/nQNuE0pktN2jyho8mDDtWxDW4drj7e9/vr97hoDw8BAACUBHo8jKSiu3Rmx+fPNqFXfjlKs/s30NuXEzM3je9AufkKSZExSxDoVXQRMAAAgOJC4GEinWoH0NlZvQoNLHiGCQ9rWJpAT2lRrycaSkuvAwAAlAQCDxOytN6M4gj11dSueK55CE1sr0lEBQAAKCnrOxOCSWkv8BboiWEXAAAoGwQeUKQN49rT/9pVo1c7VTf3oQAAgJXDUAsUqUGwt7jk5+dTmrkPBgAArBp6PAAAAMBkEHgAAACAySDwAAAAAJNB4AEAAAAmg8ADAAAATAaBBwAAAJgMptMSkUKhENcpKSnl8no87TQ1NZVcXFzIzk4+sZ0c2yXHNsm1XXJsk1zbJcc2yb1daWlpkvOhMSHwIBK/SCw0NNTchwIAAGDW86G3t7dR36OCwhThjRVEe7GxseTp6SkWaysr7jnhICY6Opq8vLxILuTYLjm2Sa7tkmOb5NouObbJFtoVFRVFderUMXpvDno8ONHFzo5CQkLK/XX5F1NOv5xybpcc2yTXdsmxTXJtlxzbJOd2ValSxSRDSPIZpAIAAACLh8ADAAAATAaBhxE4OzvTe++9J67lRI7tkmOb5NouObZJru2SY5sY2lU+kFwKAAAAJoMeDwAAADAZBB4AAABgMgg8AAAAwGQQeAAAAIDJIPAwYO7cudSiRQtRyTQwMJCefPJJunjxomSfzMxMGjNmDPn7+5OHhwcNHDiQ7ty5I9ln3LhxFBkZKTKFIyIiDL7Xli1bqHXr1uK9AgICxOvcuHHD6tv1xx9/iMfc3NwoPDycPv30U6O0qbzaderUKRo0aJCo3ufq6kr16tWjr776Su+9du3aRc2aNRNtr1mzJi1dutSq2xQXF0cvvPAC1a5dWxQOmjBhglHaY+p2rVmzhnr06CH+T3GhpzZt2oj/a9bcpn379lG7du3Ea/A+devWpXnz5hmlTaZsl7b9+/eTg4NDoX9XrKld/LeCK2HrXuLj4622TSwrK4veeecd8Xed/w5WrVqVfvrpp5IdMM9qAalevXoplixZojh79qzi5MmTij59+ijCwsIUaWlp6n1ee+01RWhoqGLHjh2Ko0ePKlq3bq1o27at5HXGjh2r+PbbbxVDhgxRNGnSRO99rl27pnB2dlZMmzZNceXKFcWxY8cUHTt2VDRt2tSq27Vx40aFg4ODYuHChYqrV68q1q9fr6hcubLim2++sdh2LV68WDFu3DjFrl27xDH/+uuvCldXV8kx8+fl5uammDRpkiIqKko8Zm9vr9i8ebPVtun69etin59//lkRERGhGD9+fLm3xRzt4nZ8/PHHisOHDysuXbok/o85Ojoqjh8/brVt4mNfvny5eB/+3Hgf/n38/vvvy71NpmyXyoMHDxTVq1dX9OzZ0+DfFWtr17///sszRhUXL15UxMXFqS95eXlW2yb2xBNPKFq1aqXYtm2b+D08cOCAYt++fSU6XgQexZCQkCB+gXbv3i3uJyUliT9iq1atUu9z/vx5sc/Bgwf1nv/ee+8Z/I/Ez+cTtPYv4t9//62oUKGCIjs7W2Gt7Ro0aJDi6aeflmz7+uuvFSEhIYr8/HyFpbdLZfTo0YouXbqo77/55puKBg0aSPZ57rnnxH96a22Ttk6dOhk98DBHu1Tq16+vmDVrlkJObXrqqacUgwcPVpiCsdvF/5emT59e6N8Va2uXKvDggMrUEozUpk2bNim8vb0V9+7dK9PxYailGJKTk8W1n5+fuD527Bjl5ORQ9+7d1ftwt2dYWBgdPHiw2K/LwxXcvb1kyRLKy8sT7/Prr7+K13V0dCRrbRd3xfGy0dq46y4mJoZu3rxJ1tIufh3VazDeV/s1WK9evUr0s7G0NpmbqdqlWs7cFG03VZtOnDhBBw4coE6dOpXr8T/qeIzVLv4beO3aNVHEytSM/XnxsFHlypXF0B8PJVlzm/7++29q3rw5ffLJJ2JdFx6inTx5MmVkZJTo+LBIXBH4DxaPe/PYasOGDcU2HqNzcnIiHx8fyb5BQUElGr+rVq0abd26lZ599ll69dVXRfDBY9EbN24ka24Xn4wnTpxIw4cPpy5dutCVK1fo888/V+cU8JigpbeL/6CvXLmSNmzYoN7G+/JzdF+DV3bk/3gcXFlbm8zJlO367LPPKC0tTfxfs/Y28YKWiYmJlJubSzNnzqSRI0eSsRmzXZcvX6apU6fS3r17RX6HKRmzXRxsfPfdd+JEzV/GFi1aRJ07d6b//vtP5IlZY5s4OORcI/5iuXbtWrp79y6NHj2a7t27J4LH4kLgUQROxjl79qz4YZc3/sBffvllGjZsmEjq4W9kM2bMoKeffpq2bdsmEpGssV3cpqtXr9Ljjz8uomxO7hs/frz4I2nslQ/Lo138/P79+4tvXz179iRzk2ObTNmu5cuX06xZs+ivv/4SiXfW3iY+QXMQdejQIXHC5iRn/vthje3iL1uc3MyfD397NjVjfl68vDxfVNq2bSv+Ls6bN0/0bFtjmzio4fPSsmXLyNvbW2z74osvxDlrwYIFxf/yVaaBGpkbM2aMyEvgpEJtnJxjaOyOk3m++OILvdcpbMySxzObN28u2RYdHV3kuJult0slNzdXERMTo8jKyhIJp/zaPPZoye06d+6cIjAwUPH222/rvX6HDh30ciB++uknhZeXl8Ja22SuHA9Ttev3338XCXKc4GxspvysVN5//31F7dq1FdbaLn4uvwYnaasunOOm2sbvYY3tKszkyZNFUqe1tmno0KGKGjVqSLZxoj2/NidxFxcCDwM4AZI/wODgYIM/TFWizurVq9XbLly4UOIkTJ4d0bJlS8m22NhY8Tr79+9XWGu7DOEZMG3atFEYQ3m1izPC+T/clClTDL4PJ5c2bNhQL5HWGMmlpmqTqQMPU7aLZ4C4uLgo1q1bpzAmc3xWKpwsGx4errDWdnFi/ZkzZySXUaNGKerUqSNua8/KsKZ2FaZ79+4iIdha28QzqDiQT01NVW/j/192dnaK9PT0Yh8vAg8D+BefM3d5WpH2NCjtHyxPTeJocefOnWJqEp9UdU+sly9fVpw4cULx6quvim8lfJsv3AOgikI5uuc/HvzLwtNp+STGf0hK8iFaWrsSExPFVFrOmubtPEWLTwD//fdfubepvNrFf+QCAgLEDAHt19DuoVFNp+X/lNy2+fPnG206ranaxFSfX2RkpOKFF14Qt/lbjzGYql3Lli0TM8b4M9Leh/8AW2ubeAo7z3rjvxV8WbRokcLT01PxzjvvlHubTNkuXcae1WKqds2bN0+clPnvJe/PQT2foLdv3261beKAg3tUeNYi/43gWTO1atVSjBw5skTHi8DDAI4CDV14nrRKRkaGmGrk6+srTkYcxfKHpPsN0tDr8Nxn7a5grtvh7u4uPnSeI80nNWtuFwce3J3IbeLX6Natm+LQoUNGaVN5tYv/2Bl6Dd1vkzxFjutdODk5iZoD2u9hrW0qzj7W1q7CfkeHDRtmtW3iKek8nZufz8N7/HdjwYIFRqkLYcp2mTrwMFW7uI4MD0vwly4/Pz9F586dxUnf2j8rPj9xzw33fHAQwj33Jf2iXKHgoAEAAACMDnU8AAAAwGQQeAAAAIDJIPAAAAAAk0HgAQAAACaDwAMAAABMBoEHAAAAmAwCDwAAADAZBB4AAABgMgg8AAAAwGQQeAAAAIDJIPAAAAAAk0HgAQAAAGQq/wel+FM28PmCKAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1500x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Fetching and Plotting Complete ---\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import json # Explicitly import json\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Portfolio Configuration\n",
        "# (Combines content from your previous Cell 2 and Cell 3)\n",
        "# -------------------------------------------------------------\n",
        "API_KEY = \"1VWXIAX2LM6F6WV0\" # Your Alpha Vantage API key\n",
        "START_YEAR = 2018           # Filter data from this year onwards\n",
        "\n",
        "TICKERS = [ 'QQQ'\n",
        "    # Nasdaq Top 20 (US Equities - Large Cap)\n",
        "        # \"NVDA\", \"MSFT\", \"AAPL\", \"GOOGL\", \"AMZN\", \"META\", \"TSLA\", \"AVGO\", \"TSM\", \"INTC\",\n",
        "    # \"ADBE\", \"CMCSA\", \"CSCO\", \"PEP\", \"TXN\", \"PYPL\", \"QCOM\", \"AMGN\", \"GILD\", \"CHTR\",\n",
        "\n",
        "    # # Europe Top (European Equities - Broad/Large Cap)\n",
        "    # \"SAP\", \"ASML\", \"MC.PA\", \"RMS.PA\", \"OR.PA\", \"AZN\", \"VGK\",\n",
        "\n",
        "    # # Asia/Global Diversifiers (Asian Equities & Resources)\n",
        "    # \"XOM\", \"TSM\", \"TCEHY\", \"FXI\", \"EWY\",\n",
        "\n",
        "    # Crypto (from Alpha Vantage, use SYMBOL-USD format)\n",
        "    # \"BTC-USD\", \"ETH-USD\"\n",
        "]\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Robust Data Fetching Function\n",
        "# (Content from your previous working fetch_alpha_vantage_data_robust function)\n",
        "# -------------------------------------------------------------\n",
        "def fetch_alpha_vantage_data_robust(ticker: str, api_key: str, output_size: str = \"full\") -> pd.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Fetch daily OHLCV data from Alpha Vantage for a single ticker (adjusted for stocks).\n",
        "    Handles both TIME_SERIES_DAILY_ADJUSTED for stocks and DIGITAL_CURRENCY_DAILY for crypto.\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.alphavantage.co/query\"\n",
        "    df = None\n",
        "\n",
        "    def print_common_msgs(j: dict, ticker: str) -> bool:\n",
        "        \"\"\"Prints common Alpha Vantage messages (Note, Error Message, etc.) and indicates if it's a failure.\"\"\"\n",
        "        is_failure = False\n",
        "        for k in (\"Note\", \"Error Message\", \"Information\", \"Message\"):\n",
        "            if k in j:\n",
        "                print(f\"{ticker}: {k}: {j[k]}\")\n",
        "                if k in (\"Error Message\", \"Information\", \"Message\") or \"frequency\" in j[k].lower():\n",
        "                    is_failure = True\n",
        "        return is_failure\n",
        "\n",
        "    # Crypto data\n",
        "    if ticker.endswith(\"-USD\"):\n",
        "        params = {\n",
        "            \"function\": \"DIGITAL_CURRENCY_DAILY\",\n",
        "            \"symbol\": ticker.split(\"-\")[0],\n",
        "            \"market\": \"USD\",\n",
        "            \"apikey\": api_key,\n",
        "        }\n",
        "        try:\n",
        "            r = requests.get(base_url, params=params, timeout=20)\n",
        "            r.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "            j = r.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request failed for {ticker}: {e}\")\n",
        "            return None\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"{ticker}: Response not JSON: {r.text[:200]}\")\n",
        "            return None\n",
        "\n",
        "        if print_common_msgs(j, ticker):\n",
        "            return None\n",
        "        \n",
        "        key = \"Time Series (Digital Currency Daily)\"\n",
        "        if key not in j:\n",
        "            print(f\"{ticker}: missing '{key}' in response for crypto\")\n",
        "            return None\n",
        "        \n",
        "        ts = j[key]\n",
        "        df = pd.DataFrame.from_dict(ts, orient=\"index\").astype(float)\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        \n",
        "        # Crypto close column is '4. close' based on your diagnostic\n",
        "        close_col = \"4. close\" \n",
        "        \n",
        "        if close_col not in df.columns:\n",
        "            print(f\"{ticker}: close column not found in crypto payload. Available: {df.columns.tolist()}\")\n",
        "            return None\n",
        "\n",
        "        df = df.rename(columns={close_col: \"Close\"})\n",
        "        df = df[[\"Close\", \"1. open\", \"2. high\", \"3. low\"]].sort_index() # Include Open, High, Low\n",
        "        df = df.rename(columns={\"1. open\": \"Open\", \"2. high\": \"High\", \"3. low\": \"Low\"}) # Rename for consistency\n",
        "        df[\"Ticker\"] = ticker\n",
        "        return df\n",
        "\n",
        "    # Stock data (adjusted for splits)\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
        "        \"symbol\": ticker,\n",
        "        \"outputsize\": output_size,\n",
        "        \"apikey\": api_key,\n",
        "    }\n",
        "    try:\n",
        "        r = requests.get(base_url, params=params, timeout=20)\n",
        "        r.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        j = r.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed for {ticker}: {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"{ticker}: Response not JSON: {r.text[:200]}\")\n",
        "        return None\n",
        "\n",
        "    if print_common_msgs(j, ticker):\n",
        "        return None\n",
        "    \n",
        "    # Accept either \"Time Series (Daily Adjusted)\" or \"Time Series (Daily)\" keys\n",
        "    key = None\n",
        "    if \"Time Series (Daily Adjusted)\" in j:\n",
        "        key = \"Time Series (Daily Adjusted)\"\n",
        "    elif \"Time Series (Daily)\" in j:\n",
        "        key = \"Time Series (Daily)\"\n",
        "    \n",
        "    if key is None:\n",
        "        print(f\"{ticker}: no daily time series key present in response\")\n",
        "        return None\n",
        "    \n",
        "    ts = j[key]\n",
        "    df = pd.DataFrame.from_dict(ts, orient=\"index\").astype(float)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    \n",
        "    # Prefer adjusted close when available, else fallback to raw close\n",
        "    close_col = None\n",
        "    if \"5. adjusted close\" in df.columns:\n",
        "        close_col = \"5. adjusted close\"\n",
        "    elif \"4. close\" in df.columns:\n",
        "        close_col = \"4. close\"\n",
        "    \n",
        "    if close_col is None:\n",
        "        print(f\"{ticker}: no close column found (expected '5. adjusted close' or '4. close'). Available: {df.columns.tolist()}\")\n",
        "        return None\n",
        "    \n",
        "    # Include '1. open', '2. high', '3. low' for stocks\n",
        "    df = df.rename(columns={\n",
        "        close_col: \"Close\",\n",
        "        \"1. open\": \"Open\",\n",
        "        \"2. high\": \"High\",\n",
        "        \"3. low\": \"Low\",\n",
        "        \"6. volume\": \"Volume\" # Include volume as well\n",
        "    })\n",
        "    # Select desired columns and ensure correct order\n",
        "    df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].sort_index()\n",
        "    df[\"Ticker\"] = ticker\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Main Data Fetching and Plotting Workflow\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- Fetching Full Portfolio Data ---\")\n",
        "all_data = []\n",
        "\n",
        "for ticker in TICKERS:\n",
        "    print(f\"Fetching {ticker} (full history)...\")\n",
        "    # Using the robust fetcher with output_size='full'\n",
        "    df = fetch_alpha_vantage_data_robust(ticker, API_KEY, output_size=\"full\") \n",
        "    if df is not None:\n",
        "        # Filter from START_YEAR\n",
        "        df = df[df.index.year >= START_YEAR]\n",
        "        all_data.append(df)\n",
        "        print(f\"Successfully fetched {len(df)} records for {ticker}\")\n",
        "    time.sleep(12) # Respect Alpha Vantage API limit (5 requests/minute)\n",
        "\n",
        "if not all_data:\n",
        "    raise RuntimeError(\"No data was fetched for any ticker from Alpha Vantage. Please check API key, internet connection, or ticker symbols.\")\n",
        "\n",
        "data = pd.concat(all_data)\n",
        "print(\"\\nâœ… All available data fetched and combined successfully!\")\n",
        "print(f\"Combined data shape: {data.shape}\")\n",
        "print(f\"Tickers in combined data: {data['Ticker'].unique().tolist()}\")\n",
        "print(f\"Data covers: {data.index.min().date()} to {data.index.max().date()}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Plotting - Individual Ticker Performance (small multiples)\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- Plotting Individual Ticker Performance ---\")\n",
        "tickers_to_plot = data['Ticker'].unique()\n",
        "n_plots = len(tickers_to_plot)\n",
        "# Determine grid size for subplots\n",
        "n_cols = 3 # You can adjust this for wider or narrower grids\n",
        "n_rows = (n_plots + n_cols - 1) // n_cols\n",
        "\n",
        "fig_multiples, axes_multiples = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 3 * n_rows), sharex=True)\n",
        "axes_multiples = axes_multiples.flatten() # Flatten for easy iteration\n",
        "\n",
        "for i, ticker in enumerate(tickers_to_plot):\n",
        "    ax = axes_multiples[i]\n",
        "    ticker_data = data[data['Ticker'] == ticker]\n",
        "\n",
        "    # Ensure unique index by grouping on Date if duplicates exist (shouldn't if data is clean)\n",
        "    if not ticker_data.index.is_unique:\n",
        "        ticker_data = ticker_data.groupby(ticker_data.index).mean(numeric_only=True)\n",
        "\n",
        "    # Plot closing price, ensuring 'Close' column exists\n",
        "    if 'Close' in ticker_data.columns:\n",
        "        ax.plot(ticker_data.index, ticker_data['Close'], label=ticker, linewidth=1.5)\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: 'Close' column not found for {ticker}. Skipping individual plot.\")\n",
        "        continue\n",
        "\n",
        "    # Formatting\n",
        "    ax.set_title(ticker)\n",
        "    ax.set_ylabel('Price')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc='upper left')\n",
        "\n",
        "# Set common X-axis locators/formatters only on the bottom row for clarity\n",
        "for ax_row in axes_multiples.reshape(n_rows, n_cols)[-1]:\n",
        "    ax_row.xaxis.set_major_locator(mdates.YearLocator())\n",
        "    ax_row.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    \n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes_multiples)):\n",
        "    fig_multiples.delaxes(axes_multiples[j])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust for suptitle\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Data Fetching and Plotting Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Updated Indicator class with pandas_ta import!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/chielg/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas_ta/__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import get_distribution, DistributionNotFound\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# UPDATED INDICATOR CLASS - WITH PANDAS_TA IMPORT\n",
        "# ================================================================================\n",
        "\n",
        "import pandas_ta as ta\n",
        "\n",
        "class Indicator:\n",
        "    def __init__(self, data: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Updated technical indicator suite using pandas_ta\n",
        "        :param data: DataFrame with OHLCV columns and datetime index\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "    \n",
        "    def kama_short(self, period: int = 12) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Calculate short-term KAMA for crossover strategy.\n",
        "        \"\"\"\n",
        "        return ta.kama(self.data['Close'], length=period)\n",
        "    \n",
        "    def kama_long(self, period: int = 30) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Calculate long-term KAMA for crossover strategy.\n",
        "        \"\"\"\n",
        "        return ta.kama(self.data['Close'], length=period)\n",
        "    \n",
        "    def supertrend(self, period: int = 14, multiplier: float = 2.0) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Calculate Supertrend using pandas_ta.\n",
        "        Supertrend combines trend detection with volatility-based trailing stops.\n",
        "        \"\"\"\n",
        "        supertrend_df = ta.supertrend(self.data['High'], self.data['Low'], self.data['Close'], \n",
        "                                    length=period, multiplier=multiplier)\n",
        "        return supertrend_df[f'SUPERT_{period}_{multiplier}']\n",
        "    \n",
        "    def mfi(self, period: int = 14) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Calculate Money Flow Index (MFI) using pandas_ta.\n",
        "        \n",
        "        MFI is a momentum indicator that uses both price and volume.\n",
        "        \"\"\"\n",
        "        return ta.mfi(self.data['High'], self.data['Low'], self.data['Close'], self.data['Volume'], length=period)\n",
        "\n",
        "print(\"âœ… Updated Indicator class with pandas_ta import!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_comprehensive_grid_search_optimized(data: pd.DataFrame, indicator_params: dict,\n",
        "                                indicator_class: Indicator, max_combinations_per_indicator: int = 50) -> dict:\n",
        "    \"\"\"\n",
        "    Run comprehensive grid search across all indicators and all tickers.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame with OHLCV data and Ticker column\n",
        "        indicator_params: Dictionary of parameter ranges for each indicator\n",
        "        indicator_class: Instance of Indicator class\n",
        "        max_combinations_per_indicator: Limit combinations per indicator to avoid explosion\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results organized by ticker and indicator\n",
        "    \"\"\"\n",
        "    print(\"ðŸš€ Starting comprehensive grid search...\")\n",
        "    print(f\"Available tickers: {data['Ticker'].unique().tolist()}\")\n",
        "    print(f\"Available indicators: {list(indicator_params.keys())}\")\n",
        "\n",
        "    # Results container - organized by ticker, then indicator\n",
        "    all_results = {}\n",
        "\n",
        "    # Get unique tickers\n",
        "    tickers = data['Ticker'].unique()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"\\nðŸ“Š Processing {ticker}...\")\n",
        "        all_results[ticker] = {}\n",
        "\n",
        "        # Get data for this ticker\n",
        "        ticker_data = data[data['Ticker'] == ticker].copy()\n",
        "        if len(ticker_data) < 100:  # Skip if too little data\n",
        "            print(f\"  âš ï¸  Skipping {ticker} - insufficient data ({len(ticker_data)} days)\")\n",
        "            continue\n",
        "\n",
        "        # Create indicator instance for this ticker\n",
        "        ticker_indicator = Indicator(ticker_data)\n",
        "\n",
        "        for indicator_name in indicator_params.keys():\n",
        "            print(f\"  ðŸ” Testing {indicator_name} on {ticker}...\")\n",
        "\n",
        "            # Generate parameter combinations\n",
        "            param_combinations = generate_param_combinations(indicator_name)\n",
        "\n",
        "            # Limit combinations if specified\n",
        "            if len(param_combinations) > max_combinations_per_indicator:\n",
        "                import random\n",
        "                param_combinations = random.sample(param_combinations, max_combinations_per_indicator)\n",
        "                print(f\"    ðŸ“‰ Limited to {len(param_combinations)} random combinations out of {len(generate_param_combinations(indicator_name))}\")\n",
        "\n",
        "            indicator_results = []\n",
        "\n",
        "            for i, params in enumerate(tqdm(param_combinations, desc=f\"    {indicator_name} combinations\")):\n",
        "                try:\n",
        "                    # Calculate indicator values based on parameters\n",
        "                    if indicator_name == 'RSI':\n",
        "                        indicator_values = ticker_indicator.rsi(params['period'])\n",
        "                    elif indicator_name == 'ADX':\n",
        "                        indicator_values = ticker_indicator.adx(params['period'])\n",
        "                    elif indicator_name == 'KAMA':\n",
        "                        indicator_values = ticker_indicator.kama(\n",
        "                            er_period=params['er_period'],\n",
        "                            fast_period=params['fast_period'],\n",
        "                            slow_period=params['slow_period']\n",
        "                        )\n",
        "                    elif indicator_name == 'ATR':\n",
        "                        indicator_values = ticker_indicator.atr(params['period'])\n",
        "                    elif indicator_name == 'MFI':\n",
        "                        indicator_values = ticker_indicator.mfi(params['period'])\n",
        "                    elif indicator_name == 'Entropy':\n",
        "                        indicator_values = ticker_indicator.entropy(params['period'])\n",
        "\n",
        "                    # Generate signals\n",
        "                    if indicator_name == 'KAMA':\n",
        "                        # KAMA needs close prices for comparison\n",
        "                        params['close'] = ticker_data['Close']\n",
        "                    signals = get_buy_signal(indicator_name, indicator_values, params)\n",
        "\n",
        "                    # Create signals DataFrame\n",
        "                    daily_return = ticker_data['Close'].pct_change()\n",
        "                    position = signals.shift(1).fillna(0).astype('float32')\n",
        "                    strategy_return = (position * daily_return).astype('float32')\n",
        "\n",
        "                    valid_returns = strategy_return.dropna()\n",
        "                    if len(valid_returns) > 0:\n",
        "                        total_return = (1 + valid_returns).prod() - 1\n",
        "                        sharpe = (valid_returns.mean() / valid_returns.std()) * np.sqrt(252) if valid_returns.std() > 0 else 0\n",
        "                        max_drawdown = calculate_max_drawdown((1 + valid_returns).cumprod())\n",
        "                        win_rate = (valid_returns > 0).mean()\n",
        "                    else:\n",
        "                        total_return = sharpe = max_drawdown = win_rate = np.nan\n",
        "\n",
        "                    # Store results\n",
        "                    result = {\n",
        "                        'Indicator': indicator_name,\n",
        "                        'Parameters': params,\n",
        "                        'Total_Return': total_return,\n",
        "                        'Sharpe_Ratio': sharpe,\n",
        "                        'Max_Drawdown': max_drawdown,\n",
        "                        'Win_Rate': win_rate,\n",
        "                        'Total_Signals': len(signals.dropna()),\n",
        "                        'Buy_Signals': (signals == 1).sum(),\n",
        "                        'Sell_Signals': (signals == -1).sum()\n",
        "                    }\n",
        "\n",
        "                    indicator_results.append(result)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    âŒ Error with {indicator_name} params {params}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            # Store results for this indicator\n",
        "            all_results[ticker][indicator_name] = indicator_results\n",
        "\n",
        "            # Show best result for this indicator\n",
        "            if len(indicator_results) > 0:\n",
        "                best_result = max(indicator_results, key=lambda x: x['Sharpe_Ratio'] if not np.isnan(x['Sharpe_Ratio']) else -999)\n",
        "                print(f\"    âœ… Best {indicator_name}: Sharpe={best_result['Sharpe_Ratio']:.3f}, Return={best_result['Total_Return']:.3f}\")\n",
        "\n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization Notes\n",
        "\n",
        "A new function `run_comprehensive_grid_search_optimized` has been added to this notebook. This function is an optimized version of the original `run_comprehensive_grid_search` function, designed to reduce computational and memory overhead.\n",
        "\n",
        "### Key Change:\n",
        "Instead of storing intermediate results as `pandas.DataFrame` objects for each ticker and indicator combination, the `run_comprehensive_grid_search_optimized` function now stores these results directly as lists of dictionaries. This change significantly reduces the memory footprint and speeds up execution, especially for large numbers of parameter combinations.\n",
        "\n",
        "### How to Use:\n",
        "To leverage this optimization, simply replace any calls to `run_comprehensive_grid_search` with `run_comprehensive_grid_search_optimized` in your notebook. For example:\n",
        "\n",
        "```python\n",
        "# Before (original function)\n",
        "# results = run_comprehensive_grid_search(data, INDICATOR_PARAMS, Indicator())\n",
        "\n",
        "# After (optimized function)\n",
        "results = run_comprehensive_grid_search_optimized(data, INDICATOR_PARAMS, Indicator())\n",
        "```\n",
        "\n",
        "This change will not affect the structure of the final `all_results` dictionary, but the internal storage mechanism for `indicator_results` within `all_results[ticker][indicator_name]` will now be a list of dictionaries instead of a DataFrame. If you need a DataFrame for further analysis, you can easily convert it like this:\n",
        "\n",
        "```python\n",
        "# To convert the results to a DataFrame for analysis:\n",
        "# my_df_results = pd.DataFrame(all_results[ticker][indicator_name])\n",
        "```\n",
        "\n",
        "This modification addresses the computational heaviness you were experiencing by optimizing the intermediate data storage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# buying and selling logic of indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-02</th>\n",
              "      <td>156.56</td>\n",
              "      <td>158.5300</td>\n",
              "      <td>156.17</td>\n",
              "      <td>150.564200</td>\n",
              "      <td>32573272.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-03</th>\n",
              "      <td>158.64</td>\n",
              "      <td>160.1700</td>\n",
              "      <td>158.61</td>\n",
              "      <td>152.027187</td>\n",
              "      <td>29383557.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-04</th>\n",
              "      <td>160.58</td>\n",
              "      <td>160.7900</td>\n",
              "      <td>160.08</td>\n",
              "      <td>152.293185</td>\n",
              "      <td>24776125.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-05</th>\n",
              "      <td>161.07</td>\n",
              "      <td>162.0300</td>\n",
              "      <td>160.77</td>\n",
              "      <td>153.822672</td>\n",
              "      <td>26992340.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-08</th>\n",
              "      <td>161.92</td>\n",
              "      <td>162.6300</td>\n",
              "      <td>161.86</td>\n",
              "      <td>154.421167</td>\n",
              "      <td>23159061.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-19</th>\n",
              "      <td>576.39</td>\n",
              "      <td>576.5600</td>\n",
              "      <td>568.25</td>\n",
              "      <td>569.280000</td>\n",
              "      <td>53752635.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-20</th>\n",
              "      <td>568.33</td>\n",
              "      <td>568.4500</td>\n",
              "      <td>558.84</td>\n",
              "      <td>565.900000</td>\n",
              "      <td>76781087.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-21</th>\n",
              "      <td>564.35</td>\n",
              "      <td>566.4900</td>\n",
              "      <td>560.98</td>\n",
              "      <td>563.280000</td>\n",
              "      <td>46436899.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-22</th>\n",
              "      <td>564.67</td>\n",
              "      <td>573.9900</td>\n",
              "      <td>563.27</td>\n",
              "      <td>571.970000</td>\n",
              "      <td>51502129.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-25</th>\n",
              "      <td>570.40</td>\n",
              "      <td>573.2871</td>\n",
              "      <td>569.16</td>\n",
              "      <td>570.320000</td>\n",
              "      <td>34044749.0</td>\n",
              "      <td>QQQ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1922 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Open      High     Low       Close      Volume Ticker\n",
              "2018-01-02  156.56  158.5300  156.17  150.564200  32573272.0    QQQ\n",
              "2018-01-03  158.64  160.1700  158.61  152.027187  29383557.0    QQQ\n",
              "2018-01-04  160.58  160.7900  160.08  152.293185  24776125.0    QQQ\n",
              "2018-01-05  161.07  162.0300  160.77  153.822672  26992340.0    QQQ\n",
              "2018-01-08  161.92  162.6300  161.86  154.421167  23159061.0    QQQ\n",
              "...            ...       ...     ...         ...         ...    ...\n",
              "2025-08-19  576.39  576.5600  568.25  569.280000  53752635.0    QQQ\n",
              "2025-08-20  568.33  568.4500  558.84  565.900000  76781087.0    QQQ\n",
              "2025-08-21  564.35  566.4900  560.98  563.280000  46436899.0    QQQ\n",
              "2025-08-22  564.67  573.9900  563.27  571.970000  51502129.0    QQQ\n",
              "2025-08-25  570.40  573.2871  569.16  570.320000  34044749.0    QQQ\n",
              "\n",
              "[1922 rows x 6 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data with Updated Technical Indicators:\n",
            "Columns: ['Open', 'High', 'Low', 'Close', 'Volume', 'Ticker', 'KAMA_SHORT_12', 'KAMA_LONG_30', 'SUPERTREND_14_2', 'MFI_14']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/tc/1rlgym492m70rw7ddhrpc1_80000gn/T/ipykernel_18375/4169994483.py:42: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[4.61132994e+09 3.90771674e+09 4.28838707e+09 ... 2.39371039e+10\n",
            " 2.93429947e+10 1.94369087e+10]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  return ta.mfi(self.data['High'], self.data['Low'], self.data['Close'], self.data['Volume'], length=period)\n",
            "/var/folders/tc/1rlgym492m70rw7ddhrpc1_80000gn/T/ipykernel_18375/4169994483.py:42: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[5.41117058e+09 9.00569893e+09 4.79547925e+09 6.24085375e+09\n",
            " 7.46051249e+09 6.29793886e+09 1.11516134e+10 1.66475014e+10\n",
            " 1.86421842e+10 1.44424979e+10 1.72902062e+10 5.98396860e+09\n",
            " 6.02284527e+09 7.01829355e+09 7.09520241e+09 1.24771199e+10\n",
            " 9.30474333e+09 5.72799662e+09 9.45125607e+09 6.84746786e+09\n",
            " 4.86301781e+09 5.25439921e+09 1.17532061e+10 7.21972086e+09\n",
            " 1.07298707e+10 1.26126446e+10 1.24822235e+10 1.38658046e+10\n",
            " 1.31631427e+10 9.28763705e+09 7.09937242e+09 5.63681086e+09\n",
            " 7.88431467e+09 5.27327280e+09 1.10322967e+10 8.10415469e+09\n",
            " 5.66478387e+09 9.56510628e+09 4.24729268e+09 8.14827456e+09\n",
            " 4.55526563e+09 4.32437736e+09 3.22917266e+09 6.85174553e+09\n",
            " 6.80414364e+09 5.44557142e+09 8.63061989e+09 5.48204372e+09\n",
            " 6.59678331e+09 7.54640687e+09 5.19510138e+09 1.31538357e+10\n",
            " 8.89611810e+09 7.79939478e+09 5.37234063e+09 4.31126949e+09\n",
            " 5.25978057e+09 3.77944107e+09 5.48732590e+09 3.90901513e+09\n",
            " 7.25404067e+09 1.04501839e+10 1.04130981e+10 6.01072000e+09\n",
            " 1.03324273e+10 6.37751197e+09 5.31356335e+09 7.71920713e+09\n",
            " 8.26118850e+09 8.32829529e+09 6.49377397e+09 5.65847455e+09\n",
            " 5.92973769e+09 5.72154834e+09 7.00232359e+09 6.30107270e+09\n",
            " 5.13560812e+09 4.69631409e+09 1.45166951e+10 1.48180691e+10\n",
            " 1.17819441e+10 1.96194331e+10 2.40015240e+10 1.10329722e+10\n",
            " 1.41480692e+10 1.42006455e+10 1.31470134e+10 1.70409831e+10\n",
            " 2.15404630e+10 1.81189980e+10 1.27264888e+10 6.57118307e+09\n",
            " 8.55633541e+09 1.03403951e+10 1.05706825e+10 1.23134824e+10\n",
            " 1.08379470e+10 1.61013565e+10 3.69812005e+09 1.16656548e+10\n",
            " 1.16684111e+10 1.29267527e+10 1.18084031e+10 7.55298036e+09\n",
            " 9.01727702e+09 1.16799797e+10 1.25972901e+10 1.49078991e+10\n",
            " 2.07829837e+10 8.03002808e+09 8.88457185e+09 1.11306219e+10\n",
            " 6.16077840e+09 4.82303019e+09 9.07494831e+09 6.08665830e+09\n",
            " 5.37141031e+09 4.91553994e+09 5.31470110e+09 4.70360035e+09\n",
            " 6.97217235e+09 4.80447851e+09 5.11217639e+09 4.26342462e+09\n",
            " 3.75523420e+09 4.29330231e+09 4.66224143e+09 6.67587684e+09\n",
            " 6.61080741e+09 4.14311526e+09 1.25549795e+10 8.13723904e+09\n",
            " 6.51761959e+09 5.16869155e+09 4.66734433e+09 3.77330968e+09\n",
            " 3.85698980e+09 5.27742968e+09 4.94044771e+09 5.87902251e+09\n",
            " 8.19387092e+09 7.15345610e+09 1.08214259e+10 6.81912018e+09\n",
            " 1.00528290e+10 1.06224079e+10 1.18858531e+10 8.10827484e+09\n",
            " 6.86257100e+09 4.35206236e+09 7.97055466e+09 4.35426112e+09\n",
            " 7.32579514e+09 7.81866728e+09 1.28049718e+10 4.99923662e+09\n",
            " 4.11466110e+09 3.60240064e+09 6.22664963e+09 4.03024317e+09\n",
            " 3.29271821e+09 3.85279499e+09 3.18850121e+09 5.50887656e+09\n",
            " 5.56790661e+09 4.47915851e+09 3.92528231e+09 3.57852617e+09\n",
            " 7.87568715e+09 1.19572917e+10 1.02224309e+10 1.34213414e+10\n",
            " 6.18884474e+09 4.49108126e+09 9.25116794e+09 6.63707524e+09\n",
            " 3.69161142e+09 4.50517878e+09 1.00750223e+10 3.96581231e+09\n",
            " 4.87789074e+09 5.14569257e+09 4.60823205e+09 4.45640968e+09\n",
            " 5.26564144e+09 3.84422580e+09 5.90115692e+09 5.69500723e+09\n",
            " 4.31167768e+09 7.67310805e+09 6.81331475e+09 7.56049114e+09\n",
            " 6.07637283e+09 8.91309883e+09 6.10759676e+09 2.90311987e+09\n",
            " 3.94497711e+09 5.25879513e+09 4.37662065e+09 2.88106408e+09\n",
            " 3.60570979e+09 4.22584230e+09 3.88816667e+09 3.02781973e+09\n",
            " 2.01940716e+09 3.20491360e+09 3.38806265e+09 7.32810600e+09\n",
            " 3.57229339e+09 1.99892671e+09 5.64890045e+09 6.16754943e+09\n",
            " 2.83579054e+09 4.40535525e+09 1.48454510e+09 4.58271204e+09\n",
            " 3.81833264e+09 5.83368634e+09 5.60605632e+09 5.86750531e+09\n",
            " 8.26943950e+09 9.11705494e+09 8.67096502e+09 1.13302797e+10\n",
            " 5.87058699e+09 5.26291032e+09 9.87883237e+09 1.51003422e+10\n",
            " 1.85490975e+10 2.10726322e+10 1.72672270e+10 2.51997149e+10\n",
            " 3.00554870e+10 2.76920839e+10 1.65505868e+10 1.83551370e+10\n",
            " 2.25186534e+10 1.73005864e+10 2.47622741e+10 1.58758019e+10\n",
            " 1.96582937e+10 2.05392268e+10 2.09458921e+10 1.08774967e+10\n",
            " 9.75839479e+09 8.64071794e+09 9.74724903e+09 8.08833298e+09\n",
            " 1.49058613e+10 9.89232080e+09 1.03167773e+10 9.10325713e+09\n",
            " 1.42760375e+10 1.12137544e+10 9.60331988e+09 6.51653121e+09\n",
            " 1.25671107e+10 9.02073979e+09 1.71140394e+10 1.68197448e+10\n",
            " 7.37450882e+09 1.11677578e+10 8.94040913e+09 1.26529855e+10\n",
            " 8.20123809e+09 1.66344252e+10 1.84431012e+10 1.21621335e+10\n",
            " 7.81826696e+09 1.32173232e+10 1.36230210e+10 7.98621077e+09\n",
            " 1.21023918e+10 9.36595519e+09 1.13828359e+10 8.29572284e+09\n",
            " 8.77133270e+09 3.16209242e+10 3.45317647e+10 2.68720888e+10\n",
            " 1.89692371e+10 1.92539118e+10 1.16118589e+10 2.18394908e+10\n",
            " 2.28136626e+10 1.50076172e+10 1.28950328e+10 1.85442772e+10\n",
            " 2.06880561e+10 1.36298066e+10 1.07854886e+10 1.21728466e+10\n",
            " 1.04128839e+10 9.08732877e+09 9.04469791e+09 1.06865878e+10\n",
            " 1.44895998e+10 1.76688748e+10 1.06989539e+10 1.18959852e+10\n",
            " 2.50241218e+10 1.93848551e+10 8.09455710e+09 7.47296423e+09\n",
            " 7.72964177e+09 8.21835368e+09 7.08301841e+09 1.46709943e+10\n",
            " 8.21508469e+09 7.97089267e+09 1.26360711e+10 1.12969946e+10\n",
            " 6.48801170e+09 6.70842894e+09 1.39148660e+10 1.61432383e+10\n",
            " 1.02643150e+10 8.90319919e+09 7.34159149e+09 1.08763279e+10\n",
            " 6.97097314e+09 1.75813154e+10 1.73054908e+10 9.13062139e+09\n",
            " 1.02374560e+10 1.05915664e+10 1.47242639e+10 3.06948839e+10\n",
            " 3.36305437e+10 3.21618043e+10 1.53532599e+10 2.66992048e+10\n",
            " 4.21107513e+10 3.67556781e+10 2.65635547e+10 2.16386857e+10\n",
            " 2.48524423e+10 2.57958869e+10 2.42932363e+10 2.01794663e+10\n",
            " 2.42364543e+10 1.38761504e+10 1.05206042e+10 1.42511427e+10\n",
            " 1.16098386e+10 1.27142073e+10 1.53185769e+10 1.12024241e+10\n",
            " 1.13105396e+10 1.28379407e+10 9.94172841e+09 2.13179893e+10\n",
            " 1.53089260e+10 1.97444634e+10 2.30771117e+10 2.87868451e+10\n",
            " 1.26566675e+10 1.17190911e+10 2.10769439e+10 9.23113062e+09\n",
            " 1.02926601e+10 7.28218640e+09 1.21252107e+10 1.22202949e+10\n",
            " 1.83277059e+10 1.88488506e+10 9.94680562e+09 1.02861774e+10\n",
            " 1.80481205e+10 1.91005649e+10 1.68313628e+10 2.26081726e+10\n",
            " 2.09288948e+10 1.31723539e+10 1.21684802e+10 1.27984827e+10\n",
            " 1.24512236e+10 9.45341824e+09 1.27625435e+10 1.62310108e+10\n",
            " 1.38971733e+10 1.74691470e+10 1.07771958e+10 9.62701011e+09\n",
            " 1.24297739e+10 1.10876823e+10 1.50998576e+10 1.82075460e+10\n",
            " 1.55992370e+10 2.26430239e+10 2.76165587e+10 1.58987461e+10\n",
            " 3.52051696e+10 2.07677627e+10 2.18787663e+10 2.00561646e+10\n",
            " 2.69563861e+10 1.50216604e+10 1.24451309e+10 1.55935274e+10\n",
            " 1.34776347e+10 1.15626058e+10 1.97044112e+10 2.13006268e+10\n",
            " 2.46397540e+10 2.76006266e+10 1.84829282e+10 2.00850503e+10\n",
            " 3.19213073e+10 3.04391857e+10 3.25498434e+10 4.02186945e+10\n",
            " 2.57728484e+10 1.96252866e+10 1.96180174e+10 2.88947242e+10\n",
            " 3.19729031e+10 3.12081289e+10 2.46347110e+10 1.28425025e+10\n",
            " 9.49784605e+09 1.31351185e+10 2.28849522e+10 2.91748506e+10\n",
            " 2.69776743e+10 2.74902716e+10 3.43080401e+10 2.95171569e+10\n",
            " 3.00908683e+10 2.74518712e+10 2.93860944e+10 3.21553542e+10\n",
            " 5.05391159e+10 6.87104909e+10 4.27135320e+10 3.25384684e+10\n",
            " 3.37030541e+10 2.06032530e+10 4.02420770e+10 3.53849546e+10\n",
            " 2.76762661e+10 2.07465734e+10 2.13458840e+10 2.71952772e+10\n",
            " 2.82952234e+10 2.85464168e+10 4.31965262e+10 2.29366261e+10\n",
            " 2.42883233e+10 2.58231189e+10 2.93995638e+10 3.75274247e+10\n",
            " 2.27293101e+10 2.15389696e+10 2.54058745e+10 2.48048168e+10\n",
            " 2.51161530e+10 2.42163847e+10 1.95641212e+10 2.20821639e+10\n",
            " 3.16202951e+10 2.23567369e+10 2.15469905e+10 2.24753120e+10\n",
            " 2.46995612e+10 2.49091584e+10 1.96829444e+10 2.19064595e+10\n",
            " 2.54725065e+10 2.69432990e+10 3.30679468e+10 3.20576828e+10\n",
            " 3.51376282e+10 2.90169156e+10 3.61971772e+10 4.85071507e+10\n",
            " 4.29366132e+10 3.38618708e+10 4.11946401e+10 3.48894400e+10\n",
            " 1.75894359e+10 2.32355422e+10 2.30546880e+10 2.62007402e+10\n",
            " 2.08778035e+10 1.99485954e+10 1.85809291e+10 1.37193863e+10\n",
            " 1.67949301e+10 2.48138740e+10 2.60168550e+10 1.77283274e+10\n",
            " 2.21423823e+10 1.76743336e+10 1.61303799e+10 2.20643669e+10\n",
            " 1.59316171e+10 1.59502498e+10 2.09048052e+10 1.72298133e+10\n",
            " 1.21094210e+10 1.47050600e+10 1.52296767e+10 1.72054202e+10\n",
            " 1.25308674e+10 1.35837585e+10 1.56271075e+10 1.66619382e+10\n",
            " 1.60201890e+10 1.23539655e+10 2.14127257e+10 1.44320444e+10\n",
            " 1.53614708e+10 1.49722204e+10 1.71329816e+10 1.63245760e+10\n",
            " 2.34394283e+10 1.61113352e+10 1.89684105e+10 2.28853478e+10\n",
            " 1.47163708e+10 2.36610996e+10 1.71419607e+10 2.04625598e+10\n",
            " 1.91188778e+10 2.19870123e+10 2.10552442e+10 1.55446800e+10\n",
            " 1.92002883e+10 1.60052429e+10 1.74461749e+10 1.32408921e+10\n",
            " 2.02919391e+10 1.42964142e+10 1.63054457e+10 1.77731189e+10\n",
            " 1.57624074e+10 1.31895303e+10 1.26530603e+10 2.14089893e+10\n",
            " 1.82217647e+10 1.70861794e+10 1.39167880e+10 1.56556366e+10\n",
            " 1.12573495e+10 4.42332148e+09 1.09691244e+10 9.95126275e+09\n",
            " 1.55249018e+10 1.18970792e+10 1.51850637e+10 1.27254024e+10\n",
            " 1.85435333e+10 1.67672330e+10 1.86641294e+10 1.18972132e+10\n",
            " 1.36801476e+10 1.59871340e+10 1.04321109e+10 1.00019895e+10\n",
            " 1.22771548e+10 9.98098814e+09 1.26020424e+10 1.18495209e+10\n",
            " 9.52374264e+09 1.33076930e+10 1.21004922e+10 1.44626732e+10\n",
            " 1.43296182e+10 2.33195534e+10 1.23371886e+10 1.72601981e+10\n",
            " 1.48440946e+10 1.49141518e+10 1.80530434e+10 1.74032442e+10\n",
            " 1.36814839e+10 1.50516718e+10 2.15667464e+10 1.41219068e+10\n",
            " 1.69205684e+10 1.95350944e+10 1.50354955e+10 2.09846976e+10\n",
            " 2.84568358e+10 1.70972676e+10 2.33216957e+10 1.76424943e+10\n",
            " 1.48329730e+10 1.97747090e+10 1.42126207e+10 1.96758659e+10\n",
            " 1.34688468e+10 1.23426725e+10 1.97001715e+10 1.85614666e+10\n",
            " 1.38455973e+10 1.77190891e+10 1.79931197e+10 1.90143039e+10\n",
            " 1.58237016e+10 1.01811362e+10 1.56709499e+10 1.60536907e+10\n",
            " 2.07449774e+10 2.22647252e+10 1.46069813e+10 2.42137322e+10\n",
            " 1.79527349e+10 1.92974107e+10 1.76735746e+10 1.88910454e+10\n",
            " 1.67119145e+10 1.80274881e+10 1.76582624e+10 2.71440038e+10\n",
            " 2.27094904e+10 1.51223129e+10 1.76890544e+10 1.57289875e+10\n",
            " 2.71886465e+10 1.78163801e+10 1.65321040e+10 1.88056543e+10\n",
            " 1.77684170e+10 1.66645480e+10 1.92891522e+10 2.24669621e+10\n",
            " 2.17805126e+10 2.42973397e+10 2.52753077e+10 1.49895471e+10\n",
            " 1.21671978e+10 1.60591257e+10 1.92530977e+10 1.54199899e+10\n",
            " 1.72439715e+10 2.58861270e+10 1.36226940e+10 1.38361569e+10\n",
            " 1.66328051e+10 2.49473985e+10 1.39870239e+10 1.89785588e+10\n",
            " 1.98006398e+10 2.14764498e+10 1.63709948e+10 1.91926621e+10\n",
            " 1.86615334e+10 1.97097773e+10 2.60480334e+10 2.57619717e+10\n",
            " 2.14787799e+10 2.49912419e+10 2.70542808e+10 1.99350980e+10\n",
            " 1.74041011e+10 1.67592095e+10 6.09892636e+09 2.04460453e+10\n",
            " 1.96447572e+10 2.18482234e+10 1.74243784e+10 2.33175613e+10\n",
            " 1.87048697e+10 1.56230634e+10 1.77226163e+10 1.79126131e+10\n",
            " 2.20132066e+10 1.88674507e+10 1.57162852e+10 1.55989803e+10\n",
            " 2.67374437e+10 1.52874154e+10 2.75493790e+10 2.31298564e+10\n",
            " 2.29910204e+10 2.12318045e+10 1.44048575e+10 1.46868280e+10\n",
            " 1.43016490e+10 2.52319505e+10 3.17773285e+10 1.98731584e+10\n",
            " 1.64899249e+10 2.26319179e+10 3.17091652e+10 1.87461618e+10\n",
            " 1.25745966e+10 1.23234031e+10 1.83376143e+10 1.93462643e+10\n",
            " 2.47943881e+10 2.68715032e+10 2.35099018e+10 2.75037728e+10\n",
            " 2.04996496e+10 2.42845353e+10 1.97284767e+10 3.12837487e+10\n",
            " 2.41980265e+10 1.86108109e+10 2.21078490e+10 1.09209166e+10\n",
            " 1.61211327e+10 1.35487929e+10 1.38640899e+10 2.50435253e+10\n",
            " 1.63317231e+10 1.89009136e+10 1.79327188e+10 1.19345275e+10\n",
            " 2.47035727e+10 1.28470801e+10 2.71889789e+10 2.36671500e+10\n",
            " 2.00156654e+10 2.73998516e+10 2.72442274e+10 1.89467271e+10\n",
            " 2.62239477e+10 2.97194463e+10 3.78714193e+10 2.40906053e+10\n",
            " 1.80088880e+10 1.30521064e+10 1.30550059e+10 1.76104478e+10\n",
            " 2.08729566e+10 1.48184481e+10 2.28756470e+10 1.06480850e+10\n",
            " 1.88701961e+10 1.66174842e+10 1.11279429e+10 1.47343245e+10\n",
            " 2.05795315e+10 1.14093907e+10 1.20508011e+10 1.71138659e+10\n",
            " 1.12540869e+10 1.49751463e+10 1.92225121e+10 1.01498892e+10\n",
            " 1.47843225e+10 2.00454587e+10 1.13167967e+10 1.23915115e+10\n",
            " 1.32213701e+10 1.25707496e+10 1.45991781e+10 2.55989008e+10\n",
            " 1.47937264e+10 1.25899506e+10 1.08043021e+10 1.26861825e+10\n",
            " 1.24119234e+10 1.52915519e+10 2.85589466e+10 2.39581154e+10\n",
            " 1.76589389e+10 1.78162754e+10 1.49257615e+10 1.85742063e+10\n",
            " 1.89948998e+10 1.58140113e+10 2.05552278e+10 1.59525724e+10\n",
            " 1.45264138e+10 1.22603117e+10 1.05368726e+10 3.12170175e+10\n",
            " 2.09489142e+10 1.55514419e+10 1.01934632e+10 1.31556299e+10\n",
            " 1.42259972e+10 2.49414830e+10 2.05129605e+10 2.51349942e+10\n",
            " 2.65405317e+10 2.40267916e+10 2.23291945e+10 3.36153754e+10\n",
            " 2.82445760e+10 2.67069562e+10 3.55160982e+10 3.24597848e+10\n",
            " 2.18824235e+10 1.85637873e+10 1.76394663e+10 2.01899787e+10\n",
            " 1.70523550e+10 1.61538168e+10 2.18989998e+10 2.46555504e+10\n",
            " 3.19488843e+10 5.01622720e+10 6.83251055e+10 4.28119430e+10\n",
            " 4.81913416e+10 2.17811076e+10 1.99183616e+10 1.91260561e+10\n",
            " 2.20723251e+10 1.37633222e+10 1.57824044e+10 2.72872828e+10\n",
            " 2.20706134e+10 3.67564500e+10 2.86087605e+10 2.96917315e+10\n",
            " 3.49849222e+10 3.22759292e+10 1.81728017e+10 2.94695562e+10\n",
            " 2.23520262e+10 2.32871124e+10 3.25618512e+10 3.07486668e+10\n",
            " 2.50320705e+10 1.92703419e+10 2.19492520e+10 2.91054638e+10\n",
            " 2.42922855e+10 2.46658972e+10 3.66899656e+10 3.85077218e+10\n",
            " 2.73597475e+10 2.63416796e+10 2.85789872e+10 1.72036740e+10\n",
            " 3.07122847e+10 4.33349896e+10 2.61710623e+10]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  return ta.mfi(self.data['High'], self.data['Low'], self.data['Close'], self.data['Volume'], length=period)\n",
            "/var/folders/tc/1rlgym492m70rw7ddhrpc1_80000gn/T/ipykernel_18375/3435584317.py:22: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  data_with_indicators = data.groupby('Ticker', group_keys=False).apply(apply_indicators)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Ticker</th>\n",
              "      <th>KAMA_SHORT_12</th>\n",
              "      <th>KAMA_LONG_30</th>\n",
              "      <th>SUPERTREND_14_2</th>\n",
              "      <th>MFI_14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2025-08-12</th>\n",
              "      <td>575.16</td>\n",
              "      <td>580.3500</td>\n",
              "      <td>572.4850</td>\n",
              "      <td>580.05</td>\n",
              "      <td>42271441.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>563.877222</td>\n",
              "      <td>562.357977</td>\n",
              "      <td>563.142637</td>\n",
              "      <td>63.812890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-13</th>\n",
              "      <td>582.76</td>\n",
              "      <td>583.3199</td>\n",
              "      <td>578.9400</td>\n",
              "      <td>580.34</td>\n",
              "      <td>41209294.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>564.551261</td>\n",
              "      <td>563.752632</td>\n",
              "      <td>568.177591</td>\n",
              "      <td>63.818841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-14</th>\n",
              "      <td>578.28</td>\n",
              "      <td>581.8800</td>\n",
              "      <td>577.9100</td>\n",
              "      <td>579.89</td>\n",
              "      <td>45425043.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>565.217192</td>\n",
              "      <td>564.827555</td>\n",
              "      <td>568.177591</td>\n",
              "      <td>57.417560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-15</th>\n",
              "      <td>579.80</td>\n",
              "      <td>579.8400</td>\n",
              "      <td>575.5743</td>\n",
              "      <td>577.34</td>\n",
              "      <td>49480161.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>565.556046</td>\n",
              "      <td>565.377563</td>\n",
              "      <td>568.177591</td>\n",
              "      <td>50.946507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-18</th>\n",
              "      <td>576.44</td>\n",
              "      <td>577.7650</td>\n",
              "      <td>575.2412</td>\n",
              "      <td>577.11</td>\n",
              "      <td>29830957.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>566.041393</td>\n",
              "      <td>566.081985</td>\n",
              "      <td>568.177591</td>\n",
              "      <td>45.033134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-19</th>\n",
              "      <td>576.39</td>\n",
              "      <td>576.5600</td>\n",
              "      <td>568.2500</td>\n",
              "      <td>569.28</td>\n",
              "      <td>53752635.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>566.252644</td>\n",
              "      <td>566.181483</td>\n",
              "      <td>568.177591</td>\n",
              "      <td>44.294657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-20</th>\n",
              "      <td>568.33</td>\n",
              "      <td>568.4500</td>\n",
              "      <td>558.8400</td>\n",
              "      <td>565.90</td>\n",
              "      <td>76781087.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>566.249757</td>\n",
              "      <td>566.176827</td>\n",
              "      <td>576.479700</td>\n",
              "      <td>43.510506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-21</th>\n",
              "      <td>564.35</td>\n",
              "      <td>566.4900</td>\n",
              "      <td>560.9800</td>\n",
              "      <td>563.28</td>\n",
              "      <td>46436899.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>566.214320</td>\n",
              "      <td>566.138180</td>\n",
              "      <td>576.440078</td>\n",
              "      <td>44.989127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-22</th>\n",
              "      <td>564.67</td>\n",
              "      <td>573.9900</td>\n",
              "      <td>563.2700</td>\n",
              "      <td>571.97</td>\n",
              "      <td>51502129.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>566.312964</td>\n",
              "      <td>566.309936</td>\n",
              "      <td>576.440078</td>\n",
              "      <td>45.369993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-08-25</th>\n",
              "      <td>570.40</td>\n",
              "      <td>573.2871</td>\n",
              "      <td>569.1600</td>\n",
              "      <td>570.32</td>\n",
              "      <td>34044749.0</td>\n",
              "      <td>QQQ</td>\n",
              "      <td>566.338633</td>\n",
              "      <td>566.399999</td>\n",
              "      <td>576.440078</td>\n",
              "      <td>51.810000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Open      High       Low   Close      Volume Ticker  \\\n",
              "2025-08-12  575.16  580.3500  572.4850  580.05  42271441.0    QQQ   \n",
              "2025-08-13  582.76  583.3199  578.9400  580.34  41209294.0    QQQ   \n",
              "2025-08-14  578.28  581.8800  577.9100  579.89  45425043.0    QQQ   \n",
              "2025-08-15  579.80  579.8400  575.5743  577.34  49480161.0    QQQ   \n",
              "2025-08-18  576.44  577.7650  575.2412  577.11  29830957.0    QQQ   \n",
              "2025-08-19  576.39  576.5600  568.2500  569.28  53752635.0    QQQ   \n",
              "2025-08-20  568.33  568.4500  558.8400  565.90  76781087.0    QQQ   \n",
              "2025-08-21  564.35  566.4900  560.9800  563.28  46436899.0    QQQ   \n",
              "2025-08-22  564.67  573.9900  563.2700  571.97  51502129.0    QQQ   \n",
              "2025-08-25  570.40  573.2871  569.1600  570.32  34044749.0    QQQ   \n",
              "\n",
              "            KAMA_SHORT_12  KAMA_LONG_30  SUPERTREND_14_2     MFI_14  \n",
              "2025-08-12     563.877222    562.357977       563.142637  63.812890  \n",
              "2025-08-13     564.551261    563.752632       568.177591  63.818841  \n",
              "2025-08-14     565.217192    564.827555       568.177591  57.417560  \n",
              "2025-08-15     565.556046    565.377563       568.177591  50.946507  \n",
              "2025-08-18     566.041393    566.081985       568.177591  45.033134  \n",
              "2025-08-19     566.252644    566.181483       568.177591  44.294657  \n",
              "2025-08-20     566.249757    566.176827       576.479700  43.510506  \n",
              "2025-08-21     566.214320    566.138180       576.440078  44.989127  \n",
              "2025-08-22     566.312964    566.309936       576.440078  45.369993  \n",
              "2025-08-25     566.338633    566.399999       576.440078  51.810000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to all_tickers_with_updated_indicators.csv\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# UPDATED APPLY_INDICATORS FUNCTION - MATCHING RECENT CHANGES\n",
        "# ================================================================================\n",
        "\n",
        "# Function to apply all indicators to a single ticker's data\n",
        "def apply_indicators(ticker_data):\n",
        "    indicator = Indicator(ticker_data)\n",
        "    \n",
        "    return ticker_data.assign(\n",
        "        # KAMA crossover components (short and long for crossover signals)\n",
        "        KAMA_SHORT_12=indicator.kama_short(12),\n",
        "        KAMA_LONG_30=indicator.kama_long(30),\n",
        "        \n",
        "        # Supertrend (replaces ATR)\n",
        "        SUPERTREND_14_2=indicator.supertrend(14, 2.0),\n",
        "        \n",
        "        # MFI (unchanged)\n",
        "        MFI_14=indicator.mfi(14)\n",
        "    )\n",
        "\n",
        "# Apply indicators to all tickers\n",
        "data_with_indicators = data.groupby('Ticker', group_keys=False).apply(apply_indicators)\n",
        "\n",
        "# Display results\n",
        "print(\"Data with Updated Technical Indicators:\")\n",
        "print(f\"Columns: {data_with_indicators.columns.tolist()}\")\n",
        "display(data_with_indicators.tail(10))\n",
        "\n",
        "# Optional: Save to CSV\n",
        "data_with_indicators.to_csv('all_tickers_with_updated_indicators.csv')\n",
        "print(\"Saved to all_tickers_with_updated_indicators.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base OHLCV data with tickers:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-02</th>\n",
              "      <td>QQQ</td>\n",
              "      <td>156.56</td>\n",
              "      <td>158.53</td>\n",
              "      <td>156.17</td>\n",
              "      <td>150.564200</td>\n",
              "      <td>32573272.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-03</th>\n",
              "      <td>QQQ</td>\n",
              "      <td>158.64</td>\n",
              "      <td>160.17</td>\n",
              "      <td>158.61</td>\n",
              "      <td>152.027187</td>\n",
              "      <td>29383557.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-04</th>\n",
              "      <td>QQQ</td>\n",
              "      <td>160.58</td>\n",
              "      <td>160.79</td>\n",
              "      <td>160.08</td>\n",
              "      <td>152.293185</td>\n",
              "      <td>24776125.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Ticker    Open    High     Low       Close      Volume\n",
              "2018-01-02    QQQ  156.56  158.53  156.17  150.564200  32573272.0\n",
              "2018-01-03    QQQ  158.64  160.17  158.61  152.027187  29383557.0\n",
              "2018-01-04    QQQ  160.58  160.79  160.08  152.293185  24776125.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tickers included: ['QQQ']\n",
            "Date range: 2018-01-02 00:00:00 to 2025-08-25 00:00:00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create master container for all calculations\n",
        "all_calculations = {}\n",
        "\n",
        "# Add OHLC data with ticker information\n",
        "base_df = data[['Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
        "all_calculations['base'] = base_df\n",
        "\n",
        "print(\"Base OHLCV data with tickers:\")\n",
        "display(base_df.head(3))\n",
        "print(f\"\\nTickers included: {base_df['Ticker'].unique().tolist()}\")\n",
        "print(f\"Date range: {base_df.index.min()} to {base_df.index.max()}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "moet combinaties van alle DF's including booleans maken en buy sell signals en van daar uit beslissen wat en hoe, kan beginnen met combinatie van 1 2 of 3 combinaties, maar in principe zou dat het moeten worden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# >tickers , gewoon per ticker want dan kan alles in 1, lekker makkelijk, een raam kan je altijd nog bouwen\n",
        "\n",
        "# > buy signals\n",
        "\n",
        "# > elke soort combinatie van tickers en lookbacks , begin gwn met 1 kolom en 1 lookback\n",
        "\n",
        "# > df met signals , etc , \n",
        "\n",
        "# >backtest module,\n",
        "\n",
        "# > als je deze logica hebt kan je door naar een bayesian en eventueel GP\n",
        "\n",
        "# > out of sample testing kan later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Updated parameter ranges and signal logic (NO ADX)!\n",
            "Available indicators: ['KAMA', 'Supertrend', 'MFI']\n",
            "Example KAMA combinations: 7700\n",
            "Example Supertrend combinations: 190\n",
            "Example MFI combinations: 78\n"
          ]
        }
      ],
      "source": [
        "# Add pandas import at the top\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define parameter ranges for each indicator (NO ADX)\n",
        "INDICATOR_PARAMS = {\n",
        "    'KAMA': {\n",
        "        'er_period': list(range(6, 13)),      # Efficiency ratio periods (7-90 days)\n",
        "        'fast_period': list(range(10, 35)),    # Fast EMA periods (7-90 days)\n",
        "        'slow_period': list(range(36, 80)),     # Slow EMA periods (7-90 days)\n",
        "    },\n",
        "    'Supertrend': {\n",
        "        'period': list(range(10, 48)),  # 7 to 90 days\n",
        "        'multiplier': [1.0, 1.5, 2.0, 2.5, 3.0]  # ATR multiplier for Supertrend\n",
        "    },\n",
        "    'MFI': {\n",
        "        'period': list(range(12, 90)),  # 7 to 90 days\n",
        "        'overbought': [80],  # Sell when MFI > overbought\n",
        "        'oversold': [20]  # Buy when MFI < oversold\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define buy signal logic for each indicator (NO ADX)\n",
        "def get_buy_signal(indicator_name: str, data: pd.Series, params: dict) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Generate buy signals for different indicators.\n",
        "    Returns: 1 (buy), -1 (sell), 0 (hold), NaN (no signal)\n",
        "    \"\"\"\n",
        "    if indicator_name == 'KAMA':\n",
        "        # KAMA crossover strategy - buy when short KAMA crosses above long KAMA\n",
        "        short_kama = params.get('short_kama', pd.Series(index=data.index))\n",
        "        long_kama = params.get('long_kama', pd.Series(index=data.index))\n",
        "        \n",
        "        # Generate crossover signals\n",
        "        crossover_up = (short_kama > long_kama) & (short_kama.shift(1) <= long_kama.shift(1))\n",
        "        crossover_down = (short_kama < long_kama) & (short_kama.shift(1) >= long_kama.shift(1))\n",
        "        \n",
        "        signals = pd.Series(0, index=data.index)\n",
        "        signals[crossover_up] = 1    # Buy signal\n",
        "        signals[crossover_down] = -1 # Sell signal\n",
        "        \n",
        "        return signals\n",
        "\n",
        "    elif indicator_name == 'Supertrend':\n",
        "        # Supertrend signals - buy when price above Supertrend, sell when below\n",
        "        supertrend = data  # The data passed will be the Supertrend values\n",
        "        close = params.get('close', pd.Series(index=data.index))\n",
        "        \n",
        "        return pd.Series(np.where(close > supertrend, 1,\n",
        "                                np.where(close < supertrend, -1, 0)), index=data.index)\n",
        "\n",
        "    elif indicator_name == 'MFI':\n",
        "        overbought = params.get('overbought', 80)\n",
        "        oversold = params.get('oversold', 20)\n",
        "        # Buy when oversold, Sell when overbought\n",
        "        return pd.Series(np.where(data < oversold, 1,\n",
        "                                np.where(data > overbought, -1, 0)), index=data.index)\n",
        "\n",
        "    return pd.Series([0] * len(data), index=data.index)  # Default: no signal\n",
        "\n",
        "# Generate all possible parameter combinations for an indicator\n",
        "def generate_param_combinations(indicator_name: str) -> list:\n",
        "    \"\"\"Generate all possible parameter combinations for a given indicator.\"\"\"\n",
        "    if indicator_name not in INDICATOR_PARAMS:\n",
        "        return []\n",
        "\n",
        "    params = INDICATOR_PARAMS[indicator_name]\n",
        "    param_names = list(params.keys())\n",
        "    param_values = list(params.values())\n",
        "\n",
        "    # Generate all combinations\n",
        "    from itertools import product\n",
        "    combinations = list(product(*param_values))\n",
        "\n",
        "    # Convert to list of dictionaries\n",
        "    return [{param_names[i]: combo[i] for i in range(len(param_names))} for combo in combinations]\n",
        "\n",
        "# Calculate distance between two parameter sets (for optimization)\n",
        "def calculate_param_distance(params1: dict, params2: dict) -> float:\n",
        "    \"\"\"Calculate Euclidean distance between two parameter sets.\"\"\"\n",
        "    if not params1 or not params2:\n",
        "        return float('inf')\n",
        "\n",
        "    common_keys = set(params1.keys()) & set(params2.keys())\n",
        "    if not common_keys:\n",
        "        return float('inf')\n",
        "\n",
        "    distance = 0\n",
        "    for key in common_keys:\n",
        "        distance += (params1[key] - params2[key]) ** 2\n",
        "\n",
        "    return distance ** 0.5\n",
        "\n",
        "print(\"âœ… Updated parameter ranges and signal logic (NO ADX)!\")\n",
        "print(f\"Available indicators: {list(INDICATOR_PARAMS.keys())}\")\n",
        "print(f\"Example KAMA combinations: {len(generate_param_combinations('KAMA'))}\")\n",
        "print(f\"Example Supertrend combinations: {len(generate_param_combinations('Supertrend'))}\")\n",
        "print(f\"Example MFI combinations: {len(generate_param_combinations('MFI'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Creating parameter combinations (ONE CSV, serial, NO ADX)...\n",
            "ðŸ“Š KAMA=7700, Supertrend=190, MFI=78 â†’ Total=114,114,000\n",
            "âœ… Parameter combinations written.\n",
            "ðŸ“¦ File: parameter_combinations.csv\n",
            "ðŸ§® Rows written: 114,114,000\n",
            "\n",
            "================================================================================\n",
            "ðŸ“‹ PARAMETER COMBINATIONS (ONE CSV, NO ADX)\n",
            "================================================================================\n",
            "ðŸ“¦ File: parameter_combinations.csv\n",
            "ðŸ§® Total rows written: 114,114,000\n",
            "ðŸ”¢ KAMA=7700 ST=190 MFI=78\n",
            "\n",
            "ðŸ” Preview (first 10 rows):\n",
            " combination_id  KAMA_er_period  KAMA_fast_period  KAMA_slow_period  Supertrend_period  Supertrend_multiplier  MFI_period  MFI_overbought  MFI_oversold                  parameter_combination\n",
            "              1               6                10                36                 10                    1.0          12              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(12,80,20)\n",
            "              2               6                10                36                 10                    1.0          13              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(13,80,20)\n",
            "              3               6                10                36                 10                    1.0          14              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(14,80,20)\n",
            "              4               6                10                36                 10                    1.0          15              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(15,80,20)\n",
            "              5               6                10                36                 10                    1.0          16              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(16,80,20)\n",
            "              6               6                10                36                 10                    1.0          17              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(17,80,20)\n",
            "              7               6                10                36                 10                    1.0          18              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(18,80,20)\n",
            "              8               6                10                36                 10                    1.0          19              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(19,80,20)\n",
            "              9               6                10                36                 10                    1.0          20              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(20,80,20)\n",
            "             10               6                10                36                 10                    1.0          21              80            20 KAMA(6,10,36)_ST(10,1.0)_MFI(21,80,20)\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# PARAMETER COMBINATIONS (ONE CSV, SERIAL, RAM-THROTTLED, CHUNKED ACROSS ALL INDICATORS)\n",
        "# ================================================================================\n",
        "\n",
        "import os, gc, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "# Throttling knobs (tune as needed)\n",
        "THROTTLE_EVERY   = 50_000   # check RAM every N rows generated\n",
        "THROTTLE_SLEEP_S = 1.0      # sleep when throttling\n",
        "MEM_TARGET       = 0.80     # throttle when â‰¥80% RAM used\n",
        "MIN_FREE_GB      = 1.0      # or when <1 GB free\n",
        "\n",
        "# Flush rows to disk in chunks\n",
        "FLUSH_ROWS = 50_000\n",
        "\n",
        "def _should_throttle() -> bool:\n",
        "    if psutil is None:\n",
        "        return False\n",
        "    vm = psutil.virtual_memory()\n",
        "    return (vm.percent >= MEM_TARGET*100) or (vm.available/1e9 < MIN_FREE_GB)\n",
        "\n",
        "def _row_from_params(lin_id: int, er, fast, slow, st_p, st_mul, mfi_p, mfi_ob, mfi_os) -> dict:\n",
        "    return {\n",
        "        'combination_id': lin_id,\n",
        "        'KAMA_er_period': er,\n",
        "        'KAMA_fast_period': fast,\n",
        "        'KAMA_slow_period': slow,\n",
        "        'Supertrend_period': st_p,\n",
        "        'Supertrend_multiplier': st_mul,\n",
        "        'MFI_period': mfi_p,\n",
        "        'MFI_overbought': mfi_ob,\n",
        "        'MFI_oversold': mfi_os,\n",
        "        'parameter_combination': (\n",
        "            f\"KAMA({er},{fast},{slow})_ST({st_p},{st_mul})_MFI({mfi_p},{mfi_ob},{mfi_os})\"\n",
        "        ),\n",
        "    }\n",
        "\n",
        "def _count_triplen(a, b, c) -> int:\n",
        "    return len(a) * len(b) * len(c)\n",
        "\n",
        "def create_parameter_combinations_df_streamed_onefile(output_csv: str = \"parameter_combinations.csv\") -> dict:\n",
        "    \"\"\"\n",
        "    Build all combinations KAMA x Supertrend x MFI with serial streaming to ONE CSV.\n",
        "    - No temp files\n",
        "    - RAM throttled\n",
        "    - Chunked flushes\n",
        "    - Returns minimal metadata and a small preview\n",
        "    \"\"\"\n",
        "    print(\"ðŸŽ¯ Creating parameter combinations (ONE CSV, serial, NO ADX)...\")\n",
        "\n",
        "    if 'INDICATOR_PARAMS' not in globals():\n",
        "        print(\"âŒ INDICATOR_PARAMS not found. Run the parameters cell first!\")\n",
        "        return {}\n",
        "\n",
        "    k_er   = list(INDICATOR_PARAMS['KAMA']['er_period'])\n",
        "    k_fast = list(INDICATOR_PARAMS['KAMA']['fast_period'])\n",
        "    k_slow = list(INDICATOR_PARAMS['KAMA']['slow_period'])\n",
        "\n",
        "    st_p   = list(INDICATOR_PARAMS['Supertrend']['period'])\n",
        "    st_mul = list(INDICATOR_PARAMS['Supertrend']['multiplier'])\n",
        "\n",
        "    mfi_p  = list(INDICATOR_PARAMS['MFI']['period'])\n",
        "    mfi_ob = list(INDICATOR_PARAMS['MFI']['overbought'])\n",
        "    mfi_os = list(INDICATOR_PARAMS['MFI']['oversold'])\n",
        "\n",
        "    n_kama = _count_triplen(k_er, k_fast, k_slow)\n",
        "    n_st   = _count_triplen(st_p, st_mul, [None])     # 2-D for ST\n",
        "    n_mfi  = _count_triplen(mfi_p, mfi_ob, mfi_os)\n",
        "\n",
        "    # For ST 2-D above, treat len([None]) as 1 so n_st = len(st_p)*len(st_mul)\n",
        "    n_st = len(st_p) * len(st_mul)\n",
        "\n",
        "    total = n_kama * n_st * n_mfi\n",
        "    print(f\"ðŸ“Š KAMA={n_kama}, Supertrend={n_st}, MFI={n_mfi} â†’ Total={total:,}\")\n",
        "\n",
        "    # Prepare output\n",
        "    os.makedirs(os.path.dirname(output_csv) or '.', exist_ok=True)\n",
        "    if os.path.exists(output_csv):\n",
        "        os.remove(output_csv)\n",
        "\n",
        "    rows = []\n",
        "    written = 0\n",
        "    header_written = False\n",
        "\n",
        "    # Precompute multipliers for linear combination_id\n",
        "    # combo_id = 1 + k_index*(n_st*n_mfi) + st_index*n_mfi + mfi_index\n",
        "    n_st_times_n_mfi = n_st * n_mfi\n",
        "\n",
        "    k_index = 0\n",
        "    for er in k_er:\n",
        "        for fast in k_fast:\n",
        "            for slow in k_slow:\n",
        "                st_index = 0\n",
        "                for sp in st_p:\n",
        "                    for sm in st_mul:\n",
        "                        mfi_index = 0\n",
        "                        for mp in mfi_p:\n",
        "                            for mob in mfi_ob:\n",
        "                                for mos in mfi_os:\n",
        "                                    combo_id = 1 + k_index*(n_st_times_n_mfi) + st_index*n_mfi + mfi_index\n",
        "                                    rows.append(_row_from_params(combo_id, er, fast, slow, sp, sm, mp, mob, mos))\n",
        "                                    mfi_index += 1\n",
        "\n",
        "                                    # Flush periodically\n",
        "                                    if len(rows) >= FLUSH_ROWS:\n",
        "                                        pd.DataFrame(rows).to_csv(output_csv, mode='a',\n",
        "                                                                  header=not header_written, index=False)\n",
        "                                        header_written = True\n",
        "                                        written += len(rows)\n",
        "                                        rows.clear()\n",
        "\n",
        "                                    # Throttle RAM if needed\n",
        "                                    if (combo_id % THROTTLE_EVERY) == 0 and _should_throttle():\n",
        "                                        gc.collect()\n",
        "                                        time.sleep(THROTTLE_SLEEP_S)\n",
        "\n",
        "                        st_index += 1\n",
        "                k_index += 1\n",
        "                # small per-KAMA cleanup\n",
        "                gc.collect()\n",
        "\n",
        "    if rows:\n",
        "        pd.DataFrame(rows).to_csv(output_csv, mode='a',\n",
        "                                  header=not header_written, index=False)\n",
        "        written += len(rows)\n",
        "        rows.clear()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"âœ… Parameter combinations written.\")\n",
        "    print(f\"ðŸ“¦ File: {output_csv}\")\n",
        "    print(f\"ðŸ§® Rows written: {written:,}\")\n",
        "\n",
        "    # Small preview\n",
        "    try:\n",
        "        preview = pd.read_csv(output_csv, nrows=10)\n",
        "    except Exception:\n",
        "        preview = pd.DataFrame()\n",
        "\n",
        "    return {\n",
        "        'file': output_csv,\n",
        "        'rows_written': int(written),\n",
        "        'kama': int(n_kama), 'supertrend': int(n_st), 'mfi': int(n_mfi),\n",
        "        'preview': preview\n",
        "    }\n",
        "\n",
        "def show_parameter_combinations(output_csv: str = \"parameter_combinations.csv\"):\n",
        "    meta = create_parameter_combinations_df_streamed_onefile(output_csv=output_csv)\n",
        "    if not meta:\n",
        "        return\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸ“‹ PARAMETER COMBINATIONS (ONE CSV, NO ADX)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"ðŸ“¦ File: {meta['file']}\")\n",
        "    print(f\"ðŸ§® Total rows written: {meta['rows_written']:,}\")\n",
        "    print(f\"ðŸ”¢ KAMA={meta['kama']} ST={meta['supertrend']} MFI={meta['mfi']}\")\n",
        "    if isinstance(meta.get('preview'), pd.DataFrame) and not meta['preview'].empty:\n",
        "        print(\"\\nðŸ” Preview (first 10 rows):\")\n",
        "        print(meta['preview'].to_string(index=False))\n",
        "\n",
        "# Run (serial, one final file, no temp files)\n",
        "show_parameter_combinations(output_csv=\"parameter_combinations.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ================================================================================\n",
        "# # UPDATED APPLY_INDICATORS FUNCTION - MATCHING RECENT CHANGES\n",
        "# # ================================================================================\n",
        "\n",
        "# # Function to apply all indicators to a single ticker's data\n",
        "# def apply_indicators(ticker_data):\n",
        "#     indicator = Indicator(ticker_data)\n",
        "    \n",
        "#     return ticker_data.assign(\n",
        "#         # KAMA crossover components (short and long for crossover signals)\n",
        "#         KAMA_SHORT_12=indicator.kama_short(12),\n",
        "#         KAMA_LONG_30=indicator.kama_long(30),\n",
        "        \n",
        "#         # Supertrend (replaces ATR)\n",
        "#         SUPERTREND_14_2=indicator.supertrend(14, 2.0),\n",
        "        \n",
        "#         # MFI (unchanged)\n",
        "#         MFI_14=indicator.mfi(14)\n",
        "#     )\n",
        "\n",
        "# # Apply indicators to all tickers\n",
        "# data_with_indicators = data.groupby('Ticker', group_keys=False).apply(apply_indicators)\n",
        "\n",
        "# # Display results\n",
        "# print(\"Data with Updated Technical Indicators:\")\n",
        "# print(f\"Columns: {data_with_indicators.columns.tolist()}\")\n",
        "# display(data_with_indicators.tail(10))\n",
        "\n",
        "# # Optional: Save to CSV\n",
        "# data_with_indicators.to_csv('all_tickers_with_updated_indicators.csv')\n",
        "# print(\"Saved to all_tickers_with_updated_indicators.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Use a tiny set to inspect (adjust head() as you like)\n",
        "# combos_to_test = pd.read_csv(\"/Users/chielg/Documents/GitHub/TA_optimization/parameter_combinations.csv\")\n",
        "\n",
        "# print(\"Sample parameter combinations:\")\n",
        "# print(combos_to_test[[\n",
        "#     \"combination_id\",\n",
        "#     \"KAMA_er_period\",\"KAMA_fast_period\",\"KAMA_slow_period\",\n",
        "#     \"Supertrend_period\",\"Supertrend_multiplier\",\n",
        "#     \"MFI_period\",\"MFI_oversold\",\"MFI_overbought\"\n",
        "# ]].to_string(index=False))\n",
        "\n",
        "# # One ticker to walk through\n",
        "# ticker = data[\"Ticker\"].unique()[0]\n",
        "# td = data[data[\"Ticker\"] == ticker].sort_index().copy()\n",
        "# print(f\"\\nTicker: {ticker}\")\n",
        "# print(\"Dates:\", td.index.min().date(), \"â†’\", td.index.max().date())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Missing column provided to 'parse_dates': 'Date'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load data (adjust path if needed)\u001b[39;00m\n\u001b[32m      6\u001b[39m data_path = \u001b[33m\"\u001b[39m\u001b[33m/Users/chielg/Documents/GitHub/TA_optimization/all_tickers_with_updated_indicators.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m data = df.sort_values([\u001b[33m\"\u001b[39m\u001b[33mTicker\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m]).set_index(\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Pick a ticker and build td\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:161\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m    155\u001b[39m         \u001b[38;5;28mself\u001b[39m._validate_usecols_names(\n\u001b[32m    156\u001b[39m             usecols,\n\u001b[32m    157\u001b[39m             \u001b[38;5;28mself\u001b[39m.names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    158\u001b[39m         )\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_parse_dates_presence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m._set_noconvert_columns()\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/base_parser.py:243\u001b[39m, in \u001b[36mParserBase._validate_parse_dates_presence\u001b[39m\u001b[34m(self, columns)\u001b[39m\n\u001b[32m    233\u001b[39m missing_cols = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    234\u001b[39m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m    235\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     )\n\u001b[32m    241\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    244\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing column provided to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mparse_dates\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m     )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    248\u001b[39m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[32m    250\u001b[39m ]\n",
            "\u001b[31mValueError\u001b[39m: Missing column provided to 'parse_dates': 'Date'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_ta as ta\n",
        "\n",
        "# Load data (adjust path if needed)\n",
        "data_path = \"/Users/chielg/Documents/GitHub/TA_optimization/all_tickers_with_updated_indicators.csv\"\n",
        "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
        "data = df.sort_values([\"Ticker\", \"Date\"]).set_index(\"Date\")\n",
        "\n",
        "# Pick a ticker and build td\n",
        "ticker = data[\"Ticker\"].dropna().unique()[0]\n",
        "td = data[data[\"Ticker\"] == ticker].sort_index().copy()\n",
        "print(f\"Ticker: {ticker} | Dates: {td.index.min().date()} â†’ {td.index.max().date()}\")\n",
        "\n",
        "# Ensure Indicator is defined in prior cells\n",
        "ind = Indicator(td)\n",
        "\n",
        "# Take first combo\n",
        "combo = pd.read_csv(\"/Users/chielg/Documents/GitHub/TA_optimization/parameter_combinations.csv\").head(1).iloc[0]\n",
        "print(\"Using combo:\", dict(combo))\n",
        "\n",
        "# KAMA crossover: prefer existing columns if present, else compute\n",
        "short_cols = [c for c in td.columns if c.startswith(\"KAMA_SHORT_\")]\n",
        "long_cols  = [c for c in td.columns if c.startswith(\"KAMA_LONG_\")]\n",
        "\n",
        "if short_cols and long_cols:\n",
        "    short_kama = td[short_cols[0]].astype(\"float64\")\n",
        "    long_kama  = td[long_cols[0]].astype(\"float64\")\n",
        "    print(f\"Using existing KAMA series: {short_cols[0]} vs {long_cols[0]}\")\n",
        "else:\n",
        "    short_kama = ta.kama(td[\"Close\"].astype(\"float64\"),\n",
        "                         length=int(combo[\"KAMA_er_period\"]),\n",
        "                         fast=int(combo[\"KAMA_fast_period\"]),\n",
        "                         slow=int(combo[\"KAMA_slow_period\"]))\n",
        "    long_kama  = ta.kama(td[\"Close\"].astype(\"float64\"),\n",
        "                         length=int(combo[\"KAMA_er_period\"]) + 20,\n",
        "                         fast=int(combo[\"KAMA_fast_period\"]) + 10,\n",
        "                         slow=int(combo[\"KAMA_slow_period\"]) + 20)\n",
        "    print(\"Computed KAMA series from combo parameters.\")\n",
        "\n",
        "# Signals\n",
        "sig_kama = get_buy_signal(\"KAMA\", data=short_kama, params={\"short_kama\": short_kama, \"long_kama\": long_kama})\n",
        "st_line  = ind.supertrend(int(combo[\"Supertrend_period\"]), float(combo[\"Supertrend_multiplier\"]))\n",
        "sig_st   = get_buy_signal(\"Supertrend\", data=st_line, params={\"close\": td[\"Close\"].astype(\"float64\")})\n",
        "mfi_s    = ind.mfi(int(combo[\"MFI_period\"]))\n",
        "sig_mfi  = get_buy_signal(\"MFI\", data=mfi_s,\n",
        "                          params={\"oversold\": float(combo[\"MFI_oversold\"]), \"overbought\": float(combo[\"MFI_overbought\"])})\n",
        "\n",
        "print(\"\\nSignal value counts (last 200 rows):\")\n",
        "print(pd.DataFrame({\n",
        "    \"KAMA\": sig_kama.tail(200).value_counts(dropna=False),\n",
        "    \"ST\":   sig_st.tail(200).value_counts(dropna=False),\n",
        "    \"MFI\":  sig_mfi.tail(200).value_counts(dropna=False),\n",
        "}).fillna(0).astype(int))\n",
        "\n",
        "print(\"\\nFirst 10 non-null rows (KAMA/ST/MFI):\")\n",
        "print(pd.DataFrame({\"KAMA\": sig_kama, \"ST\": sig_st, \"MFI\": sig_mfi}).dropna().head(10).to_string())\n",
        "\n",
        "# OR logic with SELL priority and persistent target\n",
        "parts = pd.DataFrame({\"KAMA\": sig_kama, \"ST\": sig_st, \"MFI\": sig_mfi})\n",
        "sell_mask = (parts == -1).any(axis=1).fillna(False)\n",
        "buy_mask  = (parts ==  1).any(axis=1).fillna(False)\n",
        "\n",
        "events = pd.Series(np.nan, index=parts.index, dtype=\"float64\")\n",
        "events[sell_mask] = 0.0\n",
        "events[~sell_mask & buy_mask] = 1.0\n",
        "target = events.ffill().fillna(0.0)\n",
        "\n",
        "print(\"\\nEvents preview where BUY/SELL happened (first 10):\")\n",
        "print(pd.DataFrame({\"BUY\": buy_mask, \"SELL\": sell_mask}).loc[buy_mask | sell_mask].head(10).to_string())\n",
        "\n",
        "print(\"\\nTargets preview (last 12 rows):\")\n",
        "print(target.tail(12).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CONFIG] USE_PARALLEL=False, MAX_WORKERS=2, STREAM_EVERY=300, THROTTLE_EVERY=1000, MEM_TARGET=80%, MIN_FREE_GB=1.0\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# FULL EVALUATION OVER ALL TICKERS AND ALL COMBINATIONS (with detailed logging)\n",
        "# ================================================================================\n",
        "\n",
        "import os, gc, time, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "\n",
        "# Suppress noisy warnings\n",
        "np.set_printoptions(suppress=True)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='overflow encountered in*')\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in*')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# ---------------------- Throttling & Streaming knobs ----------------------\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "USE_PARALLEL = False        # keep False for stability; turn True only if needed\n",
        "MAX_WORKERS  = 2            # if you enable parallel, keep small (2â€“3)\n",
        "\n",
        "THROTTLE_EVERY   = 1_000    # check RAM every N combos\n",
        "THROTTLE_SLEEP_S = 1.0      # sleep when throttling\n",
        "MEM_TARGET       = 0.80     # throttle when >=80% RAM used\n",
        "MIN_FREE_GB      = 1.0      # or when <1 GB free\n",
        "\n",
        "STREAM_DIR    = \"grid_eval_metrics_batches\"  # per-ticker output directory\n",
        "STREAM_EVERY  = 300          # smaller batch for lower RAM\n",
        "FINAL_OUT_CSV = \"grid_eval_metrics_full.csv\" # final merged file\n",
        "\n",
        "# Extra safety guards (tunable)\n",
        "TICKER_TIME_BUDGET_S   = 600   # stop a ticker after 10 minutes (None to disable)\n",
        "MAX_COMBOS_PER_TICKER  = None  # e.g., 100000 to cap per ticker (None to disable)\n",
        "ADAPTIVE_BACKOFF_STEP  = 0.5   # multiply STREAM_EVERY by this when throttling (min 100)\n",
        "\n",
        "print(f\"[CONFIG] USE_PARALLEL={USE_PARALLEL}, MAX_WORKERS={MAX_WORKERS}, STREAM_EVERY={STREAM_EVERY}, \"\n",
        "      f\"THROTTLE_EVERY={THROTTLE_EVERY}, MEM_TARGET={int(MEM_TARGET*100)}%, MIN_FREE_GB={MIN_FREE_GB}\")\n",
        "\n",
        "\n",
        "def _fmt_ram() -> str:\n",
        "    if not psutil:\n",
        "        return \"RAM: n/a\"\n",
        "    vm = psutil.virtual_memory()\n",
        "    return f\"RAM used={vm.percent:.1f}% free={vm.available/1e9:.2f}GB\"\n",
        "\n",
        "\n",
        "def _should_throttle() -> bool:\n",
        "    if psutil is None:\n",
        "        return False\n",
        "    vm = psutil.virtual_memory()\n",
        "    return (vm.percent >= MEM_TARGET*100) or (vm.available/1e9 < MIN_FREE_GB)\n",
        "\n",
        "\n",
        "# ---------------------- Per-ticker evaluation ----------------------\n",
        "def _evaluate_ticker_metrics(t: str, data: pd.DataFrame, combos: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate all combos for ticker t. Streams to a per-ticker CSV and returns its path.\n",
        "    Applies RAM throttling, adaptive backoff, and optional time/combo budgets.\n",
        "    \"\"\"\n",
        "    td = data[data[\"Ticker\"] == t].sort_index().copy()\n",
        "    if len(td) < 60:\n",
        "        print(f\"[SKIP] {t}: insufficient data ({len(td)} rows)\")\n",
        "        return \"\"\n",
        "\n",
        "    os.makedirs(STREAM_DIR, exist_ok=True)\n",
        "    out_path = os.path.join(STREAM_DIR, f\"grid_eval_metrics_{t}.csv\")\n",
        "    if os.path.exists(out_path):\n",
        "        os.remove(out_path)\n",
        "\n",
        "    print(f\"[START] {t}: rows={len(td)}, cols={len(td.columns)} | {_fmt_ram()}\")\n",
        "\n",
        "    ind = Indicator(td)\n",
        "\n",
        "    # Prefer existing KAMA short/long columns if present\n",
        "    short_cols = [c for c in td.columns if c.startswith(\"KAMA_SHORT_\")]\n",
        "    long_cols  = [c for c in td.columns if c.startswith(\"KAMA_LONG_\")]\n",
        "\n",
        "    print(f\"[INFO] {t}: evaluating {len(combos):,} combos | KAMA_short/long cached? {bool(short_cols)} / {bool(long_cols)}\")\n",
        "\n",
        "    start_t = time.time()\n",
        "    combo_count = 0\n",
        "\n",
        "    # local adaptive stream size\n",
        "    local_stream_every = STREAM_EVERY\n",
        "\n",
        "    # progress cadence\n",
        "    progress_every = max(1000, len(combos)//100) if len(combos) else 1000\n",
        "\n",
        "    batch = []\n",
        "    written = 0\n",
        "\n",
        "    for i, row in combos.iterrows():\n",
        "        # per-ticker time budget\n",
        "        if TICKER_TIME_BUDGET_S is not None and (time.time() - start_t) > TICKER_TIME_BUDGET_S:\n",
        "            print(f\"[STOP] {t}: time budget {TICKER_TIME_BUDGET_S}s reached; stopping early at {combo_count:,} combos.\")\n",
        "            break\n",
        "\n",
        "        # optional per-ticker combo cap\n",
        "        if MAX_COMBOS_PER_TICKER is not None and combo_count >= MAX_COMBOS_PER_TICKER:\n",
        "            print(f\"[STOP] {t}: combo cap {MAX_COMBOS_PER_TICKER:,} reached; stopping early.\")\n",
        "            break\n",
        "\n",
        "        combo_count += 1\n",
        "\n",
        "        try:\n",
        "            # --- KAMA crossover ---\n",
        "            if short_cols and long_cols:\n",
        "                short_kama = td[short_cols[0]].astype('float64')\n",
        "                long_kama  = td[long_cols[0]].astype('float64')\n",
        "            else:\n",
        "                short_kama = ta.kama(td[\"Close\"].astype('float64'),\n",
        "                                     length=int(row[\"KAMA_er_period\"]),\n",
        "                                     fast=int(row[\"KAMA_fast_period\"]),\n",
        "                                     slow=int(row[\"KAMA_slow_period\"]))\n",
        "                long_kama  = ta.kama(td[\"Close\"].astype('float64'),\n",
        "                                     length=int(row[\"KAMA_er_period\"]) + 20,\n",
        "                                     fast=int(row[\"KAMA_fast_period\"]) + 10,\n",
        "                                     slow=int(row[\"KAMA_slow_period\"]) + 20)\n",
        "\n",
        "            sig_kama = get_buy_signal(\"KAMA\", data=short_kama,\n",
        "                                      params={\"short_kama\": short_kama, \"long_kama\": long_kama})\n",
        "\n",
        "            # --- Supertrend ---\n",
        "            st_line = ind.supertrend(int(row[\"Supertrend_period\"]), float(row[\"Supertrend_multiplier\"]))\n",
        "            sig_st = get_buy_signal(\"Supertrend\", data=st_line, params={\"close\": td[\"Close\"].astype('float64')})\n",
        "\n",
        "            # --- MFI ---\n",
        "            mfi_s = ind.mfi(int(row[\"MFI_period\"]))\n",
        "            sig_mfi = get_buy_signal(\"MFI\", data=mfi_s,\n",
        "                                     params={\"oversold\": float(row[\"MFI_oversold\"]),\n",
        "                                             \"overbought\": float(row[\"MFI_overbought\"])})\n",
        "\n",
        "            # Combine signals (OR with SELL priority) without big DataFrame\n",
        "            sell_mask = ((sig_kama == -1) | (sig_st == -1) | (sig_mfi == -1)).fillna(False)\n",
        "            buy_mask  = ((sig_kama ==  1) | (sig_st ==  1) | (sig_mfi ==  1)).fillna(False)\n",
        "\n",
        "            events = pd.Series(np.nan, index=td.index, dtype=\"float32\")\n",
        "            events[sell_mask] = 0.0\n",
        "            events[~sell_mask & buy_mask] = 1.0\n",
        "            target = events.ffill().fillna(0.0)\n",
        "            position = target.shift(1).fillna(0.0)  # next-day execution\n",
        "\n",
        "            # Backtest vector math\n",
        "            r_close = td[\"Close\"].astype('float64').pct_change().astype('float32')\n",
        "            strat_r = (position.astype('float32') * r_close).astype('float32')\n",
        "\n",
        "            r = strat_r.dropna()\n",
        "            equity = (1.0 + r).cumprod() if len(r) else pd.Series(dtype='float32')\n",
        "\n",
        "            # --- Metrics with native Python type conversion ---\n",
        "            s_val   = _sharpe(r);   sharpe   = float(s_val) if not pd.isna(s_val) else None\n",
        "            so_val  = _sortino(r);  sortino  = float(so_val) if not pd.isna(so_val) else None\n",
        "            totalret = float(equity.iloc[-1] - 1.0) if len(equity) else None\n",
        "            cg_val  = _cagr(r);     cagr     = float(cg_val) if len(r) and not pd.isna(cg_val) else None\n",
        "            md_val  = _max_dd(equity); max_dd   = float(md_val) if len(equity) and not pd.isna(md_val) else None\n",
        "            ad_val  = _avg_dd(equity); avg_dd   = float(ad_val) if len(equity) and not pd.isna(ad_val) else None\n",
        "            win_rate = float((r > 0).mean()) if len(r) else None\n",
        "            avg_win  = float(r[r > 0].mean()) if (r > 0).any() else None\n",
        "            avg_loss = float(r[r < 0].mean()) if (r < 0).any() else None\n",
        "            pf_val   = _profit_factor(r); pf   = float(pf_val) if not pd.isna(pf_val) else None\n",
        "            run_stats = _run_lengths(position, strat_r)\n",
        "            toom_val  = _time_out_of_mkt(position); time_out  = float(toom_val) if not pd.isna(toom_val) else None\n",
        "            pbar_val  = _pbar_time_rate(strat_r, position); pbar_rate = float(pbar_val) if not pd.isna(pbar_val) else None\n",
        "            rs_tail   = [float(x) for x in _rolling_sharpe(strat_r, window=63).dropna().tail(3).round(3)] if len(strat_r) > 63 else [None, None, None]\n",
        "\n",
        "            # Build row\n",
        "            batch.append({\n",
        "                \"Ticker\": t,\n",
        "                \"combination_id\": int(row.get(\"combination_id\", -1)),\n",
        "                \"KAMA_er_period\": int(row[\"KAMA_er_period\"]),\n",
        "                \"KAMA_fast_period\": int(row[\"KAMA_fast_period\"]),\n",
        "                \"KAMA_slow_period\": int(row[\"KAMA_slow_period\"]),\n",
        "                \"ST_period\": int(row[\"Supertrend_period\"]),\n",
        "                \"ST_multiplier\": float(row[\"Supertrend_multiplier\"]),\n",
        "                \"MFI_period\": int(row[\"MFI_period\"]),\n",
        "                \"MFI_oversold\": float(row[\"MFI_oversold\"]),\n",
        "                \"MFI_overbought\": float(row[\"MFI_overbought\"]),\n",
        "                \"Sharpe\": sharpe, \"Sortino\": sortino, \"Total_Return\": totalret, \"CAGR\": cagr,\n",
        "                \"Max_Drawdown\": max_dd, \"Avg_Drawdown\": avg_dd, \"Win_Rate\": win_rate,\n",
        "                \"Avg_Win\": avg_win, \"Avg_Loss\": avg_loss, \"Profit_Factor\": pf,\n",
        "                \"Trades\": int(run_stats[\"Trades\"]),\n",
        "                \"Avg_Time_In_Trade\": float(run_stats[\"Avg_Time_In_Trade\"]),\n",
        "                \"Avg_Time_In_Win_Trade\": float(run_stats[\"Avg_Time_In_Win_Trade\"]),\n",
        "                \"Avg_Time_In_Loss_Trade\": float(run_stats[\"Avg_Time_In_Loss_Trade\"]),\n",
        "                \"Time_Out_Of_Market\": time_out,\n",
        "                \"Profitable_Bar_Time_Rate\": pbar_rate,\n",
        "                \"RS63_last3\": str(rs_tail),\n",
        "            })\n",
        "\n",
        "            # Stream to CSV in batches\n",
        "            if len(batch) >= local_stream_every:\n",
        "                pd.DataFrame(batch).to_csv(out_path, mode='a',\n",
        "                                           header=not os.path.exists(out_path), index=False)\n",
        "                written += len(batch)\n",
        "                print(f\"[FLUSH] {t}: +{len(batch):,} rows (total={written:,}) â†’ {out_path}\")\n",
        "                batch.clear()\n",
        "                gc.collect()\n",
        "\n",
        "            # Throttle RAM if needed + adaptive backoff\n",
        "            if (i+1) % THROTTLE_EVERY == 0 and _should_throttle():\n",
        "                print(f\"[THROTTLE] {t}: gc + sleep {THROTTLE_SLEEP_S}s | stream_every={local_stream_every} | {_fmt_ram()}\")\n",
        "                gc.collect()\n",
        "                time.sleep(THROTTLE_SLEEP_S)\n",
        "                # adaptive backoff: reduce batch size (min 100)\n",
        "                new_stream_every = max(100, int(local_stream_every * ADAPTIVE_BACKOFF_STEP))\n",
        "                if new_stream_every < local_stream_every:\n",
        "                    print(f\"[ADAPT] {t}: stream batch {local_stream_every} â†’ {new_stream_every}\")\n",
        "                    local_stream_every = new_stream_every\n",
        "\n",
        "            # periodic progress\n",
        "            if combo_count % progress_every == 0 or combo_count == 1:\n",
        "                elapsed = time.time() - start_t\n",
        "                pct = 100.0 * combo_count / max(1, len(combos))\n",
        "                print(f\"[PROG] {t}: {combo_count:,}/{len(combos):,} ({pct:5.1f}%) | written={written:,} | \"\n",
        "                      f\"stream_every={local_stream_every} | elapsed={elapsed:,.1f}s | {_fmt_ram()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERR ] {t} / combo {int(row.get('combination_id', -1))}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Flush remaining rows\n",
        "    if batch:\n",
        "        pd.DataFrame(batch).to_csv(out_path, mode='a', header=not os.path.exists(out_path), index=False)\n",
        "        written += len(batch)\n",
        "        print(f\"[FLUSH] {t}: +{len(batch):,} rows (total={written:,}) â†’ {out_path}\")\n",
        "        batch.clear()\n",
        "        gc.collect()\n",
        "\n",
        "    elapsed = time.time() - start_t\n",
        "    print(f\"[DONE] {t}: wrote {written:,} rows in {elapsed:,.1f}s â†’ {out_path} | {_fmt_ram()}\")\n",
        "    del ind, td\n",
        "    gc.collect()\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# ---------------------- Driver: serial or parallel per-ticker ----------------------\n",
        "# Ensure combos are available from CSV (robust to undefined in session)\n",
        "PARAM_CSV = \"/Users/chielg/Documents/GitHub/TA_optimization/parameter_combinations.csv\"\n",
        "combos  = pd.read_csv(PARAM_CSV)\n",
        "\n",
        "tickers = data[\"Ticker\"].unique()\n",
        "print(f\"[DRIVER] Evaluating {len(tickers)} tickers Ã— {len(combos):,} combinations = {len(tickers)*len(combos):,} evals (serial={not USE_PARALLEL})\")\n",
        "\n",
        "per_ticker_files = []\n",
        "\n",
        "if USE_PARALLEL:\n",
        "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "    print(f\"[DRIVER] Starting parallel pool with max_workers={MAX_WORKERS}\")\n",
        "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futs = {ex.submit(_evaluate_ticker_metrics, t, data, combos): t for t in tickers}\n",
        "        for f in as_completed(futs):\n",
        "            t = futs[f]\n",
        "            try:\n",
        "                p = f.result()\n",
        "                if p:\n",
        "                    per_ticker_files.append(p)\n",
        "                    print(f\"[DRIVER] Finished {t} â†’ {p}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[DRIVER] {t} raised: {e}\")\n",
        "else:\n",
        "    for t in tickers:\n",
        "        _t0 = time.time()\n",
        "        p = _evaluate_ticker_metrics(t, data, combos)\n",
        "        dt = time.time() - _t0\n",
        "        if p:\n",
        "            per_ticker_files.append(p)\n",
        "            print(f\"[DRIVER] Finished {t} in {dt:,.1f}s â†’ {p}\")\n",
        "\n",
        "# ---------------------- Merge all per-ticker CSVs into final file ----------------------\n",
        "if os.path.exists(FINAL_OUT_CSV):\n",
        "    os.remove(FINAL_OUT_CSV)\n",
        "\n",
        "print(f\"[MERGE] Collating {len(per_ticker_files)} files â†’ {FINAL_OUT_CSV}\")\n",
        "header_written = False\n",
        "total_merged = 0\n",
        "for fp in per_ticker_files:\n",
        "    try:\n",
        "        print(f\"[MERGE] {fp} ...\")\n",
        "        for chunk in pd.read_csv(fp, chunksize=200_000):\n",
        "            chunk.to_csv(FINAL_OUT_CSV, mode='a', header=not header_written, index=False)\n",
        "            header_written = True\n",
        "            total_merged += len(chunk)\n",
        "            print(f\"[MERGE] {fp}: +{len(chunk):,} rows (total={total_merged:,})\")\n",
        "    except Exception as e:\n",
        "        print(f\"[MERGE] warning for {fp}: {e}\")\n",
        "\n",
        "print(f\"[DONE ] Saved all metrics ({total_merged:,} rows) â†’ {FINAL_OUT_CSV}\")\n",
        "\n",
        "# Small preview without loading everything\n",
        "try:\n",
        "    preview_df = pd.read_csv(FINAL_OUT_CSV, nrows=20)\n",
        "    display(preview_df)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CONFIG] USE_PARALLEL=False, MAX_WORKERS=2, STREAM_EVERY=300, THROTTLE_EVERY=1000, MEM_TARGET=80%, MIN_FREE_GB=1.0\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# FULL EVALUATION OVER ALL TICKERS AND ALL COMBINATIONS (with detailed logging)\n",
        "# ================================================================================\n",
        "\n",
        "import os, gc, time, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "\n",
        "# Suppress noisy warnings\n",
        "np.set_printoptions(suppress=True)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='overflow encountered in*')\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in*')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# ---------------------- Throttling & Streaming knobs ----------------------\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "USE_PARALLEL = False        # keep False for stability; turn True only if needed\n",
        "MAX_WORKERS  = 2            # if you enable parallel, keep small (2â€“3)\n",
        "\n",
        "THROTTLE_EVERY   = 1_000    # check RAM every N combos\n",
        "THROTTLE_SLEEP_S = 1.0      # sleep when throttling\n",
        "MEM_TARGET       = 0.80     # throttle when >=80% RAM used\n",
        "MIN_FREE_GB      = 1.0      # or when <1 GB free\n",
        "\n",
        "STREAM_DIR    = \"grid_eval_metrics_batches\"  # per-ticker output directory\n",
        "STREAM_EVERY  = 300          # smaller batch for lower RAM\n",
        "FINAL_OUT_CSV = \"grid_eval_metrics_full.csv\" # final merged file\n",
        "\n",
        "# Extra safety guards (tunable)\n",
        "TICKER_TIME_BUDGET_S   = 600   # stop a ticker after 10 minutes (None to disable)\n",
        "MAX_COMBOS_PER_TICKER  = None  # e.g., 100000 to cap per ticker (None to disable)\n",
        "ADAPTIVE_BACKOFF_STEP  = 0.5   # multiply STREAM_EVERY by this when throttling (min 100)\n",
        "\n",
        "print(f\"[CONFIG] USE_PARALLEL={USE_PARALLEL}, MAX_WORKERS={MAX_WORKERS}, STREAM_EVERY={STREAM_EVERY}, \"\n",
        "      f\"THROTTLE_EVERY={THROTTLE_EVERY}, MEM_TARGET={int(MEM_TARGET*100)}%, MIN_FREE_GB={MIN_FREE_GB}\")\n",
        "\n",
        "\n",
        "def _fmt_ram() -> str:\n",
        "    if not psutil:\n",
        "        return \"RAM: n/a\"\n",
        "    vm = psutil.virtual_memory()\n",
        "    return f\"RAM used={vm.percent:.1f}% free={vm.available/1e9:.2f}GB\"\n",
        "\n",
        "\n",
        "def _should_throttle() -> bool:\n",
        "    if psutil is None:\n",
        "        return False\n",
        "    vm = psutil.virtual_memory()\n",
        "    return (vm.percent >= MEM_TARGET*100) or (vm.available/1e9 < MIN_FREE_GB)\n",
        "\n",
        "\n",
        "# ---------------------- Per-ticker evaluation ----------------------\n",
        "def _evaluate_ticker_metrics(t: str, data: pd.DataFrame, combos: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate all combos for ticker t. Streams to a per-ticker CSV and returns its path.\n",
        "    Applies RAM throttling, adaptive backoff, and optional time/combo budgets.\n",
        "    \"\"\"\n",
        "    td = data[data[\"Ticker\"] == t].sort_index().copy()\n",
        "    if len(td) < 60:\n",
        "        print(f\"[SKIP] {t}: insufficient data ({len(td)} rows)\")\n",
        "        return \"\"\n",
        "\n",
        "    os.makedirs(STREAM_DIR, exist_ok=True)\n",
        "    out_path = os.path.join(STREAM_DIR, f\"grid_eval_metrics_{t}.csv\")\n",
        "    if os.path.exists(out_path):\n",
        "        os.remove(out_path)\n",
        "\n",
        "    print(f\"[START] {t}: rows={len(td)}, cols={len(td.columns)} | {_fmt_ram()}\")\n",
        "\n",
        "    ind = Indicator(td)\n",
        "\n",
        "    # Prefer existing KAMA short/long columns if present\n",
        "    short_cols = [c for c in td.columns if c.startswith(\"KAMA_SHORT_\")]\n",
        "    long_cols  = [c for c in td.columns if c.startswith(\"KAMA_LONG_\")]\n",
        "\n",
        "    print(f\"[INFO] {t}: evaluating {len(combos):,} combos | KAMA_short/long cached? {bool(short_cols)} / {bool(long_cols)}\")\n",
        "\n",
        "    start_t = time.time()\n",
        "    combo_count = 0\n",
        "\n",
        "    # local adaptive stream size\n",
        "    local_stream_every = STREAM_EVERY\n",
        "\n",
        "    # progress cadence\n",
        "    progress_every = max(1000, len(combos)//100) if len(combos) else 1000\n",
        "\n",
        "    batch = []\n",
        "    written = 0\n",
        "\n",
        "    for i, row in combos.iterrows():\n",
        "        # per-ticker time budget\n",
        "        if TICKER_TIME_BUDGET_S is not None and (time.time() - start_t) > TICKER_TIME_BUDGET_S:\n",
        "            print(f\"[STOP] {t}: time budget {TICKER_TIME_BUDGET_S}s reached; stopping early at {combo_count:,} combos.\")\n",
        "            break\n",
        "\n",
        "        # optional per-ticker combo cap\n",
        "        if MAX_COMBOS_PER_TICKER is not None and combo_count >= MAX_COMBOS_PER_TICKER:\n",
        "            print(f\"[STOP] {t}: combo cap {MAX_COMBOS_PER_TICKER:,} reached; stopping early.\")\n",
        "            break\n",
        "\n",
        "        combo_count += 1\n",
        "\n",
        "        try:\n",
        "            # --- KAMA crossover ---\n",
        "            if short_cols and long_cols:\n",
        "                short_kama = td[short_cols[0]].astype('float64')\n",
        "                long_kama  = td[long_cols[0]].astype('float64')\n",
        "            else:\n",
        "                short_kama = ta.kama(td[\"Close\"].astype('float64'),\n",
        "                                     length=int(row[\"KAMA_er_period\"]),\n",
        "                                     fast=int(row[\"KAMA_fast_period\"]),\n",
        "                                     slow=int(row[\"KAMA_slow_period\"]))\n",
        "                long_kama  = ta.kama(td[\"Close\"].astype('float64'),\n",
        "                                     length=int(row[\"KAMA_er_period\"]) + 20,\n",
        "                                     fast=int(row[\"KAMA_fast_period\"]) + 10,\n",
        "                                     slow=int(row[\"KAMA_slow_period\"]) + 20)\n",
        "\n",
        "            sig_kama = get_buy_signal(\"KAMA\", data=short_kama,\n",
        "                                      params={\"short_kama\": short_kama, \"long_kama\": long_kama})\n",
        "\n",
        "            # --- Supertrend ---\n",
        "            st_line = ind.supertrend(int(row[\"Supertrend_period\"]), float(row[\"Supertrend_multiplier\"]))\n",
        "            sig_st = get_buy_signal(\"Supertrend\", data=st_line, params={\"close\": td[\"Close\"].astype('float64')})\n",
        "\n",
        "            # --- MFI ---\n",
        "            mfi_s = ind.mfi(int(row[\"MFI_period\"]))\n",
        "            sig_mfi = get_buy_signal(\"MFI\", data=mfi_s,\n",
        "                                     params={\"oversold\": float(row[\"MFI_oversold\"]),\n",
        "                                             \"overbought\": float(row[\"MFI_overbought\"])})\n",
        "\n",
        "            # Combine signals (OR with SELL priority) without big DataFrame\n",
        "            sell_mask = ((sig_kama == -1) | (sig_st == -1) | (sig_mfi == -1)).fillna(False)\n",
        "            buy_mask  = ((sig_kama ==  1) | (sig_st ==  1) | (sig_mfi ==  1)).fillna(False)\n",
        "\n",
        "            events = pd.Series(np.nan, index=td.index, dtype=\"float32\")\n",
        "            events[sell_mask] = 0.0\n",
        "            events[~sell_mask & buy_mask] = 1.0\n",
        "            target = events.ffill().fillna(0.0)\n",
        "            position = target.shift(1).fillna(0.0)  # next-day execution\n",
        "\n",
        "            # Backtest vector math\n",
        "            r_close = td[\"Close\"].astype('float64').pct_change().astype('float32')\n",
        "            strat_r = (position.astype('float32') * r_close).astype('float32')\n",
        "\n",
        "            r = strat_r.dropna()\n",
        "            equity = (1.0 + r).cumprod() if len(r) else pd.Series(dtype='float32')\n",
        "\n",
        "            # --- Metrics with native Python type conversion ---\n",
        "            s_val   = _sharpe(r);   sharpe   = float(s_val) if not pd.isna(s_val) else None\n",
        "            so_val  = _sortino(r);  sortino  = float(so_val) if not pd.isna(so_val) else None\n",
        "            totalret = float(equity.iloc[-1] - 1.0) if len(equity) else None\n",
        "            cg_val  = _cagr(r);     cagr     = float(cg_val) if len(r) and not pd.isna(cg_val) else None\n",
        "            md_val  = _max_dd(equity); max_dd   = float(md_val) if len(equity) and not pd.isna(md_val) else None\n",
        "            ad_val  = _avg_dd(equity); avg_dd   = float(ad_val) if len(equity) and not pd.isna(ad_val) else None\n",
        "            win_rate = float((r > 0).mean()) if len(r) else None\n",
        "            avg_win  = float(r[r > 0].mean()) if (r > 0).any() else None\n",
        "            avg_loss = float(r[r < 0].mean()) if (r < 0).any() else None\n",
        "            pf_val   = _profit_factor(r); pf   = float(pf_val) if not pd.isna(pf_val) else None\n",
        "            run_stats = _run_lengths(position, strat_r)\n",
        "            toom_val  = _time_out_of_mkt(position); time_out  = float(toom_val) if not pd.isna(toom_val) else None\n",
        "            pbar_val  = _pbar_time_rate(strat_r, position); pbar_rate = float(pbar_val) if not pd.isna(pbar_val) else None\n",
        "            rs_tail   = [float(x) for x in _rolling_sharpe(strat_r, window=63).dropna().tail(3).round(3)] if len(strat_r) > 63 else [None, None, None]\n",
        "\n",
        "            # Build row\n",
        "            batch.append({\n",
        "                \"Ticker\": t,\n",
        "                \"combination_id\": int(row.get(\"combination_id\", -1)),\n",
        "                \"KAMA_er_period\": int(row[\"KAMA_er_period\"]),\n",
        "                \"KAMA_fast_period\": int(row[\"KAMA_fast_period\"]),\n",
        "                \"KAMA_slow_period\": int(row[\"KAMA_slow_period\"]),\n",
        "                \"ST_period\": int(row[\"Supertrend_period\"]),\n",
        "                \"ST_multiplier\": float(row[\"Supertrend_multiplier\"]),\n",
        "                \"MFI_period\": int(row[\"MFI_period\"]),\n",
        "                \"MFI_oversold\": float(row[\"MFI_oversold\"]),\n",
        "                \"MFI_overbought\": float(row[\"MFI_overbought\"]),\n",
        "                \"Sharpe\": sharpe, \"Sortino\": sortino, \"Total_Return\": totalret, \"CAGR\": cagr,\n",
        "                \"Max_Drawdown\": max_dd, \"Avg_Drawdown\": avg_dd, \"Win_Rate\": win_rate,\n",
        "                \"Avg_Win\": avg_win, \"Avg_Loss\": avg_loss, \"Profit_Factor\": pf,\n",
        "                \"Trades\": int(run_stats[\"Trades\"]),\n",
        "                \"Avg_Time_In_Trade\": float(run_stats[\"Avg_Time_In_Trade\"]),\n",
        "                \"Avg_Time_In_Win_Trade\": float(run_stats[\"Avg_Time_In_Win_Trade\"]),\n",
        "                \"Avg_Time_In_Loss_Trade\": float(run_stats[\"Avg_Time_In_Loss_Trade\"]),\n",
        "                \"Time_Out_Of_Market\": time_out,\n",
        "                \"Profitable_Bar_Time_Rate\": pbar_rate,\n",
        "                \"RS63_last3\": str(rs_tail),\n",
        "            })\n",
        "\n",
        "            # Stream to CSV in batches\n",
        "            if len(batch) >= local_stream_every:\n",
        "                pd.DataFrame(batch).to_csv(out_path, mode='a',\n",
        "                                           header=not os.path.exists(out_path), index=False)\n",
        "                written += len(batch)\n",
        "                print(f\"[FLUSH] {t}: +{len(batch):,} rows (total={written:,}) â†’ {out_path}\")\n",
        "                batch.clear()\n",
        "                gc.collect()\n",
        "\n",
        "            # Throttle RAM if needed + adaptive backoff\n",
        "            if (i+1) % THROTTLE_EVERY == 0 and _should_throttle():\n",
        "                print(f\"[THROTTLE] {t}: gc + sleep {THROTTLE_SLEEP_S}s | stream_every={local_stream_every} | {_fmt_ram()}\")\n",
        "                gc.collect()\n",
        "                time.sleep(THROTTLE_SLEEP_S)\n",
        "                # adaptive backoff: reduce batch size (min 100)\n",
        "                new_stream_every = max(100, int(local_stream_every * ADAPTIVE_BACKOFF_STEP))\n",
        "                if new_stream_every < local_stream_every:\n",
        "                    print(f\"[ADAPT] {t}: stream batch {local_stream_every} â†’ {new_stream_every}\")\n",
        "                    local_stream_every = new_stream_every\n",
        "\n",
        "            # periodic progress\n",
        "            if combo_count % progress_every == 0 or combo_count == 1:\n",
        "                elapsed = time.time() - start_t\n",
        "                pct = 100.0 * combo_count / max(1, len(combos))\n",
        "                print(f\"[PROG] {t}: {combo_count:,}/{len(combos):,} ({pct:5.1f}%) | written={written:,} | \"\n",
        "                      f\"stream_every={local_stream_every} | elapsed={elapsed:,.1f}s | {_fmt_ram()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERR ] {t} / combo {int(row.get('combination_id', -1))}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Flush remaining rows\n",
        "    if batch:\n",
        "        pd.DataFrame(batch).to_csv(out_path, mode='a', header=not os.path.exists(out_path), index=False)\n",
        "        written += len(batch)\n",
        "        print(f\"[FLUSH] {t}: +{len(batch):,} rows (total={written:,}) â†’ {out_path}\")\n",
        "        batch.clear()\n",
        "        gc.collect()\n",
        "\n",
        "    elapsed = time.time() - start_t\n",
        "    print(f\"[DONE] {t}: wrote {written:,} rows in {elapsed:,.1f}s â†’ {out_path} | {_fmt_ram()}\")\n",
        "    del ind, td\n",
        "    gc.collect()\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# ---------------------- Driver: serial or parallel per-ticker ----------------------\n",
        "# Ensure combos are available from CSV (robust to undefined in session)\n",
        "PARAM_CSV = \"/Users/chielg/Documents/GitHub/TA_optimization/parameter_combinations.csv\"\n",
        "combos  = pd.read_csv(PARAM_CSV)\n",
        "\n",
        "tickers = data[\"Ticker\"].unique()\n",
        "print(f\"[DRIVER] Evaluating {len(tickers)} tickers Ã— {len(combos):,} combinations = {len(tickers)*len(combos):,} evals (serial={not USE_PARALLEL})\")\n",
        "\n",
        "per_ticker_files = []\n",
        "\n",
        "if USE_PARALLEL:\n",
        "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "    print(f\"[DRIVER] Starting parallel pool with max_workers={MAX_WORKERS}\")\n",
        "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futs = {ex.submit(_evaluate_ticker_metrics, t, data, combos): t for t in tickers}\n",
        "        for f in as_completed(futs):\n",
        "            t = futs[f]\n",
        "            try:\n",
        "                p = f.result()\n",
        "                if p:\n",
        "                    per_ticker_files.append(p)\n",
        "                    print(f\"[DRIVER] Finished {t} â†’ {p}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[DRIVER] {t} raised: {e}\")\n",
        "else:\n",
        "    for t in tickers:\n",
        "        _t0 = time.time()\n",
        "        p = _evaluate_ticker_metrics(t, data, combos)\n",
        "        dt = time.time() - _t0\n",
        "        if p:\n",
        "            per_ticker_files.append(p)\n",
        "            print(f\"[DRIVER] Finished {t} in {dt:,.1f}s â†’ {p}\")\n",
        "\n",
        "# ---------------------- Merge all per-ticker CSVs into final file ----------------------\n",
        "if os.path.exists(FINAL_OUT_CSV):\n",
        "    os.remove(FINAL_OUT_CSV)\n",
        "\n",
        "print(f\"[MERGE] Collating {len(per_ticker_files)} files â†’ {FINAL_OUT_CSV}\")\n",
        "header_written = False\n",
        "total_merged = 0\n",
        "for fp in per_ticker_files:\n",
        "    try:\n",
        "        print(f\"[MERGE] {fp} ...\")\n",
        "        for chunk in pd.read_csv(fp, chunksize=200_000):\n",
        "            chunk.to_csv(FINAL_OUT_CSV, mode='a', header=not header_written, index=False)\n",
        "            header_written = True\n",
        "            total_merged += len(chunk)\n",
        "            print(f\"[MERGE] {fp}: +{len(chunk):,} rows (total={total_merged:,})\")\n",
        "    except Exception as e:\n",
        "        print(f\"[MERGE] warning for {fp}: {e}\")\n",
        "\n",
        "print(f\"[DONE ] Saved all metrics ({total_merged:,} rows) â†’ {FINAL_OUT_CSV}\")\n",
        "\n",
        "# Small preview without loading everything\n",
        "try:\n",
        "    preview_df = pd.read_csv(FINAL_OUT_CSV, nrows=20)\n",
        "    display(preview_df)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CONFIG] USE_PARALLEL=False, MAX_WORKERS=2, STREAM_EVERY=300, THROTTLE_EVERY=1000, MEM_TARGET=80%, MIN_FREE_GB=1.0\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# FULL EVALUATION OVER ALL TICKERS AND ALL COMBINATIONS (with detailed logging)\n",
        "# ================================================================================\n",
        "\n",
        "import os, gc, time, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "\n",
        "# Suppress noisy warnings\n",
        "np.set_printoptions(suppress=True)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='overflow encountered in*')\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in*')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# ---------------------- Throttling & Streaming knobs ----------------------\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "USE_PARALLEL = False        # keep False for stability; turn True only if needed\n",
        "MAX_WORKERS  = 2            # if you enable parallel, keep small (2â€“3)\n",
        "\n",
        "THROTTLE_EVERY   = 1_000    # check RAM every N combos\n",
        "THROTTLE_SLEEP_S = 1.0      # sleep when throttling\n",
        "MEM_TARGET       = 0.80     # throttle when >=80% RAM used\n",
        "MIN_FREE_GB      = 1.0      # or when <1 GB free\n",
        "\n",
        "STREAM_DIR    = \"grid_eval_metrics_batches\"  # per-ticker output directory\n",
        "STREAM_EVERY  = 300          # smaller batch for lower RAM\n",
        "FINAL_OUT_CSV = \"grid_eval_metrics_full.csv\" # final merged file\n",
        "\n",
        "# Extra safety guards (tunable)\n",
        "TICKER_TIME_BUDGET_S   = 600   # stop a ticker after 10 minutes (None to disable)\n",
        "MAX_COMBOS_PER_TICKER  = None  # e.g., 100000 to cap per ticker (None to disable)\n",
        "ADAPTIVE_BACKOFF_STEP  = 0.5   # multiply STREAM_EVERY by this when throttling (min 100)\n",
        "\n",
        "print(f\"[CONFIG] USE_PARALLEL={USE_PARALLEL}, MAX_WORKERS={MAX_WORKERS}, STREAM_EVERY={STREAM_EVERY}, \"\n",
        "      f\"THROTTLE_EVERY={THROTTLE_EVERY}, MEM_TARGET={int(MEM_TARGET*100)}%, MIN_FREE_GB={MIN_FREE_GB}\")\n",
        "\n",
        "\n",
        "def _fmt_ram() -> str:\n",
        "    if not psutil:\n",
        "        return \"RAM: n/a\"\n",
        "    vm = psutil.virtual_memory()\n",
        "    return f\"RAM used={vm.percent:.1f}% free={vm.available/1e9:.2f}GB\"\n",
        "\n",
        "\n",
        "def _should_throttle() -> bool:\n",
        "    if psutil is None:\n",
        "        return False\n",
        "    vm = psutil.virtual_memory()\n",
        "    return (vm.percent >= MEM_TARGET*100) or (vm.available/1e9 < MIN_FREE_GB)\n",
        "\n",
        "\n",
        "# ---------------------- Per-ticker evaluation ----------------------\n",
        "def _evaluate_ticker_metrics(t: str, data: pd.DataFrame, combos: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate all combos for ticker t. Streams to a per-ticker CSV and returns its path.\n",
        "    Applies RAM throttling, adaptive backoff, and optional time/combo budgets.\n",
        "    \"\"\"\n",
        "    td = data[data[\"Ticker\"] == t].sort_index().copy()\n",
        "    if len(td) < 60:\n",
        "        print(f\"[SKIP] {t}: insufficient data ({len(td)} rows)\")\n",
        "        return \"\"\n",
        "\n",
        "    os.makedirs(STREAM_DIR, exist_ok=True)\n",
        "    out_path = os.path.join(STREAM_DIR, f\"grid_eval_metrics_{t}.csv\")\n",
        "    if os.path.exists(out_path):\n",
        "        os.remove(out_path)\n",
        "\n",
        "    print(f\"[START] {t}: rows={len(td)}, cols={len(td.columns)} | {_fmt_ram()}\")\n",
        "\n",
        "    ind = Indicator(td)\n",
        "\n",
        "    # Prefer existing KAMA short/long columns if present\n",
        "    short_cols = [c for c in td.columns if c.startswith(\"KAMA_SHORT_\")]\n",
        "    long_cols  = [c for c in td.columns if c.startswith(\"KAMA_LONG_\")]\n",
        "\n",
        "    print(f\"[INFO] {t}: evaluating {len(combos):,} combos | KAMA_short/long cached? {bool(short_cols)} / {bool(long_cols)}\")\n",
        "\n",
        "    start_t = time.time()\n",
        "    combo_count = 0\n",
        "\n",
        "    # local adaptive stream size\n",
        "    local_stream_every = STREAM_EVERY\n",
        "\n",
        "    # progress cadence\n",
        "    progress_every = max(1000, len(combos)//100) if len(combos) else 1000\n",
        "\n",
        "    batch = []\n",
        "    written = 0\n",
        "\n",
        "    for i, row in combos.iterrows():\n",
        "        # per-ticker time budget\n",
        "        if TICKER_TIME_BUDGET_S is not None and (time.time() - start_t) > TICKER_TIME_BUDGET_S:\n",
        "            print(f\"[STOP] {t}: time budget {TICKER_TIME_BUDGET_S}s reached; stopping early at {combo_count:,} combos.\")\n",
        "            break\n",
        "\n",
        "        # optional per-ticker combo cap\n",
        "        if MAX_COMBOS_PER_TICKER is not None and combo_count >= MAX_COMBOS_PER_TICKER:\n",
        "            print(f\"[STOP] {t}: combo cap {MAX_COMBOS_PER_TICKER:,} reached; stopping early.\")\n",
        "            break\n",
        "\n",
        "        combo_count += 1\n",
        "\n",
        "        try:\n",
        "            # --- KAMA crossover ---\n",
        "            if short_cols and long_cols:\n",
        "                short_kama = td[short_cols[0]].astype('float64')\n",
        "                long_kama  = td[long_cols[0]].astype('float64')\n",
        "            else:\n",
        "                short_kama = ta.kama(td[\"Close\"].astype('float64'),\n",
        "                                     length=int(row[\"KAMA_er_period\"]),\n",
        "                                     fast=int(row[\"KAMA_fast_period\"]),\n",
        "                                     slow=int(row[\"KAMA_slow_period\"]))\n",
        "                long_kama  = ta.kama(td[\"Close\"].astype('float64'),\n",
        "                                     length=int(row[\"KAMA_er_period\"]) + 20,\n",
        "                                     fast=int(row[\"KAMA_fast_period\"]) + 10,\n",
        "                                     slow=int(row[\"KAMA_slow_period\"]) + 20)\n",
        "\n",
        "            sig_kama = get_buy_signal(\"KAMA\", data=short_kama,\n",
        "                                      params={\"short_kama\": short_kama, \"long_kama\": long_kama})\n",
        "\n",
        "            # --- Supertrend ---\n",
        "            st_line = ind.supertrend(int(row[\"Supertrend_period\"]), float(row[\"Supertrend_multiplier\"]))\n",
        "            sig_st = get_buy_signal(\"Supertrend\", data=st_line, params={\"close\": td[\"Close\"].astype('float64')})\n",
        "\n",
        "            # --- MFI ---\n",
        "            mfi_s = ind.mfi(int(row[\"MFI_period\"]))\n",
        "            sig_mfi = get_buy_signal(\"MFI\", data=mfi_s,\n",
        "                                     params={\"oversold\": float(row[\"MFI_oversold\"]),\n",
        "                                             \"overbought\": float(row[\"MFI_overbought\"])})\n",
        "\n",
        "            # Combine signals (OR with SELL priority) without big DataFrame\n",
        "            sell_mask = ((sig_kama == -1) | (sig_st == -1) | (sig_mfi == -1)).fillna(False)\n",
        "            buy_mask  = ((sig_kama ==  1) | (sig_st ==  1) | (sig_mfi ==  1)).fillna(False)\n",
        "\n",
        "            events = pd.Series(np.nan, index=td.index, dtype=\"float32\")\n",
        "            events[sell_mask] = 0.0\n",
        "            events[~sell_mask & buy_mask] = 1.0\n",
        "            target = events.ffill().fillna(0.0)\n",
        "            position = target.shift(1).fillna(0.0)  # next-day execution\n",
        "\n",
        "            # Backtest vector math\n",
        "            r_close = td[\"Close\"].astype('float64').pct_change().astype('float32')\n",
        "            strat_r = (position.astype('float32') * r_close).astype('float32')\n",
        "\n",
        "            r = strat_r.dropna()\n",
        "            equity = (1.0 + r).cumprod() if len(r) else pd.Series(dtype='float32')\n",
        "\n",
        "            # --- Metrics with native Python type conversion ---\n",
        "            s_val   = _sharpe(r);   sharpe   = float(s_val) if not pd.isna(s_val) else None\n",
        "            so_val  = _sortino(r);  sortino  = float(so_val) if not pd.isna(so_val) else None\n",
        "            totalret = float(equity.iloc[-1] - 1.0) if len(equity) else None\n",
        "            cg_val  = _cagr(r);     cagr     = float(cg_val) if len(r) and not pd.isna(cg_val) else None\n",
        "            md_val  = _max_dd(equity); max_dd   = float(md_val) if len(equity) and not pd.isna(md_val) else None\n",
        "            ad_val  = _avg_dd(equity); avg_dd   = float(ad_val) if len(equity) and not pd.isna(ad_val) else None\n",
        "            win_rate = float((r > 0).mean()) if len(r) else None\n",
        "            avg_win  = float(r[r > 0].mean()) if (r > 0).any() else None\n",
        "            avg_loss = float(r[r < 0].mean()) if (r < 0).any() else None\n",
        "            pf_val   = _profit_factor(r); pf   = float(pf_val) if not pd.isna(pf_val) else None\n",
        "            run_stats = _run_lengths(position, strat_r)\n",
        "            toom_val  = _time_out_of_mkt(position); time_out  = float(toom_val) if not pd.isna(toom_val) else None\n",
        "            pbar_val  = _pbar_time_rate(strat_r, position); pbar_rate = float(pbar_val) if not pd.isna(pbar_val) else None\n",
        "            rs_tail   = [float(x) for x in _rolling_sharpe(strat_r, window=63).dropna().tail(3).round(3)] if len(strat_r) > 63 else [None, None, None]\n",
        "\n",
        "            # Build row\n",
        "            batch.append({\n",
        "                \"Ticker\": t,\n",
        "                \"combination_id\": int(row.get(\"combination_id\", -1)),\n",
        "                \"KAMA_er_period\": int(row[\"KAMA_er_period\"]),\n",
        "                \"KAMA_fast_period\": int(row[\"KAMA_fast_period\"]),\n",
        "                \"KAMA_slow_period\": int(row[\"KAMA_slow_period\"]),\n",
        "                \"ST_period\": int(row[\"Supertrend_period\"]),\n",
        "                \"ST_multiplier\": float(row[\"Supertrend_multiplier\"]),\n",
        "                \"MFI_period\": int(row[\"MFI_period\"]),\n",
        "                \"MFI_oversold\": float(row[\"MFI_oversold\"]),\n",
        "                \"MFI_overbought\": float(row[\"MFI_overbought\"]),\n",
        "                \"Sharpe\": sharpe, \"Sortino\": sortino, \"Total_Return\": totalret, \"CAGR\": cagr,\n",
        "                \"Max_Drawdown\": max_dd, \"Avg_Drawdown\": avg_dd, \"Win_Rate\": win_rate,\n",
        "                \"Avg_Win\": avg_win, \"Avg_Loss\": avg_loss, \"Profit_Factor\": pf,\n",
        "                \"Trades\": int(run_stats[\"Trades\"]),\n",
        "                \"Avg_Time_In_Trade\": float(run_stats[\"Avg_Time_In_Trade\"]),\n",
        "                \"Avg_Time_In_Win_Trade\": float(run_stats[\"Avg_Time_In_Win_Trade\"]),\n",
        "                \"Avg_Time_In_Loss_Trade\": float(run_stats[\"Avg_Time_In_Loss_Trade\"]),\n",
        "                \"Time_Out_Of_Market\": time_out,\n",
        "                \"Profitable_Bar_Time_Rate\": pbar_rate,\n",
        "                \"RS63_last3\": str(rs_tail),\n",
        "            })\n",
        "\n",
        "            # Stream to CSV in batches\n",
        "            if len(batch) >= local_stream_every:\n",
        "                pd.DataFrame(batch).to_csv(out_path, mode='a',\n",
        "                                           header=not os.path.exists(out_path), index=False)\n",
        "                written += len(batch)\n",
        "                print(f\"[FLUSH] {t}: +{len(batch):,} rows (total={written:,}) â†’ {out_path}\")\n",
        "                batch.clear()\n",
        "                gc.collect()\n",
        "\n",
        "            # Throttle RAM if needed + adaptive backoff\n",
        "            if (i+1) % THROTTLE_EVERY == 0 and _should_throttle():\n",
        "                print(f\"[THROTTLE] {t}: gc + sleep {THROTTLE_SLEEP_S}s | stream_every={local_stream_every} | {_fmt_ram()}\")\n",
        "                gc.collect()\n",
        "                time.sleep(THROTTLE_SLEEP_S)\n",
        "                # adaptive backoff: reduce batch size (min 100)\n",
        "                new_stream_every = max(100, int(local_stream_every * ADAPTIVE_BACKOFF_STEP))\n",
        "                if new_stream_every < local_stream_every:\n",
        "                    print(f\"[ADAPT] {t}: stream batch {local_stream_every} â†’ {new_stream_every}\")\n",
        "                    local_stream_every = new_stream_every\n",
        "\n",
        "            # periodic progress\n",
        "            if combo_count % progress_every == 0 or combo_count == 1:\n",
        "                elapsed = time.time() - start_t\n",
        "                pct = 100.0 * combo_count / max(1, len(combos))\n",
        "                print(f\"[PROG] {t}: {combo_count:,}/{len(combos):,} ({pct:5.1f}%) | written={written:,} | \"\n",
        "                      f\"stream_every={local_stream_every} | elapsed={elapsed:,.1f}s | {_fmt_ram()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERR ] {t} / combo {int(row.get('combination_id', -1))}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Flush remaining rows\n",
        "    if batch:\n",
        "        pd.DataFrame(batch).to_csv(out_path, mode='a', header=not os.path.exists(out_path), index=False)\n",
        "        written += len(batch)\n",
        "        print(f\"[FLUSH] {t}: +{len(batch):,} rows (total={written:,}) â†’ {out_path}\")\n",
        "        batch.clear()\n",
        "        gc.collect()\n",
        "\n",
        "    elapsed = time.time() - start_t\n",
        "    print(f\"[DONE] {t}: wrote {written:,} rows in {elapsed:,.1f}s â†’ {out_path} | {_fmt_ram()}\")\n",
        "    del ind, td\n",
        "    gc.collect()\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# ---------------------- Driver: serial or parallel per-ticker ----------------------\n",
        "# Ensure combos are available from CSV (robust to undefined in session)\n",
        "PARAM_CSV = \"/Users/chielg/Documents/GitHub/TA_optimization/parameter_combinations.csv\"\n",
        "combos  = pd.read_csv(PARAM_CSV)\n",
        "\n",
        "tickers = data[\"Ticker\"].unique()\n",
        "print(f\"[DRIVER] Evaluating {len(tickers)} tickers Ã— {len(combos):,} combinations = {len(tickers)*len(combos):,} evals (serial={not USE_PARALLEL})\")\n",
        "\n",
        "per_ticker_files = []\n",
        "\n",
        "if USE_PARALLEL:\n",
        "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "    print(f\"[DRIVER] Starting parallel pool with max_workers={MAX_WORKERS}\")\n",
        "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futs = {ex.submit(_evaluate_ticker_metrics, t, data, combos): t for t in tickers}\n",
        "        for f in as_completed(futs):\n",
        "            t = futs[f]\n",
        "            try:\n",
        "                p = f.result()\n",
        "                if p:\n",
        "                    per_ticker_files.append(p)\n",
        "                    print(f\"[DRIVER] Finished {t} â†’ {p}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[DRIVER] {t} raised: {e}\")\n",
        "else:\n",
        "    for t in tickers:\n",
        "        _t0 = time.time()\n",
        "        p = _evaluate_ticker_metrics(t, data, combos)\n",
        "        dt = time.time() - _t0\n",
        "        if p:\n",
        "            per_ticker_files.append(p)\n",
        "            print(f\"[DRIVER] Finished {t} in {dt:,.1f}s â†’ {p}\")\n",
        "\n",
        "# ---------------------- Merge all per-ticker CSVs into final file ----------------------\n",
        "if os.path.exists(FINAL_OUT_CSV):\n",
        "    os.remove(FINAL_OUT_CSV)\n",
        "\n",
        "print(f\"[MERGE] Collating {len(per_ticker_files)} files â†’ {FINAL_OUT_CSV}\")\n",
        "header_written = False\n",
        "total_merged = 0\n",
        "for fp in per_ticker_files:\n",
        "    try:\n",
        "        print(f\"[MERGE] {fp} ...\")\n",
        "        for chunk in pd.read_csv(fp, chunksize=200_000):\n",
        "            chunk.to_csv(FINAL_OUT_CSV, mode='a', header=not header_written, index=False)\n",
        "            header_written = True\n",
        "            total_merged += len(chunk)\n",
        "            print(f\"[MERGE] {fp}: +{len(chunk):,} rows (total={total_merged:,})\")\n",
        "    except Exception as e:\n",
        "        print(f\"[MERGE] warning for {fp}: {e}\")\n",
        "\n",
        "print(f\"[DONE ] Saved all metrics ({total_merged:,} rows) â†’ {FINAL_OUT_CSV}\")\n",
        "\n",
        "# Small preview without loading everything\n",
        "try:\n",
        "    preview_df = pd.read_csv(FINAL_OUT_CSV, nrows=20)\n",
        "    display(preview_df)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sig_kama' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Combine with OR logic (enter if ANY is 1; exit if ANY is -1; else out)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# We respect your raw signals:\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# - 1  = in (buy)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# - 0  = neutral (no entry)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# - -1 = out (sell/exit)\u001b[39;00m\n\u001b[32m      6\u001b[39m parts_raw = pd.DataFrame({\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mKAMA\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43msig_kama\u001b[49m,     \u001b[38;5;66;03m# expects {-1,0,1} per your get_buy_signal\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mST\u001b[39m\u001b[33m\"\u001b[39m:   sig_st,       \u001b[38;5;66;03m# supertrend can produce -1\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMFI\u001b[39m\u001b[33m\"\u001b[39m:  sig_mfi\n\u001b[32m     10\u001b[39m })\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Row-wise logic:\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# - If any -1 â†’ exit (0)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# - Else if any 1 â†’ enter (1)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# - Else â†’ 0 (stay out)\u001b[39;00m\n\u001b[32m     16\u001b[39m sell_mask = (parts_raw == -\u001b[32m1\u001b[39m).any(axis=\u001b[32m1\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'sig_kama' is not defined"
          ]
        }
      ],
      "source": [
        "# Combine with OR logic (enter if ANY is 1; exit if ANY is -1; else out)\n",
        "# We respect your raw signals:\n",
        "# - 1  = in (buy)\n",
        "# - 0  = neutral (no entry)\n",
        "# - -1 = out (sell/exit)\n",
        "parts_raw = pd.DataFrame({\n",
        "    \"KAMA\": sig_kama,     # expects {-1,0,1} per your get_buy_signal\n",
        "    \"ST\":   sig_st,       # supertrend can produce -1\n",
        "    \"MFI\":  sig_mfi\n",
        "})\n",
        "\n",
        "# Row-wise logic:\n",
        "# - If any -1 â†’ exit (0)\n",
        "# - Else if any 1 â†’ enter (1)\n",
        "# - Else â†’ 0 (stay out)\n",
        "sell_mask = (parts_raw == -1).any(axis=1)\n",
        "buy_mask  = (parts_raw ==  1).any(axis=1)\n",
        "\n",
        "combined_signal = pd.Series(0.0, index=parts_raw.index)\n",
        "combined_signal[sell_mask] = 0.0\n",
        "combined_signal[~sell_mask & buy_mask] = 1.0\n",
        "\n",
        "# If all inputs are NaN on a row, keep NaN (unknown)\n",
        "all_nan = parts_raw.isna().all(axis=1)\n",
        "combined_signal[all_nan] = np.nan\n",
        "\n",
        "print(\"\\nCombined components (last 50 rows):\")\n",
        "print(parts_raw.tail(50).to_string())\n",
        "print(\"\\nCombined (OR with sell priority) signal (last 50 rows):\")\n",
        "print(combined_signal.tail(200).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'target' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Next-day execution\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m position = \u001b[43mtarget\u001b[49m.shift(\u001b[32m1\u001b[39m).fillna(\u001b[32m0.0\u001b[39m)\n\u001b[32m      6\u001b[39m bt = pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m\"\u001b[39m: td[\u001b[33m\"\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mPosition\u001b[39m\u001b[33m\"\u001b[39m: position})\n\u001b[32m      7\u001b[39m bt[\u001b[33m\"\u001b[39m\u001b[33mR_close\u001b[39m\u001b[33m\"\u001b[39m] = bt[\u001b[33m\"\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m\"\u001b[39m].pct_change()\n",
            "\u001b[31mNameError\u001b[39m: name 'target' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Next-day execution\n",
        "position = target.shift(1).fillna(0.0)\n",
        "\n",
        "bt = pd.DataFrame({\"Close\": td[\"Close\"], \"Position\": position})\n",
        "bt[\"R_close\"] = bt[\"Close\"].pct_change()\n",
        "bt[\"Strategy_Return\"] = bt[\"Position\"] * bt[\"R_close\"]\n",
        "bt[\"Equity\"] = (1.0 + bt[\"Strategy_Return\"].fillna(0)).cumprod()\n",
        "\n",
        "def rolling_sharpe(returns: pd.Series, window: int = 63, trading_days: int = 252) -> pd.Series:\n",
        "    m = returns.rolling(window, min_periods=window).mean()\n",
        "    s = returns.rolling(window, min_periods=window).std()\n",
        "    rs = (m / s) * np.sqrt(trading_days)\n",
        "    return rs.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "win = 63\n",
        "bt[\"Rolling_Sharpe\"] = rolling_sharpe(bt[\"Strategy_Return\"], window=win)\n",
        "\n",
        "print(\"\\nBacktest preview (first 8):\")\n",
        "print(bt[[\"Position\",\"R_close\",\"Strategy_Return\",\"Equity\",\"Rolling_Sharpe\"]].head(32).to_string())\n",
        "print(\"\\nBacktest preview (last 8):\")\n",
        "print(bt[[\"Position\",\"R_close\",\"Strategy_Return\",\"Equity\",\"Rolling_Sharpe\"]].tail(32).to_string())\n",
        "\n",
        "# Plots\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
        "ax[0].plot(bt.index, bt[\"Equity\"], label=\"Equity\", color=\"tab:blue\")\n",
        "ax[0].set_title(f\"{ticker} â€“ Equity (OR combine, next-day execution)\")\n",
        "ax[0].grid(True, alpha=0.3)\n",
        "ax[0].legend(loc=\"upper left\")\n",
        "\n",
        "ax[1].plot(bt.index, bt[\"Rolling_Sharpe\"], label=f\"Rolling Sharpe ({win})\", color=\"tab:orange\")\n",
        "ax[1].axhline(0, color=\"gray\", lw=1, alpha=0.6)\n",
        "ax[1].set_title(\"Rolling Sharpe\")\n",
        "ax[1].grid(True, alpha=0.3)\n",
        "ax[1].legend(loc=\"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep this small so you can inspect. Adjust head() limits as you like.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "\n",
        "# --- inline metric helpers (kept inside this cell so you don't have to jump around) ---\n",
        "TRADING_DAYS = 252\n",
        "\n",
        "def _cagr(returns: pd.Series) -> float:\n",
        "    if returns.empty: return np.nan\n",
        "    eq = (1.0 + returns).cumprod()\n",
        "    start, end = returns.index.min(), returns.index.max()\n",
        "    years = (end - start).days / 365.25 if isinstance(start, pd.Timestamp) else len(returns) / TRADING_DAYS\n",
        "    if not years or years <= 0: return np.nan\n",
        "    end_val = float(eq.iloc[-1]) if len(eq) else np.nan\n",
        "    if not np.isfinite(end_val) or end_val <= 0: return np.nan\n",
        "    return end_val**(1.0 / years) - 1.0\n",
        "\n",
        "def _max_dd(equity: pd.Series) -> float:\n",
        "    if equity.empty: return np.nan\n",
        "    rm = equity.cummax()\n",
        "    dd = equity / rm - 1.0\n",
        "    return float(dd.min())\n",
        "\n",
        "def _avg_dd(equity: pd.Series) -> float:\n",
        "    if equity.empty: return 0.0\n",
        "    rm = equity.cummax()\n",
        "    dd = equity / rm - 1.0\n",
        "    dd_neg = dd[dd < 0]\n",
        "    return float(dd_neg.mean()) if len(dd_neg) else 0.0\n",
        "\n",
        "def _sharpe(returns: pd.Series) -> float:\n",
        "    if returns.empty: return np.nan\n",
        "    m, s = returns.mean(), returns.std()\n",
        "    return float((m / s) * np.sqrt(TRADING_DAYS)) if s and s > 0 else np.nan\n",
        "\n",
        "def _sortino(returns: pd.Series) -> float:\n",
        "    if returns.empty: return np.nan\n",
        "    downside = returns[returns < 0]\n",
        "    ds = downside.std()\n",
        "    return float((returns.mean() / ds) * np.sqrt(TRADING_DAYS)) if ds and ds > 0 else np.nan\n",
        "\n",
        "def _profit_factor(returns: pd.Series) -> float:\n",
        "    if returns.empty: return np.nan\n",
        "    gain = returns[returns > 0].sum()\n",
        "    loss = returns[returns < 0].sum()\n",
        "    if loss == 0: return np.inf if gain > 0 else np.nan\n",
        "    return float(gain / abs(loss))\n",
        "\n",
        "def _run_lengths(position: pd.Series, ret: pd.Series) -> dict:\n",
        "    pos = position.fillna(0).astype(int)\n",
        "    change = pos.ne(pos.shift(1)).cumsum()\n",
        "    df = pd.DataFrame({\"pos\": pos, \"gid\": change, \"ret\": ret}).loc[pos == 1]\n",
        "    if df.empty:\n",
        "        return {\"Avg_Time_In_Trade\": 0.0, \"Avg_Time_In_Win_Trade\": 0.0, \"Avg_Time_In_Loss_Trade\": 0.0, \"Trades\": 0}\n",
        "    grouped = df.groupby(\"gid\")\n",
        "    lengths = grouped.size()\n",
        "    trade_returns = grouped[\"ret\"].sum()\n",
        "    return {\n",
        "        \"Avg_Time_In_Trade\": float(lengths.mean()) if len(lengths) else 0.0,\n",
        "        \"Avg_Time_In_Win_Trade\": float(lengths[trade_returns > 0].mean()) if (trade_returns > 0).any() else 0.0,\n",
        "        \"Avg_Time_In_Loss_Trade\": float(lengths[trade_returns < 0].mean()) if (trade_returns < 0).any() else 0.0,\n",
        "        \"Trades\": int((pos.diff() == 1).sum())\n",
        "    }\n",
        "\n",
        "def _time_out_of_mkt(position: pd.Series) -> float:\n",
        "    return float((position.fillna(0) == 0).mean())\n",
        "\n",
        "def _pbar_time_rate(returns: pd.Series, position: pd.Series) -> float:\n",
        "    mask = position.fillna(0).astype(int) == 1\n",
        "    return float((returns[mask] > 0).mean()) if mask.any() else 0.0\n",
        "\n",
        "def _rolling_sharpe(returns: pd.Series, window: int = 63) -> pd.Series:\n",
        "    m = returns.rolling(window, min_periods=window).mean()\n",
        "    s = returns.rolling(window, min_periods=window).std()\n",
        "    rs = (m / s) * np.sqrt(TRADING_DAYS)\n",
        "    return rs.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 213\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# ---------------------- Driver: serial or parallel per-ticker ----------------------\u001b[39;00m\n\u001b[32m    212\u001b[39m tickers = data[\u001b[33m\"\u001b[39m\u001b[33mTicker\u001b[39m\u001b[33m\"\u001b[39m].unique()\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m combos  = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparameter_combinations.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tickers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tickers Ã— \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(combos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m combinations...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    217\u001b[39m per_ticker_files = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1968\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m         new_col_dict = col_dict\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     df = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m     \u001b[38;5;28mself\u001b[39m._currow += new_rows\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/core/frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/core/internals/construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:2139\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[32m   2122\u001b[39m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[32m   2123\u001b[39m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2135\u001b[39m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[32m   2136\u001b[39m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[32m   2138\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2139\u001b[39m         blocks = \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2140\u001b[39m         mgr = BlockManager(blocks, axes, verify_integrity=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2141\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:2212\u001b[39m, in \u001b[36m_form_blocks\u001b[39m\u001b[34m(arrays, consolidate, refs)\u001b[39m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype.type, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[32m   2210\u001b[39m     dtype = np.dtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m values, placement = \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[32m   2214\u001b[39m     values = ensure_wrapped_if_datetimelike(values)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TA_optimization/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:2254\u001b[39m, in \u001b[36m_stack_arrays\u001b[39m\u001b[34m(tuples, dtype)\u001b[39m\n\u001b[32m   2252\u001b[39m stacked = np.empty(shape, dtype=dtype)\n\u001b[32m   2253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m     stacked[i] = arr\n\u001b[32m   2256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stacked, placement\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# # ================================================================================\n",
        "# # FULL EVALUATION OVER ALL TICKERS AND ALL COMBINATIONS\n",
        "# # (proactive throttling, adaptive backoff, per-ticker time budget, serial by default)\n",
        "# # ================================================================================\n",
        "\n",
        "# import os, gc, time, warnings, uuid\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import pandas_ta as ta\n",
        "\n",
        "# # Suppress noisy warnings\n",
        "# np.set_printoptions(suppress=True)\n",
        "# warnings.filterwarnings('ignore', category=RuntimeWarning, message='overflow encountered in*')\n",
        "# warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in*')\n",
        "# warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# # ---------------------- Throttling & Streaming knobs ----------------------\n",
        "# try:\n",
        "#     import psutil\n",
        "# except Exception:\n",
        "#     psutil = None\n",
        "\n",
        "# USE_PARALLEL = False        # keep False for stability; turn True only if needed\n",
        "# MAX_WORKERS  = 2            # if you enable parallel, keep small (2â€“3)\n",
        "\n",
        "# THROTTLE_EVERY   = 1_000    # check RAM every N combos\n",
        "# THROTTLE_SLEEP_S = 1.0      # sleep when throttling\n",
        "# MEM_TARGET       = 0.80     # throttle when >=80% RAM used\n",
        "# MIN_FREE_GB      = 1.0      # or when <1 GB free\n",
        "\n",
        "# STREAM_DIR    = \"grid_eval_metrics_batches\"  # per-ticker output directory\n",
        "# STREAM_EVERY  = 300          # smaller batch for lower RAM\n",
        "# FINAL_OUT_CSV = \"grid_eval_metrics_full.csv\" # final merged file\n",
        "\n",
        "# # Extra safety guards (tunable)\n",
        "# TICKER_TIME_BUDGET_S   = 600   # stop a ticker after 10 minutes (None to disable)\n",
        "# MAX_COMBOS_PER_TICKER  = None  # e.g., 100000 to cap per ticker (None to disable)\n",
        "# ADAPTIVE_BACKOFF_STEP  = 0.5   # multiply STREAM_EVERY by this when throttling (min 100)\n",
        "\n",
        "# def _should_throttle() -> bool:\n",
        "#     if psutil is None:\n",
        "#         return False\n",
        "#     vm = psutil.virtual_memory()\n",
        "#     return (vm.percent >= MEM_TARGET*100) or (vm.available/1e9 < MIN_FREE_GB)\n",
        "\n",
        "# # ---------------------- Per-ticker evaluation ----------------------\n",
        "# def _evaluate_ticker_metrics(t: str, data: pd.DataFrame, combos: pd.DataFrame) -> str:\n",
        "#     \"\"\"\n",
        "#     Evaluate all combos for ticker t. Streams to a per-ticker CSV and returns its path.\n",
        "#     Applies RAM throttling, adaptive backoff, and optional time/combo budgets.\n",
        "#     \"\"\"\n",
        "#     td = data[data[\"Ticker\"] == t].sort_index().copy()\n",
        "#     if len(td) < 60:\n",
        "#         print(f\"Skip {t}: insufficient data\")\n",
        "#         return \"\"\n",
        "\n",
        "#     os.makedirs(STREAM_DIR, exist_ok=True)\n",
        "#     out_path = os.path.join(STREAM_DIR, f\"grid_eval_metrics_{t}.csv\")\n",
        "#     if os.path.exists(out_path):\n",
        "#         os.remove(out_path)\n",
        "\n",
        "#     ind = Indicator(td)\n",
        "\n",
        "#     # Prefer existing KAMA short/long columns if present\n",
        "#     short_cols = [c for c in td.columns if c.startswith(\"KAMA_SHORT_\")]\n",
        "#     long_cols  = [c for c in td.columns if c.startswith(\"KAMA_LONG_\")]\n",
        "\n",
        "#     print(f\"\\nTicker {t}: {len(combos)} combos\")\n",
        "#     start_t = time.time()\n",
        "#     combo_count = 0\n",
        "\n",
        "#     # local adaptive stream size\n",
        "#     local_stream_every = STREAM_EVERY\n",
        "\n",
        "#     batch = []\n",
        "#     written = 0\n",
        "\n",
        "#     for i, row in combos.iterrows():\n",
        "#         # per-ticker time budget\n",
        "#         if TICKER_TIME_BUDGET_S is not None and (time.time() - start_t) > TICKER_TIME_BUDGET_S:\n",
        "#             print(f\"  {t}: time budget reached ({TICKER_TIME_BUDGET_S}s), stopping early.\")\n",
        "#             break\n",
        "\n",
        "#         # optional per-ticker combo cap\n",
        "#         if MAX_COMBOS_PER_TICKER is not None and combo_count >= MAX_COMBOS_PER_TICKER:\n",
        "#             print(f\"  {t}: combo cap reached ({MAX_COMBOS_PER_TICKER}), stopping early.\")\n",
        "#             break\n",
        "\n",
        "#         combo_count += 1\n",
        "\n",
        "#         try:\n",
        "#             # --- KAMA crossover ---\n",
        "#             if short_cols and long_cols:\n",
        "#                 short_kama = td[short_cols[0]].astype('float64')\n",
        "#                 long_kama  = td[long_cols[0]].astype('float64')\n",
        "#             else:\n",
        "#                 short_kama = ta.kama(td[\"Close\"].astype('float64'),\n",
        "#                                      length=int(row[\"KAMA_er_period\"]),\n",
        "#                                      fast=int(row[\"KAMA_fast_period\"]),\n",
        "#                                      slow=int(row[\"KAMA_slow_period\"]))\n",
        "#                 long_kama  = ta.kama(td[\"Close\"].astype('float64'),\n",
        "#                                      length=int(row[\"KAMA_er_period\"]) + 20,\n",
        "#                                      fast=int(row[\"KAMA_fast_period\"]) + 10,\n",
        "#                                      slow=int(row[\"KAMA_slow_period\"]) + 20)\n",
        "\n",
        "#             sig_kama = get_buy_signal(\"KAMA\", data=short_kama,\n",
        "#                                       params={\"short_kama\": short_kama, \"long_kama\": long_kama})\n",
        "\n",
        "#             # --- Supertrend ---\n",
        "#             st_line = ind.supertrend(int(row[\"Supertrend_period\"]), float(row[\"Supertrend_multiplier\"]))\n",
        "#             sig_st = get_buy_signal(\"Supertrend\", data=st_line, params={\"close\": td[\"Close\"].astype('float64')})\n",
        "\n",
        "#             # --- MFI ---\n",
        "#             mfi_s = ind.mfi(int(row[\"MFI_period\"]))\n",
        "#             sig_mfi = get_buy_signal(\"MFI\", data=mfi_s,\n",
        "#                                      params={\"oversold\": float(row[\"MFI_oversold\"]),\n",
        "#                                              \"overbought\": float(row[\"MFI_overbought\"])})\n",
        "\n",
        "#             # Combine signals (OR with SELL priority) without big DataFrame\n",
        "#             sell_mask = ((sig_kama == -1) | (sig_st == -1) | (sig_mfi == -1)).fillna(False)\n",
        "#             buy_mask  = ((sig_kama ==  1) | (sig_st ==  1) | (sig_mfi ==  1)).fillna(False)\n",
        "\n",
        "#             events = pd.Series(np.nan, index=td.index, dtype=\"float32\")\n",
        "#             events[sell_mask] = 0.0\n",
        "#             events[~sell_mask & buy_mask] = 1.0\n",
        "#             target = events.ffill().fillna(0.0)\n",
        "#             position = target.shift(1).fillna(0.0)  # next-day execution\n",
        "\n",
        "#             # Backtest vector math\n",
        "#             r_close = td[\"Close\"].astype('float64').pct_change().astype('float32')\n",
        "#             strat_r = (position.astype('float32') * r_close).astype('float32')\n",
        "\n",
        "#             r = strat_r.dropna()\n",
        "#             equity = (1.0 + r).cumprod() if len(r) else pd.Series(dtype='float32')\n",
        "\n",
        "#             # --- Metrics with native Python type conversion ---\n",
        "#             s_val   = _sharpe(r);   sharpe   = float(s_val) if not pd.isna(s_val) else None\n",
        "#             so_val  = _sortino(r);  sortino  = float(so_val) if not pd.isna(so_val) else None\n",
        "#             totalret = float(equity.iloc[-1] - 1.0) if len(equity) else None\n",
        "#             cg_val  = _cagr(r);     cagr     = float(cg_val) if len(r) and not pd.isna(cg_val) else None\n",
        "#             md_val  = _max_dd(equity); max_dd   = float(md_val) if len(equity) and not pd.isna(md_val) else None\n",
        "#             ad_val  = _avg_dd(equity); avg_dd   = float(ad_val) if len(equity) and not pd.isna(ad_val) else None\n",
        "#             win_rate = float((r > 0).mean()) if len(r) else None\n",
        "#             avg_win  = float(r[r > 0].mean()) if (r > 0).any() else None\n",
        "#             avg_loss = float(r[r < 0].mean()) if (r < 0).any() else None\n",
        "#             pf_val   = _profit_factor(r); pf   = float(pf_val) if not pd.isna(pf_val) else None\n",
        "#             run_stats = _run_lengths(position, strat_r)\n",
        "#             toom_val  = _time_out_of_mkt(position); time_out  = float(toom_val) if not pd.isna(toom_val) else None\n",
        "#             pbar_val  = _pbar_time_rate(strat_r, position); pbar_rate = float(pbar_val) if not pd.isna(pbar_val) else None\n",
        "#             rs_tail   = [float(x) for x in _rolling_sharpe(strat_r, window=63).dropna().tail(3).round(3)] if len(strat_r) > 63 else [None, None, None]\n",
        "\n",
        "#             # Build row\n",
        "#             batch.append({\n",
        "#                 \"Ticker\": t,\n",
        "#                 \"combination_id\": int(row.get(\"combination_id\", -1)),\n",
        "#                 \"KAMA_er_period\": int(row[\"KAMA_er_period\"]),\n",
        "#                 \"KAMA_fast_period\": int(row[\"KAMA_fast_period\"]),\n",
        "#                 \"KAMA_slow_period\": int(row[\"KAMA_slow_period\"]),\n",
        "#                 \"ST_period\": int(row[\"Supertrend_period\"]),\n",
        "#                 \"ST_multiplier\": float(row[\"Supertrend_multiplier\"]),\n",
        "#                 \"MFI_period\": int(row[\"MFI_period\"]),\n",
        "#                 \"MFI_oversold\": float(row[\"MFI_oversold\"]),\n",
        "#                 \"MFI_overbought\": float(row[\"MFI_overbought\"]),\n",
        "#                 \"Sharpe\": sharpe, \"Sortino\": sortino, \"Total_Return\": totalret, \"CAGR\": cagr,\n",
        "#                 \"Max_Drawdown\": max_dd, \"Avg_Drawdown\": avg_dd, \"Win_Rate\": win_rate,\n",
        "#                 \"Avg_Win\": avg_win, \"Avg_Loss\": avg_loss, \"Profit_Factor\": pf,\n",
        "#                 \"Trades\": int(run_stats[\"Trades\"]),\n",
        "#                 \"Avg_Time_In_Trade\": float(run_stats[\"Avg_Time_In_Trade\"]),\n",
        "#                 \"Avg_Time_In_Win_Trade\": float(run_stats[\"Avg_Time_In_Win_Trade\"]),\n",
        "#                 \"Avg_Time_In_Loss_Trade\": float(run_stats[\"Avg_Time_In_Loss_Trade\"]),\n",
        "#                 \"Time_Out_Of_Market\": time_out,\n",
        "#                 \"Profitable_Bar_Time_Rate\": pbar_rate,\n",
        "#                 \"RS63_last3\": str(rs_tail),\n",
        "#             })\n",
        "\n",
        "#             # Stream to CSV in batches\n",
        "#             if len(batch) >= local_stream_every:\n",
        "#                 pd.DataFrame(batch).to_csv(out_path, mode='a',\n",
        "#                                            header=not os.path.exists(out_path), index=False)\n",
        "#                 written += len(batch)\n",
        "#                 batch.clear()\n",
        "#                 gc.collect()\n",
        "\n",
        "#             # Throttle RAM if needed + adaptive backoff\n",
        "#             if (i+1) % THROTTLE_EVERY == 0 and _should_throttle():\n",
        "#                 print(f\"  {t}: throttling (gc + sleep {THROTTLE_SLEEP_S}s) | stream_every={local_stream_every}\")\n",
        "#                 gc.collect()\n",
        "#                 time.sleep(THROTTLE_SLEEP_S)\n",
        "#                 # adaptive backoff: reduce batch size (min 100)\n",
        "#                 new_stream_every = max(100, int(local_stream_every * ADAPTIVE_BACKOFF_STEP))\n",
        "#                 if new_stream_every < local_stream_every:\n",
        "#                     print(f\"  {t}: lowering stream batch {local_stream_every} â†’ {new_stream_every}\")\n",
        "#                     local_stream_every = new_stream_every\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"{t} / combo {int(row.get('combination_id', -1))}: error ({e})\")\n",
        "#             continue\n",
        "\n",
        "#     # Flush remaining rows\n",
        "#     if batch:\n",
        "#         pd.DataFrame(batch).to_csv(out_path, mode='a', header=not os.path.exists(out_path), index=False)\n",
        "#         written += len(batch)\n",
        "#         batch.clear()\n",
        "#         gc.collect()\n",
        "\n",
        "#     print(f\"Ticker {t}: wrote {written} rows â†’ {out_path}\")\n",
        "#     del ind, td\n",
        "#     gc.collect()\n",
        "#     return out_path\n",
        "\n",
        "# # ---------------------- Driver: serial or parallel per-ticker ----------------------\n",
        "# tickers = data[\"Ticker\"].unique()\n",
        "# combos  = pd.read_csv(\"parameter_combinations.csv\")\n",
        "\n",
        "# print(f\"Evaluating {len(tickers)} tickers Ã— {len(combos)} combinations...\")\n",
        "\n",
        "# per_ticker_files = []\n",
        "\n",
        "# if USE_PARALLEL:\n",
        "#     # Enable only if you need it; processes can be brittle in notebooks/macOS\n",
        "#     from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "#     with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "#         futs = {ex.submit(_evaluate_ticker_metrics, t, data, combos): t for t in tickers}\n",
        "#         for f in as_completed(futs):\n",
        "#             p = f.result()\n",
        "#             if p:\n",
        "#                 per_ticker_files.append(p)\n",
        "# else:\n",
        "#     for t in tickers:\n",
        "#         p = _evaluate_ticker_metrics(t, data, combos)\n",
        "#         if p:\n",
        "#             per_ticker_files.append(p)\n",
        "\n",
        "# # ---------------------- Merge all per-ticker CSVs into final file ----------------------\n",
        "# if os.path.exists(FINAL_OUT_CSV):\n",
        "#     os.remove(FINAL_OUT_CSV)\n",
        "\n",
        "# header_written = False\n",
        "# total_merged = 0\n",
        "# for fp in per_ticker_files:\n",
        "#     try:\n",
        "#         for chunk in pd.read_csv(fp, chunksize=200_000):\n",
        "#             chunk.to_csv(FINAL_OUT_CSV, mode='a', header=not header_written, index=False)\n",
        "#             header_written = True\n",
        "#             total_merged += len(chunk)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Merge warning for {fp}: {e}\")\n",
        "\n",
        "# print(f\"\\nSaved all metrics ({total_merged:,} rows) to: {FINAL_OUT_CSV}\")\n",
        "\n",
        "# # Small preview without loading everything\n",
        "# try:\n",
        "#     preview_df = pd.read_csv(FINAL_OUT_CSV, nrows=20)\n",
        "#     display(preview_df)\n",
        "# except Exception:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CONFIG] USE_PARALLEL=False, MAX_WORKERS=2, STREAM_EVERY=300, THROTTLE_EVERY=1000, MEM_TARGET=80%, MIN_FREE_GB=1.0\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# FULL EVALUATION OVER ALL TICKERS AND ALL COMBINATIONS (with detailed logging)\n",
        "# ================================================================================\n",
        "\n",
        "import os, gc, time, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "\n",
        "# Suppress noisy warnings\n",
        "np.set_printoptions(suppress=True)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='overflow encountered in*')\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in*')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# ---------------------- Throttling & Streaming knobs ----------------------\n",
        "try:\n",
        "    import psutil\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "USE_PARALLEL = False        # keep False for stability; turn True only if needed\n",
        "MAX_WORKERS  = 2            # if you enable parallel, keep small (2â€“3)\n",
        "\n",
        "THROTTLE_EVERY   = 1_000    # check RAM every N combos\n",
        "THROTTLE_SLEEP_S = 1.0      # sleep when throttling\n",
        "MEM_TARGET       = 0.80     # throttle when >=80% RAM used\n",
        "MIN_FREE_GB      = 1.0      # or when <1 GB free\n",
        "\n",
        "STREAM_DIR    = \"grid_eval_metrics_batches\"  # per-ticker output directory\n",
        "STREAM_EVERY  = 300          # smaller batch for lower RAM\n",
        "FINAL_OUT_CSV = \"grid_eval_metrics_full.csv\" # final merged file\n",
        "\n",
        "# Extra safety guards (tunable)\n",
        "TICKER_TIME_BUDGET_S   = 600   # stop a ticker after 10 minutes (None to disable)\n",
        "MAX_COMBOS_PER_TICKER  = None  # e.g., 100000 to cap per ticker (None to disable)\n",
        "ADAPTIVE_BACKOFF_STEP  = 0.5   # multiply STREAM_EVERY by this when throttling (min 100)\n",
        "\n",
        "print(f\"[CONFIG] USE_PARALLEL={USE_PARALLEL}, MAX_WORKERS={MAX_WORKERS}, STREAM_EVERY={STREAM_EVERY}, \"\n",
        "      f\"THROTTLE_EVERY={THROTTLE_EVERY}, MEM_TARGET={int(MEM_TARGET*100)}%, MIN_FREE_GB={MIN_FREE_GB}\")\n",
        "\n",
        "\n",
        "def _fmt_ram() -> str:\n",
        "    if not psutil:\n",
        "        return \"RAM: n/a\"\n",
        "    vm = psutil.virtual_memory()\n",
        "    return f\"RAM used={vm.percent:.1f}% free={vm.available/1e9:.2f}GB\"\n",
        "\n",
        "\n",
        "def _should_throttle() -> bool:\n",
        "    if psutil is None:\n",
        "        return False\n",
        "    vm = psutil.virtual_memory()\n",
        "    return (vm.percent >= MEM_TARGET*100) or (vm.available/1e9 < MIN_FREE_GB)\n",
        "\n",
        "\n",
        "# ---------------------- Per-ticker evaluation ----------------------\n",
        "def _evaluate_ticker_metrics(t: str, data: pd.DataFrame, combos: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate all combos for ticker t. Streams to a per-ticker CSV and returns its path.\n",
        "    Applies RAM throttling, adaptive backoff, and optional time/combo budgets.\n",
        "    \"\"\"\n",
        "    td = data[data[\"Ticker\"] == t].sort_index().copy()\n",
        "    if len(td) < 60:\n",
        "        print(f\"[SKIP] {t}: insufficient data ({len(td)} rows)\")\n",
        "        return \"\"\n",
        "\n",
        "    os.makedirs(STREAM_DIR, exist_ok=True)\n",
        "    out_path = os.path.join(STREAM_DIR, f\"grid_eval_metrics_{t}.csv\")\n",
        "    if os.path.exists(out_path):\n",
        "        os.remove(out_path)\n",
        "\n",
        "    print(f\"[START] {t}: rows={len(td)}, cols={len(td.columns)} | {_fmt_ram()}\")\n",
        "\n",
        "    ind = Indicator(td)\n",
        "\n",
        "    # Prefer existing KAMA short/long columns if present\n",
        "    short_cols = [c for c in td.columns if c.startswith(\"KAMA_SHORT_\")]\n",
        "    long_cols  = [c for c in td.columns if c.startswith(\"KAMA_LONG_\")]\n",
        "\n",
        "    print(f\"[INFO] {t}: evaluating {len(combos):,} combos | KAMA_short/long cached? {bool(short_cols)} / {bool(long_cols)}\")\n",
        "\n",
        "    start_t = time.time()\n",
        "    combo_count = 0\n",
        "\n",
        "    # local adaptive stream size\n",
        "    local_stream_every = STREAM_EVERY\n",
        "\n",
        "    # progress cadence\n",
        "    progress_every = max(1000, len(combos)//100) if len(combos) else 1000\n",
        "\n",
        "    batch = []\n",
        "    written = 0\n",
        "\n",
        "    for i, row in combos.iterrows():\n",
        "        # per-ticker time budget\n",
        "        if TICKER_TIME_BUDGET_S is not None and (time.time() - start_t) > TICKER_TIME_BUDGET_S:\n",
        "            print(f\"[STOP] {t}: time budget {TICKER_TIME_BUDGET_S}s reached; stopping early at {combo_count:,} combos.\")\n",
        "            break\n",
        "\n",
        "        # optional per-ticker combo cap\n",
        "        if MAX_COMBOS_PER_TICKER is not None and combo_count >= MAX_COMBOS_PER_TICKER:\n",
        "            print(f\"[STOP] {t}: combo cap {MAX_COMBOS_PER_TICKER:,} reached; stopping early.\")\n",
        "            break\n",
        "\n",
        "        combo_count += 1\n",
        "\n",
        "        try:\n",
        "            # --- KAMA crossover ---\n",
        "            if short_cols and long_cols:\n",
        "                short_kama = td[short_cols[0]].astype('float64')\n",
        "                long_kama  = td[long_cols[0]].astype('float64')\n",
        "            else:\n",
        "                short_kama = ta.kama(td[\"Close\"].astype('float64'),\n",
        "                                     length=int(row[\"KAMA_er_period\"]),\n",
        "                                     fast=int(row[\"KAMA_fast_period\"]),\n",
        "                                     slow=int(row[\"KAMA_slow_period\"]))\n",
        "                long_kama  = ta.kama(td[\"Close\"].astype('float64'),\n",
        "                                     length=int(row[\"KAMA_er_period\"]) + 20,\n",
        "                                     fast=int(row[\"KAMA_fast_period\"]) + 10,\n",
        "                                     slow=int(row[\"KAMA_slow_period\"]) + 20)\n",
        "\n",
        "            sig_kama = get_buy_signal(\"KAMA\", data=short_kama,\n",
        "                                      params={\"short_kama\": short_kama, \"long_kama\": long_kama})\n",
        "\n",
        "            # --- Supertrend ---\n",
        "            st_line = ind.supertrend(int(row[\"Supertrend_period\"]), float(row[\"Supertrend_multiplier\"]))\n",
        "            sig_st = get_buy_signal(\"Supertrend\", data=st_line, params={\"close\": td[\"Close\"].astype('float64')})\n",
        "\n",
        "            # --- MFI ---\n",
        "            mfi_s = ind.mfi(int(row[\"MFI_period\"]))\n",
        "            sig_mfi = get_buy_signal(\"MFI\", data=mfi_s,\n",
        "                                     params={\"oversold\": float(row[\"MFI_oversold\"]),\n",
        "                                             \"overbought\": float(row[\"MFI_overbought\"])})\n",
        "\n",
        "            # Combine signals (OR with SELL priority) without big DataFrame\n",
        "            sell_mask = ((sig_kama == -1) | (sig_st == -1) | (sig_mfi == -1)).fillna(False)\n",
        "            buy_mask  = ((sig_kama ==  1) | (sig_st ==  1) | (sig_mfi ==  1)).fillna(False)\n",
        "\n",
        "            events = pd.Series(np.nan, index=td.index, dtype=\"float32\")\n",
        "            events[sell_mask] = 0.0\n",
        "            events[~sell_mask & buy_mask] = 1.0\n",
        "            target = events.ffill().fillna(0.0)\n",
        "            position = target.shift(1).fillna(0.0)  # next-day execution\n",
        "\n",
        "            # Backtest vector math\n",
        "            r_close = td[\"Close\"].astype('float64').pct_change().astype('float32')\n",
        "            strat_r = (position.astype('float32') * r_close).astype('float32')\n",
        "\n",
        "            r = strat_r.dropna()\n",
        "            equity = (1.0 + r).cumprod() if len(r) else pd.Series(dtype='float32')\n",
        "\n",
        "            # --- Metrics with native Python type conversion ---\n",
        "            s_val   = _sharpe(r);   sharpe   = float(s_val) if not pd.isna(s_val) else None\n",
        "            so_val  = _sortino(r);  sortino  = float(so_val) if not pd.isna(so_val) else None\n",
        "            totalret = float(equity.iloc[-1] - 1.0) if len(equity) else None\n",
        "            cg_val  = _cagr(r);     cagr     = float(cg_val) if len(r) and not pd.isna(cg_val) else None\n",
        "            md_val  = _max_dd(equity); max_dd   = float(md_val) if len(equity) and not pd.isna(md_val) else None\n",
        "            ad_val  = _avg_dd(equity); avg_dd   = float(ad_val) if len(equity) and not pd.isna(ad_val) else None\n",
        "            win_rate = float((r > 0).mean()) if len(r) else None\n",
        "            avg_win  = float(r[r > 0].mean()) if (r > 0).any() else None\n",
        "            avg_loss = float(r[r < 0].mean()) if (r < 0).any() else None\n",
        "            pf_val   = _profit_factor(r); pf   = float(pf_val) if not pd.isna(pf_val) else None\n",
        "            run_stats = _run_lengths(position, strat_r)\n",
        "            toom_val  = _time_out_of_mkt(position); time_out  = float(toom_val) if not pd.isna(toom_val) else None\n",
        "            pbar_val  = _pbar_time_rate(strat_r, position); pbar_rate = float(pbar_val) if not pd.isna(pbar_val) else None\n",
        "            rs_tail   = [float(x) for x in _rolling_sharpe(strat_r, window=63).dropna().tail(3).round(3)] if len(strat_r) > 63 else [None, None, None]\n",
        "\n",
        "            # Build row\n",
        "            batch.append({\n",
        "                \"Ticker\": t,\n",
        "                \"combination_id\": int(row.get(\"combination_id\", -1)),\n",
        "                \"KAMA_er_period\": int(row[\"KAMA_er_period\"]),\n",
        "                \"KAMA_fast_period\": int(row[\"KAMA_fast_period\"]),\n",
        "                \"KAMA_slow_period\": int(row[\"KAMA_slow_period\"]),\n",
        "                \"ST_period\": int(row[\"Supertrend_period\"]),\n",
        "                \"ST_multiplier\": float(row[\"Supertrend_multiplier\"]),\n",
        "                \"MFI_period\": int(row[\"MFI_period\"]),\n",
        "                \"MFI_oversold\": float(row[\"MFI_oversold\"]),\n",
        "                \"MFI_overbought\": float(row[\"MFI_overbought\"]),\n",
        "                \"Sharpe\": sharpe, \"Sortino\": sortino, \"Total_Return\": totalret, \"CAGR\": cagr,\n",
        "                \"Max_Drawdown\": max_dd, \"Avg_Drawdown\": avg_dd, \"Win_Rate\": win_rate,\n",
        "                \"Avg_Win\": avg_win, \"Avg_Loss\": avg_loss, \"Profit_Factor\": pf,\n",
        "                \"Trades\": int(run_stats[\"Trades\"]),\n",
        "                \"Avg_Time_In_Trade\": float(run_stats[\"Avg_Time_In_Trade\"]),\n",
        "                \"Avg_Time_In_Win_Trade\": float(run_stats[\"Avg_Time_In_Win_Trade\"]),\n",
        "                \"Avg_Time_In_Loss_Trade\": float(run_stats[\"Avg_Time_In_Loss_Trade\"]),\n",
        "                \"Time_Out_Of_Market\": time_out,\n",
        "                \"Profitable_Bar_Time_Rate\": pbar_rate,\n",
        "                \"RS63_last3\": str(rs_tail),\n",
        "            })\n",
        "\n",
        "            # Stream to CSV in batches\n",
        "            if len(batch) >= local_stream_every:\n",
        "                pd.DataFrame(batch).to_csv(out_path, mode='a',\n",
        "                                           header=not os.path.exists(out_path), index=False)\n",
        "                written += len(batch)\n",
        "                print(f\"[FLUSH] {t}: +{len(batch):,} rows (total={written:,}) â†’ {out_path}\")\n",
        "                batch.clear()\n",
        "                gc.collect()\n",
        "\n",
        "            # Throttle RAM if needed + adaptive backoff\n",
        "            if (i+1) % THROTTLE_EVERY == 0 and _should_throttle():\n",
        "                print(f\"[THROTTLE] {t}: gc + sleep {THROTTLE_SLEEP_S}s | stream_every={local_stream_every} | {_fmt_ram()}\")\n",
        "                gc.collect()\n",
        "                time.sleep(THROTTLE_SLEEP_S)\n",
        "                # adaptive backoff: reduce batch size (min 100)\n",
        "                new_stream_every = max(100, int(local_stream_every * ADAPTIVE_BACKOFF_STEP))\n",
        "                if new_stream_every < local_stream_every:\n",
        "                    print(f\"[ADAPT] {t}: stream batch {local_stream_every} â†’ {new_stream_every}\")\n",
        "                    local_stream_every = new_stream_every\n",
        "\n",
        "            # periodic progress\n",
        "            if combo_count % progress_every == 0 or combo_count == 1:\n",
        "                elapsed = time.time() - start_t\n",
        "                pct = 100.0 * combo_count / max(1, len(combos))\n",
        "                print(f\"[PROG] {t}: {combo_count:,}/{len(combos):,} ({pct:5.1f}%) | written={written:,} | \"\n",
        "                      f\"stream_every={local_stream_every} | elapsed={elapsed:,.1f}s | {_fmt_ram()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERR ] {t} / combo {int(row.get('combination_id', -1))}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Flush remaining rows\n",
        "    if batch:\n",
        "        pd.DataFrame(batch).to_csv(out_path, mode='a', header=not os.path.exists(out_path), index=False)\n",
        "        written += len(batch)\n",
        "        print(f\"[FLUSH] {t}: +{len(batch):,} rows (total={written:,}) â†’ {out_path}\")\n",
        "        batch.clear()\n",
        "        gc.collect()\n",
        "\n",
        "    elapsed = time.time() - start_t\n",
        "    print(f\"[DONE] {t}: wrote {written:,} rows in {elapsed:,.1f}s â†’ {out_path} | {_fmt_ram()}\")\n",
        "    del ind, td\n",
        "    gc.collect()\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# ---------------------- Driver: serial or parallel per-ticker ----------------------\n",
        "# Ensure combos are available from CSV (robust to undefined in session)\n",
        "PARAM_CSV = \"/Users/chielg/Documents/GitHub/TA_optimization/parameter_combinations.csv\"\n",
        "combos  = pd.read_csv(PARAM_CSV)\n",
        "\n",
        "tickers = data[\"Ticker\"].unique()\n",
        "print(f\"[DRIVER] Evaluating {len(tickers)} tickers Ã— {len(combos):,} combinations = {len(tickers)*len(combos):,} evals (serial={not USE_PARALLEL})\")\n",
        "\n",
        "per_ticker_files = []\n",
        "\n",
        "if USE_PARALLEL:\n",
        "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "    print(f\"[DRIVER] Starting parallel pool with max_workers={MAX_WORKERS}\")\n",
        "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futs = {ex.submit(_evaluate_ticker_metrics, t, data, combos): t for t in tickers}\n",
        "        for f in as_completed(futs):\n",
        "            t = futs[f]\n",
        "            try:\n",
        "                p = f.result()\n",
        "                if p:\n",
        "                    per_ticker_files.append(p)\n",
        "                    print(f\"[DRIVER] Finished {t} â†’ {p}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[DRIVER] {t} raised: {e}\")\n",
        "else:\n",
        "    for t in tickers:\n",
        "        _t0 = time.time()\n",
        "        p = _evaluate_ticker_metrics(t, data, combos)\n",
        "        dt = time.time() - _t0\n",
        "        if p:\n",
        "            per_ticker_files.append(p)\n",
        "            print(f\"[DRIVER] Finished {t} in {dt:,.1f}s â†’ {p}\")\n",
        "\n",
        "# ---------------------- Merge all per-ticker CSVs into final file ----------------------\n",
        "if os.path.exists(FINAL_OUT_CSV):\n",
        "    os.remove(FINAL_OUT_CSV)\n",
        "\n",
        "print(f\"[MERGE] Collating {len(per_ticker_files)} files â†’ {FINAL_OUT_CSV}\")\n",
        "header_written = False\n",
        "total_merged = 0\n",
        "for fp in per_ticker_files:\n",
        "    try:\n",
        "        print(f\"[MERGE] {fp} ...\")\n",
        "        for chunk in pd.read_csv(fp, chunksize=200_000):\n",
        "            chunk.to_csv(FINAL_OUT_CSV, mode='a', header=not header_written, index=False)\n",
        "            header_written = True\n",
        "            total_merged += len(chunk)\n",
        "            print(f\"[MERGE] {fp}: +{len(chunk):,} rows (total={total_merged:,})\")\n",
        "    except Exception as e:\n",
        "        print(f\"[MERGE] warning for {fp}: {e}\")\n",
        "\n",
        "print(f\"[DONE ] Saved all metrics ({total_merged:,} rows) â†’ {FINAL_OUT_CSV}\")\n",
        "\n",
        "# Small preview without loading everything\n",
        "try:\n",
        "    preview_df = pd.read_csv(FINAL_OUT_CSV, nrows=20)\n",
        "    display(preview_df)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1612969181.py, line 1)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmooi dashboard bouwen met equity curve voor analyse, trade timing, equity curve voor lineariteit, die dingen\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "mooi dashboard bouwen met equity curve voor analyse, trade timing, equity curve voor lineariteit, die dingen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (venv)",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
