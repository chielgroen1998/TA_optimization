{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ MEMORY OPTIMIZATION HELPERS LOADED! Ready for NASDAQ 100+ scale.\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# CRITICAL MEMORY OPTIMIZATION FOR NASDAQ 100+ SCALE COMPUTATION\n",
        "# ================================================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from operator import mul\n",
        "from typing import Iterator, Dict, List, Any, Optional\n",
        "import gc\n",
        "\n",
        "# --- Core Memory Optimization Helpers ---\n",
        "\n",
        "def iter_param_combinations(indicator_name: str, INDICATOR_PARAMS: Dict) -> Iterator[Dict[str, Any]]:\n",
        "    if indicator_name not in INDICATOR_PARAMS:\n",
        "        return\n",
        "    param_names = list(INDICATOR_PARAMS[indicator_name].keys())\n",
        "    param_values = list(INDICATOR_PARAMS[indicator_name].values())\n",
        "    from itertools import product\n",
        "    for combo in product(*param_values):\n",
        "        yield {param_names[i]: combo[i] for i in range(len(param_names))}\n",
        "\n",
        "\n",
        "def count_param_combinations(indicator_name: str, INDICATOR_PARAMS: Dict) -> int:\n",
        "    if indicator_name not in INDICATOR_PARAMS:\n",
        "        return 0\n",
        "    lengths = [len(v) for v in INDICATOR_PARAMS[indicator_name].values()]\n",
        "    return reduce(mul, lengths, 1)\n",
        "\n",
        "\n",
        "def _ensure_df(results_like):\n",
        "    if isinstance(results_like, list):\n",
        "        return pd.DataFrame(results_like)\n",
        "    return results_like\n",
        "\n",
        "\n",
        "def calculate_signals_vectorized(ticker_data: pd.DataFrame, signals: pd.Series) -> Dict[str, Any]:\n",
        "    daily_return = ticker_data['Close'].pct_change()\n",
        "    position = signals.shift(1).fillna(0).astype('float32')\n",
        "    strategy_return = (position * daily_return).astype('float32')\n",
        "\n",
        "    valid_returns = strategy_return.dropna()\n",
        "    if valid_returns.empty:\n",
        "        return {\n",
        "            'total_return': np.nan, 'sharpe': np.nan, 'max_drawdown': np.nan,\n",
        "            'win_rate': np.nan, 'total_signals': 0, 'buy_signals': 0, 'sell_signals': 0\n",
        "        }\n",
        "\n",
        "    total_return = float((1.0 + valid_returns).prod() - 1.0)\n",
        "    std = float(valid_returns.std())\n",
        "    sharpe = float((valid_returns.mean() / std) * np.sqrt(252)) if std > 1e-8 else 0.0\n",
        "\n",
        "    equity = (1.0 + valid_returns).cumprod()\n",
        "    running_max = equity.cummax()\n",
        "    drawdown = (equity - running_max) / running_max\n",
        "    max_drawdown = float(drawdown.min())\n",
        "\n",
        "    win_rate = float((valid_returns > 0).mean())\n",
        "\n",
        "    return {\n",
        "        'total_return': total_return,\n",
        "        'sharpe': sharpe,\n",
        "        'max_drawdown': max_drawdown,\n",
        "        'win_rate': win_rate,\n",
        "        'total_signals': int(signals.notna().sum()),\n",
        "        'buy_signals': int((signals == 1).sum()),\n",
        "        'sell_signals': int((signals == -1).sum())\n",
        "    }\n",
        "\n",
        "\n",
        "def stream_results_to_csv(results: List[Dict], filename: str, mode: str = 'a') -> None:\n",
        "    if not results:\n",
        "        return\n",
        "    df_chunk = pd.DataFrame(results)\n",
        "    file_exists = os.path.exists(filename)\n",
        "    df_chunk.to_csv(filename, mode=mode, header=not file_exists, index=False)\n",
        "    del df_chunk\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "print(\"ðŸš€ MEMORY OPTIMIZATION HELPERS LOADED! Ready for NASDAQ 100+ scale.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Clean indicator params & signal logic loaded.\n",
            "  KAMA: 150 combinations\n",
            "  Supertrend: 18 combinations\n",
            "  MFI: 8 combinations\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# INDICATOR PARAMS (CLEAN) AND SIGNAL LOGIC\n",
        "# ================================================================================\n",
        "\n",
        "# Indicators and parameter space (NO ADX/RSI/ATR/Entropy)\n",
        "INDICATOR_PARAMS = {\n",
        "    'KAMA': {\n",
        "        'er_period': list(range(5, 8)),\n",
        "        'fast_period': list(range(7, 12)),\n",
        "        'slow_period': list(range(15, 25)),\n",
        "    },\n",
        "    'Supertrend': {\n",
        "        'period': list(range(7, 16)),\n",
        "        'multiplier': [1.0, 2.0]\n",
        "    },\n",
        "    'MFI': {\n",
        "        'period': list(range(7, 15)),\n",
        "        'overbought': [80],\n",
        "        'oversold': [20]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Signal logic\n",
        "\n",
        "def get_buy_signal(indicator_name: str, data: pd.Series, params: dict) -> pd.Series:\n",
        "    if indicator_name == 'KAMA':\n",
        "        short_kama = params.get('short_kama', pd.Series(index=data.index))\n",
        "        long_kama = params.get('long_kama', pd.Series(index=data.index))\n",
        "        up = (short_kama > long_kama) & (short_kama.shift(1) <= long_kama.shift(1))\n",
        "        dn = (short_kama < long_kama) & (short_kama.shift(1) >= long_kama.shift(1))\n",
        "        s = pd.Series(0, index=data.index)\n",
        "        s[up] = 1\n",
        "        s[dn] = -1\n",
        "        return s\n",
        "\n",
        "    if indicator_name == 'Supertrend':\n",
        "        close = params.get('close', pd.Series(index=data.index))\n",
        "        return pd.Series(np.where(close > data, 1, np.where(close < data, -1, 0)), index=data.index)\n",
        "\n",
        "    if indicator_name == 'MFI':\n",
        "        overbought = params.get('overbought', 80)\n",
        "        oversold = params.get('oversold', 20)\n",
        "        return pd.Series(np.where(data < oversold, 1, np.where(data > overbought, -1, 0)), index=data.index)\n",
        "\n",
        "    return pd.Series(0, index=data.index)\n",
        "\n",
        "print(\"âœ… Clean indicator params & signal logic loaded.\")\n",
        "for k in INDICATOR_PARAMS:\n",
        "    print(f\"  {k}: {count_param_combinations(k, INDICATOR_PARAMS)} combinations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# OPTIMIZED GRID SEARCH FUNCTION\n",
        "# ================================================================================\n",
        "\n",
        "def run_comprehensive_grid_search_optimized(\n",
        "    data: pd.DataFrame,\n",
        "    indicator_params: Dict,\n",
        "    indicator_class,\n",
        "    max_combinations_per_indicator: int = 0,\n",
        "    stream_results_dir: Optional[str] = None,\n",
        "    summary_interval: int = 100\n",
        ") -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
        "    all_results: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}\n",
        "    tickers = pd.Index(data['Ticker']).unique().tolist()\n",
        "\n",
        "    print(\"\\nðŸš€ Starting Optimized Grid Search...\")\n",
        "    print(f\"Processing {len(tickers)} tickers with {len(indicator_params)} indicators.\")\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"\\nðŸ“Š Processing {ticker}...\")\n",
        "        td = data.loc[data['Ticker'] == ticker].copy()\n",
        "        td.sort_index(inplace=True)\n",
        "        ind = indicator_class(td)\n",
        "        all_results[ticker] = {}\n",
        "\n",
        "        for indicator_name, _ in indicator_params.items():\n",
        "            total_combos = count_param_combinations(indicator_name, indicator_params)\n",
        "            if total_combos == 0:\n",
        "                continue\n",
        "            limit = max_combinations_per_indicator if max_combinations_per_indicator > 0 else total_combos\n",
        "            print(f\"    ðŸ” {indicator_name}: evaluating {limit}/{total_combos} combinations\")\n",
        "\n",
        "            indicator_results: List[Dict[str, Any]] = []\n",
        "            batch_stream: List[Dict[str, Any]] = []\n",
        "\n",
        "            for i, params in enumerate(iter_param_combinations(indicator_name, indicator_params), start=1):\n",
        "                if i > limit:\n",
        "                    break\n",
        "                if i % summary_interval == 0 or i == limit:\n",
        "                    print(f\"      -> {i}/{limit} combos processed\")\n",
        "                try:\n",
        "                    if indicator_name == 'KAMA':\n",
        "                        vals = ind.kama(params['er_period'], params['fast_period'], params['slow_period'])\n",
        "                        s_params = {**params, 'close': td['Close']}\n",
        "                    elif indicator_name == 'Supertrend':\n",
        "                        vals = ind.supertrend(period=params['period'], multiplier=params['multiplier'])\n",
        "                        s_params = {**params, 'close': td['Close']}\n",
        "                    elif indicator_name == 'MFI':\n",
        "                        vals = ind.mfi(params['period'])\n",
        "                        s_params = params\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    sig = get_buy_signal(indicator_name, vals, s_params)\n",
        "                    metrics = calculate_signals_vectorized(td, sig)\n",
        "\n",
        "                    entry = {'Ticker': ticker, 'Indicator': indicator_name, 'Parameters': params, **metrics}\n",
        "                    indicator_results.append(entry)\n",
        "                    batch_stream.append(entry)\n",
        "\n",
        "                    if stream_results_dir and (len(batch_stream) >= 500 or i == limit):\n",
        "                        os.makedirs(stream_results_dir, exist_ok=True)\n",
        "                        fname = os.path.join(stream_results_dir, f\"{ticker}_{indicator_name}.csv\")\n",
        "                        stream_results_to_csv(batch_stream, fname)\n",
        "                        batch_stream.clear()\n",
        "                        gc.collect()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    âŒ {indicator_name} params {params} failed: {e}\")\n",
        "                    continue\n",
        "\n",
        "            all_results[ticker][indicator_name] = indicator_results\n",
        "\n",
        "            if indicator_results:\n",
        "                df_show = pd.DataFrame(indicator_results)\n",
        "                if 'sharpe' in df_show.columns and not df_show['sharpe'].dropna().empty:\n",
        "                    bi = df_show['sharpe'].idxmax()\n",
        "                    br = df_show.loc[bi]\n",
        "                    print(f\"    âœ… Best {indicator_name}: Sharpe={br['sharpe']:.3f}, Return={br['total_return']:.3f}\")\n",
        "\n",
        "        del td, ind\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"\\nâœ… Optimized Grid Search Complete!\")\n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Optimized analysis & plotting loaded.\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# OPTIMIZED ANALYSIS & PLOTTING\n",
        "# ================================================================================\n",
        "\n",
        "def analyze_grid_search_results_optimized(all_results: Dict[str, Dict[str, List[Dict[str, Any]]]]) -> Dict[str, Any]:\n",
        "    analysis = {'best_per_ticker': {}, 'best_indicators_overall': {}, 'summary_stats': {}}\n",
        "    perf: Dict[str, List[float]] = {}\n",
        "    all_sharpes: List[float] = []\n",
        "\n",
        "    for ticker, tr in all_results.items():\n",
        "        best_sh, best_ind, best_params = -np.inf, None, None\n",
        "        for ind_name, res_list in tr.items():\n",
        "            df = _ensure_df(res_list)\n",
        "            if df.empty or 'sharpe' not in df.columns:\n",
        "                continue\n",
        "            s = pd.to_numeric(df['sharpe'], errors='coerce').dropna()\n",
        "            if s.empty:\n",
        "                continue\n",
        "            imax = s.idxmax()\n",
        "            shv = float(s.loc[imax])\n",
        "            if shv > best_sh:\n",
        "                best_sh = shv\n",
        "                best_ind = ind_name\n",
        "                best_params = df.loc[imax, 'Parameters']\n",
        "            perf.setdefault(ind_name, []).extend(s.tolist())\n",
        "            all_sharpes.extend(s.tolist())\n",
        "        analysis['best_per_ticker'][ticker] = {'indicator': best_ind, 'sharpe': best_sh, 'params': best_params}\n",
        "\n",
        "    analysis['best_indicators_overall'] = {\n",
        "        k: (np.nanmean(v) if v else np.nan) for k, v in perf.items()\n",
        "    }\n",
        "    analysis['summary_stats'] = {\n",
        "        'num_tickers': len(all_results),\n",
        "        'num_indicators': len(perf),\n",
        "        'avg_sharpe_overall': float(np.nanmean(all_sharpes)) if all_sharpes else np.nan,\n",
        "    }\n",
        "    return analysis\n",
        "\n",
        "\n",
        "def create_results_summary_table_optimized(all_results: Dict[str, Dict[str, List[Dict[str, Any]]]], output_file: Optional[str] = None) -> pd.DataFrame:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    for ticker, tr in all_results.items():\n",
        "        for ind_name, rl in tr.items():\n",
        "            df = _ensure_df(rl)\n",
        "            if df.empty:\n",
        "                continue\n",
        "            for _, row in df.iterrows():\n",
        "                rows.append({\n",
        "                    'Ticker': ticker,\n",
        "                    'Indicator': ind_name,\n",
        "                    'Parameters': row.get('Parameters'),\n",
        "                    'Sharpe_Ratio': row.get('sharpe'),\n",
        "                    'Total_Return': row.get('total_return'),\n",
        "                    'Max_Drawdown': row.get('max_drawdown'),\n",
        "                    'Win_Rate': row.get('win_rate'),\n",
        "                    'Total_Signals': row.get('total_signals'),\n",
        "                    'Buy_Signals': row.get('buy_signals'),\n",
        "                    'Sell_Signals': row.get('sell_signals'),\n",
        "                })\n",
        "    out = pd.DataFrame(rows)\n",
        "    if output_file:\n",
        "        os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
        "        out.to_csv(output_file, index=False)\n",
        "    return out\n",
        "\n",
        "\n",
        "def find_similar_parameter_combinations_optimized(results_like, target_params: Dict[str, Any], max_distance: float = 5.0) -> pd.DataFrame:\n",
        "    df = _ensure_df(results_like)\n",
        "    if df.empty or 'Parameters' not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def _dist(p1: dict, p2: dict) -> float:\n",
        "        if not p1 or not p2:\n",
        "            return float('inf')\n",
        "        common = set(p1.keys()) & set(p2.keys())\n",
        "        if not common:\n",
        "            return float('inf')\n",
        "        d = 0.0\n",
        "        for k in common:\n",
        "            try:\n",
        "                d += (float(p1[k]) - float(p2[k])) ** 2\n",
        "            except Exception:\n",
        "                pass\n",
        "        return d ** 0.5\n",
        "\n",
        "    sims: List[Dict[str, Any]] = []\n",
        "    for _, row in df.iterrows():\n",
        "        dist = _dist(row['Parameters'], target_params)\n",
        "        if dist <= max_distance:\n",
        "            r = row.to_dict()\n",
        "            r['Parameter_Distance'] = dist\n",
        "            sims.append(r)\n",
        "    return pd.DataFrame(sims).sort_values('Parameter_Distance')\n",
        "\n",
        "\n",
        "def plot_indicator_performance_comparison_optimized(all_results: Dict[str, Dict[str, List[Dict[str, Any]]]], metric: str = 'sharpe') -> pd.DataFrame:\n",
        "    plot_rows: List[Dict[str, Any]] = []\n",
        "    for ticker, tr in all_results.items():\n",
        "        for ind_name, rl in tr.items():\n",
        "            df = _ensure_df(rl)\n",
        "            if df.empty or metric not in df.columns:\n",
        "                continue\n",
        "            vals = pd.to_numeric(df[metric], errors='coerce').dropna().tolist()\n",
        "            for v in vals:\n",
        "                plot_rows.append({'Ticker': ticker, 'Indicator': ind_name, metric: v})\n",
        "    return pd.DataFrame(plot_rows)\n",
        "\n",
        "print(\"âœ… Optimized analysis & plotting loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CHUNKED DATA PROCESSING FOR INDICATORS\n",
        "# ================================================================================\n",
        "\n",
        "def load_data_and_apply_indicators_chunked(\n",
        "    raw_data: pd.DataFrame,\n",
        "    tickers: List[str],\n",
        "    apply_indicators_single_ticker_func: callable,\n",
        "    chunk_size: int = 10\n",
        ") -> pd.DataFrame:\n",
        "    processed_chunks = []\n",
        "    print(f\"\\nðŸ“¦ Chunked processing for {len(tickers)} tickers...\")\n",
        "    for i in range(0, len(tickers), chunk_size):\n",
        "        part = tickers[i:i + chunk_size]\n",
        "        print(f\"  -> {i+1}-{i+len(part)} of {len(tickers)}\")\n",
        "        sub = raw_data[raw_data['Ticker'].isin(part)].copy()\n",
        "        tmp = []\n",
        "        for t in part:\n",
        "            df_t = sub[sub['Ticker'] == t].copy()\n",
        "            if df_t.empty:\n",
        "                continue\n",
        "            try:\n",
        "                tmp.append(apply_indicators_single_ticker_func(df_t))\n",
        "            except Exception as e:\n",
        "                print(f\"    âŒ {t}: {e}\")\n",
        "        if tmp:\n",
        "            processed_chunks.append(pd.concat(tmp, ignore_index=False))\n",
        "        del sub, tmp\n",
        "        gc.collect()\n",
        "    if processed_chunks:\n",
        "        print(\"âœ… Chunked processing complete.\")\n",
        "        return pd.concat(processed_chunks, ignore_index=False)\n",
        "    print(\"âš ï¸ No data processed.\")\n",
        "    return pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… TAopt_optimized.ipynb ready. Fill in your data and run the example above.\n"
          ]
        }
      ],
      "source": [
        "# ================================================================================\n",
        "# USAGE EXAMPLE (SKELETON)\n",
        "# ================================================================================\n",
        "\n",
        "# Example:\n",
        "# 1) Prepare data (ensure it contains 'Ticker' and 'Close' columns and is indexed by date)\n",
        "# data = your_loaded_dataframe\n",
        "# tickers = sorted(data['Ticker'].unique().tolist())\n",
        "\n",
        "# 2) Optionally apply indicators in a memory-friendly way if you have a per-ticker function\n",
        "# def apply_indicators_single_ticker(df_single_ticker: pd.DataFrame) -> pd.DataFrame:\n",
        "#     # Example: add your technical indicators here for a single ticker\n",
        "#     # Return the modified DataFrame\n",
        "#     return df_single_ticker\n",
        "# data_with_indicators = load_data_and_apply_indicators_chunked(data, tickers, apply_indicators_single_ticker, chunk_size=20)\n",
        "\n",
        "# 3) Run optimized grid search\n",
        "# from your_indicators_module import Indicator\n",
        "# all_results = run_comprehensive_grid_search_optimized(\n",
        "#     data=data,\n",
        "#     indicator_params=INDICATOR_PARAMS,\n",
        "#     indicator_class=Indicator,  # class that computes KAMA, Supertrend, MFI series\n",
        "#     max_combinations_per_indicator=0,  # 0 means all combinations\n",
        "#     stream_results_dir='grid_search_streamed_results',\n",
        "#     summary_interval=100,\n",
        "# )\n",
        "\n",
        "# 4) Analyze and save results\n",
        "# analysis = analyze_grid_search_results_optimized(all_results)\n",
        "# summary_df = create_results_summary_table_optimized(all_results, output_file='strategy_results/performance_summary_optimized.csv')\n",
        "# plot_df = plot_indicator_performance_comparison_optimized(all_results, metric='sharpe')\n",
        "\n",
        "print(\"âœ… TAopt_optimized.ipynb ready. Fill in your data and run the example above.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
